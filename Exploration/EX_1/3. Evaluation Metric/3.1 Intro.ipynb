{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 머신러닝 모델의 성능 평가 (Evaluation)\n",
    "\n",
    "#### 🤔 우리가 만든 모델, 충분히 잘 만든 걸까?\n",
    "\n",
    "여러분들은 그동안 다양한 머신러닝 모델을 만들어 보았을 것입니다.  \n",
    "그런데 **어떤 기준으로** 우리가 만든 모델이 **충분히 완성되었다**고 판단할 수 있을까요?\n",
    "\n",
    "> 모델의 학습이 아무리 잘 돼도, **성능 평가(evaluation)** 없이는 완성도를 확인할 수 없습니다.\n",
    "\n",
    "---\n",
    "\n",
    "  \n",
    "#### ✅ 성능 평가는 왜 중요할까?\n",
    "\n",
    "- **모델의 완성도**를 판단할 수 있는 유일한 수단  \n",
    "- **학습 성능을 수치화**하고 비교할 수 있음  \n",
    "- 잘못된 모델을 **조기에 발견**할 수 있음  \n",
    "- 실제 서비스 적용 여부를 결정하는 **기준점**\n",
    "\n",
    "---\n",
    "\n",
    "  \n",
    "#### 🧪 문제마다 다른 평가 척도\n",
    "\n",
    "모든 머신러닝 문제는 **고유의 평가 기준(metric)**이 있습니다.  \n",
    "심지어 같은 문제에서도 **목적에 따라 여러 척도**가 사용될 수 있습니다.\n",
    "\n",
    "> 예: 분류 문제에서도 Accuracy 외에 Precision, Recall, F1 Score 등 다양한 척도가 존재\n",
    "\n",
    "---\n",
    "\n",
    "  \n",
    "#### 🧠 사이킷런 평가 척도 예시\n",
    "\n",
    "사이킷런에서는 다양한 평가 척도를 제공합니다:  \n",
    "🔗 [Scikit-learn Evaluation Metrics Docs](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "\n",
    "> **꼼꼼히 읽기보다, 지금은 훑어보며 어떤 척도가 있는지만 감 잡기!**\n",
    "\n",
    "---\n",
    "\n",
    "  \n",
    "#### 📚 분류 문제 외에도 다양한 평가 척도 존재\n",
    "\n",
    "머신러닝에는 **분류(Classification)** 외에도 다양한 모델이 존재합니다.\n",
    "\n",
    "| 분야 | 예시 모델 | 대표 평가 척도 |\n",
    "|------|-----------|----------------|\n",
    "| **회귀 (Regression)** | 선형 회귀, 랜덤 포레스트 회귀 | MSE, RMSE, MAE, R² |\n",
    "| **랭킹 (Ranking)** | 추천 시스템, 검색 엔진 | NDCG, MAP |\n",
    "| **군집 (Clustering)** | K-Means, DBSCAN | Silhouette Score, ARI |\n",
    "| **딥러닝 분야** | CNN, RNN, BERT 등 | Task별 맞춤 척도 (예: BLEU, IoU 등) |\n",
    "\n",
    "---\n",
    "\n",
    "  \n",
    "#### 🎯 오늘의 목표\n",
    "\n",
    "오늘은 그중 **분류 모델**에 집중하여:\n",
    "\n",
    "- 모델 성능을 평가할 때 고려해야 할 **다양한 관점**\n",
    "- 대표적인 평가 척도의 정의와 **사용 목적**\n",
    "- **왜 여러 평가 방식이 필요한지** 이해하는 것\n",
    "\n",
    "---\n",
    "\n",
    "  \n",
    "#### 🚀 그럼 시작해볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
