{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📉 회귀 모델의 오류 지표 (MAE, MSE, RMSE)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### ✅ 회귀 문제에서의 평가 지표\n",
    "\n",
    "머신러닝 회귀 문제에서 모델의 성능을 평가할 때  \n",
    "가장 널리 사용되는 지표는 **MAE**, **MSE**, **RMSE**입니다.\n",
    "\n",
    "이 세 가지 지표는 모두 **예측값과 실제값 사이의 차이(오차)**를 측정하지만,  \n",
    "**계산 방식과 이상치에 대한 민감도에서 차이**가 있습니다.\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### 📐 MAE (Mean Absolute Error)\n",
    "\n",
    "$$ MAE = \\frac{1}{n}\\sum_{i=1}^{n} |y_i - \\hat{y}_i| $$\n",
    "\n",
    "- 예측값과 실제값의 **절대값 차이**를 평균\n",
    "- **모든 오차를 동등하게 반영**  \n",
    "- 이상치(outlier)에 **덜 민감**\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### 📐 MSE (Mean Squared Error)\n",
    "\n",
    "$$ MSE = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "- 예측값과 실제값의 차이를 **제곱**해서 평균  \n",
    "- **큰 오차를 더 크게 반영**  \n",
    "- 이상치에 **더 민감**\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### 📐 RMSE (Root Mean Squared Error)\n",
    "\n",
    "$$ RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $$\n",
    "\n",
    "- MSE에 **제곱근**을 취한 값  \n",
    "- 이상치에 **민감하긴 하지만**,  \n",
    "  **오차 단위가 실제 값과 동일**해서 **해석이 직관적**\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "### 🖼️ 시각 자료\n",
    "\n",
    "**MAE**  \n",
    "<img src=\"https://blog-web.modulabs.co.kr/wp-content/uploads/2024/05/MAE.png\" width=\"500\">\n",
    "\n",
    "**MSE**  \n",
    "<img src=\"https://blog-web.modulabs.co.kr/wp-content/uploads/2024/05/MSE.png\" width=\"500\">\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### 🎯 예시로 비교해보기\n",
    "\n",
    "실제값: `[1, 2, 3, 4, 5]`  \n",
    "모델 A 예측: `[1, 2, 3, 4, 10]`  \n",
    "모델 B 예측: `[3, 3, 3, 3, 3]`\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "**모델 A 계산:**\n",
    "\n",
    "- $$ MAE_A = \\frac{0 + 0 + 0 + 0 + 5}{5} = 1 $$\n",
    "- $$ MSE_A = \\frac{0^2 + 0^2 + 0^2 + 0^2 + 5^2}{5} = 5 $$\n",
    "- $$ RMSE_A = \\sqrt{5} \\approx 2.24 $$\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "**모델 B 계산:**\n",
    "\n",
    "- $$ MAE_B = \\frac{2 + 1 + 0 + 1 + 2}{5} = 1.2 $$\n",
    "- $$ MSE_B = \\frac{2^2 + 1^2 + 0^2 + 1^2 + 2^2}{5} = 2 $$\n",
    "- $$ RMSE_B = \\sqrt{2} \\approx 1.41 $$\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### 🧠 해석\n",
    "\n",
    "- 모델 A는 **마지막 예측값 10**이 이상치처럼 작용하여  \n",
    "  MSE, RMSE 값이 MAE보다 **훨씬 크게 증가**\n",
    "- 모델 B는 전체적으로 **균일하게 오차가 분포**되어 있어  \n",
    "  **세 지표가 비슷한 값**\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "### 📊 지표 비교 요약표\n",
    "\n",
    "| 특성 | MAE | MSE | RMSE |\n",
    "|------|-----|-----|------|\n",
    "| **오차 크기에 대한 민감도** | 낮음 (모든 오차 동등하게 처리) | 높음 (큰 오차 강조) | 높음 (MSE보단 덜하지만 큰 오차 강조) |\n",
    "| **이상치 민감도** | 낮음 | 높음 | 높음 |\n",
    "| **오차 단위 해석** | 예측 오차 단위와 같음 | 단위가 제곱됨 (해석 어려움) | 오차 단위와 같음 (직관적) |\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### ✅ 마무리\n",
    "\n",
    "- 이상치가 **많은 데이터** → **MAE**가 적합  \n",
    "- **큰 오차를 강조**하고 싶다면 → **MSE 또는 RMSE** 사용  \n",
    "- RMSE는 MSE보다 **해석이 쉬운 점에서 실무에 자주 사용됨**\n",
    "\n",
    "모델의 목적과 데이터 특성에 따라  \n",
    "**적절한 지표를 선택하는 것이 중요합니다.**\n",
    "\n",
    "<hr style=\"opacity:0.2;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔍랭킹 모델의 평가 척도\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✅ 배경 설명\n",
    "\n",
    "음악 스트리밍 서비스처럼 사용자가 \"무슨 노래를 들어야 할지 모를 때\",  \n",
    "많은 사람들이 들은 **TOP 100 리스트** 는 좋은 추천이 됩니다.\n",
    "\n",
    "이처럼 **많은 사용자 행동(재생 수, 클릭 수 등)** 을 기반으로  \n",
    "추천된 항목의 **순위(ranking)** 를 고려하는 것이 중요합니다.\n",
    "\n",
    "정보 검색 및 추천 시스템에서는 이런 랭킹 기반의 평가를 위해  \n",
    "다음 4가지 지표가 자주 사용됩니다:\n",
    "\n",
    "- **MAP (Mean Average Precision)**  \n",
    "- **MRR (Mean Reciprocal Rank)**  \n",
    "- **DCG (Discounted Cumulative Gain)**  \n",
    "- **NDCG (Normalized DCG)**\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### 🎯 1. MAP (Mean Average Precision)\n",
    "\n",
    "- 여러 쿼리에 대해, 정답이 나타날 때마다의 **정밀도(Precision)** 를 평균한 값\n",
    "- 전체적으로 **정확한 문서가 상위에 많이 배치** 될수록 높은 점수\n",
    "\n",
    "##### 📐 수식\n",
    "\n",
    "$$ AP@5 = \\frac{1}{4} \\sum_{k=1}^{5} P(k) = \\frac{1}{4} (1/1 + 2/3 + 3/4) = 0.604 $$  \n",
    "$$ AP@5 = \\frac{1}{4} \\sum_{k=1}^{5} P(k) = \\frac{1}{4} (1/2 + 2/3 + 3/4) = 0.479 $$  \n",
    "$$ MAP@5 = \\frac{1}{2} (0.604 + 0.479) = 0.546 $$\n",
    "\n",
    "##### ✅ 특징\n",
    "\n",
    "- **장점** : 전체 정답을 고려한 정확도 평가 가능  \n",
    "- **단점** : 정답이 하나도 없으면 해당 쿼리의 AP는 0이 됨\n",
    "\n",
    "💡 **MAP@K** 에서 K는 컷오프 기준이며, 사용자의 선호 아이템 개수(m)가 K보다 클 수 있습니다.  \n",
    "하지만 MAP은 **상위 K개 결과만 반영** 하므로, K 설정이 중요합니다.\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### 🎯 2. MRR (Mean Reciprocal Rank)\n",
    "\n",
    "- 사용자가 **처음으로 정답을 만난 위치의 역수** 를 평균한 값\n",
    "- 얼마나 빨리 정답이 추천되었는지를 측정\n",
    "\n",
    "##### 📐 수식\n",
    "\n",
    "$$ RR = \\frac{1}{\\text{rank}} $$  \n",
    "$$ MRR = \\frac{1}{U} \\sum_{u=1}^{U} \\frac{1}{\\text{rank}_i} = \\frac{1}{2}(1 + 0.5) = 0.75 $$\n",
    "\n",
    "##### ✅ 특징\n",
    "\n",
    "- **장점** : 매우 간단하고, 첫 번째 정답의 위치를 잘 반영\n",
    "- **단점** : 그 이후 정답 위치는 반영하지 않음 → **전반적 평가에는 약함**\n",
    "\n",
    "MRR 값이 0.75라는 것은, 평균적으로 사용자가 첫 번째 정답을 1.33번째 위치에서 발견했다는 것을 의미합니다.\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### 🎯 3. DCG (Discounted Cumulative Gain)\n",
    "\n",
    "- 추천 결과에 **관련성 점수(rel)** 를 부여하고,  \n",
    "  **하위에 있는 정답일수록 점수를 더 적게 부여** 하는 방식\n",
    "\n",
    "##### 📐 수식\n",
    "\n",
    "$$ CG@K = \\sum_{k=1}^{K} G_i $$\n",
    "\n",
    "$$ DCG@5 = rel_{1} + \\sum_{i=2}^{5} \\frac{rel_i}{\\log_2(i + 1)} $$\n",
    "\n",
    "예시 계산:\n",
    "\n",
    "$$ DCG@5 = 4 + \\frac{2}{\\log_2 4} + \\frac{3}{\\log_2 5} + \\frac{1}{\\log_2 6} = 6.68 $$\n",
    "\n",
    "💡 `rel_i`: i번째 추천 항목의 관련성 점수 (보통 0~4로 부여)\n",
    "\n",
    "##### ✅ 특징\n",
    "\n",
    "- 상위 랭킹에 정답이 위치할수록 더 높은 점수\n",
    "- 하지만 쿼리 간 비교는 어려움\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### 🎯 4. NDCG (Normalized DCG)\n",
    "\n",
    "- DCG를 **정상적인(이상적인) DCG 값으로 나눈 값**\n",
    "- 쿼리마다 다른 DCG 스케일을 **0~1 범위로 정규화**\n",
    "\n",
    "##### 📐 수식\n",
    "\n",
    "$$ IDCG@5 = 4 + \\frac{4}{\\log_2 3} + \\frac{3}{\\log_2 4} + \\frac{2}{\\log_2 5} + \\frac{1}{\\log_2 6} = 9.27 $$  \n",
    "$$ NDCG@5 = \\frac{DCG@5}{IDCG@5} = \\frac{6.68}{9.27} = 0.721 $$\n",
    "\n",
    "##### ✅ 특징\n",
    "\n",
    "- **정상적인 추천 순서와 비교** 해 얼마나 잘했는지를 평가\n",
    "- **쿼리 간 비교가 가능** , 사용자 중심적 평가 가능\n",
    "\n",
    "💡 rel은 클릭 수, 사용자 평가, 설문 결과 등으로 결정할 수 있습니다.\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### 📊 평가 지표 요약 비교\n",
    "\n",
    "| 지표 | 특징 | 장점 | 단점 |\n",
    "|------|------|------|------|\n",
    "| **MAP** | 평균 정밀도 | 전체 검색 정확도 반영 | 관련 문서가 없으면 0 |\n",
    "| **MRR** | 첫 정답 위치만 사용 | 빠른 정답 노출 평가 | 이후 결과 무시 |\n",
    "| **DCG** | 순위에 따른 점수 할인 | 상위 정답 중요도 강조 | 쿼리 간 비교 어려움 |\n",
    "| **NDCG** | 정규화된 DCG | 비교 가능, 사용자 친화 | 관련성 점수 주관적 |\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🖼️ 이미지 생성 모델의 유사성 측정 지표\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🖼️ 이미지 생성 모델의 유사성 측정 지표\n",
    "---\n",
    "\n",
    "#### ✅ 개요\n",
    "\n",
    "이미지 생성 모델의 성능을 평가한다는 것은  \n",
    "**원본 이미지와 생성된 이미지 사이의 거리(오차)**를 측정하는 것입니다.\n",
    "\n",
    "이미지 간 유사성을 측정하는 대표적인 방법은 다음 3가지입니다:\n",
    "\n",
    "- **MSE (Mean Squared Error)**  \n",
    "- **PSNR (Peak Signal-to-Noise Ratio)**  \n",
    "- **SSIM (Structural Similarity Index)**\n",
    "\n",
    "각 방법은 서로 다른 관점에서 이미지 품질을 평가합니다.\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### 📐 1. MSE (Mean Squared Error)\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{mn} \\sum_{i=1}^{m} \\sum_{j=1}^{n} (I(i,j) - K(i,j))^2\n",
    "$$\n",
    "\n",
    "- **정의**: 두 이미지의 픽셀 차이를 **제곱**한 뒤 **평균**을 낸 값  \n",
    "- **특징**:  \n",
    "  - 픽셀 단위의 정확도 측정  \n",
    "  - **낮을수록 유사**  \n",
    "  - 시각적 구조나 질감은 고려하지 않음\n",
    "\n",
    "💡 예시:  \n",
    "\n",
    "<img src=\"https://blog-web.modulabs.co.kr/wp-content/uploads/2024/03/%EA%B7%B8%EB%A6%BC1-300x163.png\" width=\"500\">\n",
    "원본 이미지 (2×2 픽셀): 모든 픽셀 값 255  \n",
    "비교 이미지: 모든 픽셀 값 235  \n",
    "\n",
    "<img src=\"https://blog-web.modulabs.co.kr/wp-content/uploads/2024/03/2-1.png\" width=\"500\">\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### 📐 2. PSNR (Peak Signal-to-Noise Ratio)\n",
    "\n",
    "$$\n",
    "PSNR = 10 \\cdot \\log_{10} \\left( \\frac{MAX_I^2}{MSE} \\right)\n",
    "$$\n",
    "\n",
    "- **정의**: 신호(원본 픽셀)와 잡음(MSE)의 비율을 로그로 표현한 값  \n",
    "- **특징**:  \n",
    "  - MSE 기반 평가  \n",
    "  - **값이 클수록 원본과 유사**  \n",
    "  - 압축 이미지의 품질 비교에 자주 사용\n",
    "\n",
    "💡 `MAX_I`는 픽셀의 최댓값 (보통 255)\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### 📐 3. SSIM (Structural Similarity Index)\n",
    "\n",
    "$$\n",
    "SSIM(x, y) = \\frac{(2\\mu_x\\mu_y + C_1)(2\\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + C_1)(\\sigma_x^2 + \\sigma_y^2 + C_2)}\n",
    "$$\n",
    "\n",
    "- **정의**: 이미지의 **밝기, 대비, 구조**를 종합적으로 고려해 유사성 평가  \n",
    "- **특징**:  \n",
    "  - 인간 시각 인식 방식에 기반  \n",
    "  - **질감, 형태, 구조**가 비슷한 이미지에 높은 점수  \n",
    "  - **1에 가까울수록 매우 유사**\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### 🔍 MSE / PSNR vs SSIM\n",
    "\n",
    "| 항목 | MSE / PSNR | SSIM |\n",
    "|------|-------------|------|\n",
    "| **비교 기준** | 픽셀 값 차이 | 구조, 질감, 대비 등 시각적 요소 |\n",
    "| **시각적 평가 반영** | ❌ 낮음 | ✅ 높음 |\n",
    "| **이상치에 대한 민감도** | ✅ 민감함 | ❌ 낮음 |\n",
    "| **값 해석 방식** | MSE는 낮을수록, PSNR은 높을수록 좋음 | 1에 가까울수록 유사 |\n",
    "\n",
    "💡 색상이 약간 다른데 구조는 유사한 경우:  \n",
    "→ **SSIM은 높게 평가**, MSE/PSNR은 낮게 평가\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### ✅ 결론\n",
    "\n",
    "- **MSE**: 픽셀 정확도만 보는 기본 오차 지표  \n",
    "- **PSNR**: MSE를 로그 변환한 비율형 지표  \n",
    "- **SSIM**: **시각적 유사성**을 반영한 고급 평가 방식\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🌍 기계번역 모델의 평가 척도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✅ 왜 기계 번역 평가가 필요할까?\n",
    "\n",
    "요즘 **기계 번역(Google 번역, Papago 같은 것)** 정말 좋아졌죠?  \n",
    "하지만 번역이 얼마나 \"좋은지\"를 알아야 더 나은 번역기를 만들 수 있어요.\n",
    "\n",
    "그래서 번역의 품질을 숫자로 평가하는 방법이 필요하고,  \n",
    "그 중 **가장 유명한 게 바로 BLEU 스코어** 예요.\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### 🧠 BLEU 스코어란?\n",
    "\n",
    "- 기계 번역한 문장과  \n",
    "- 사람이 번역한 문장을 비교해서  \n",
    "→ **얼마나 비슷한지 점수로 보여주는 지표** 예요.\n",
    "\n",
    "- BLEU는 **0점(완전 다름)**부터 **1점(완전 같음)** 사이의 값을 가집니다.  \n",
    "- 만든 사람은 IBM 연구소에서 일하던 ** Kishore Papineni** , 2002년에 발표했어요.\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### 📐 어떻게 계산할까?\n",
    "\n",
    "<img src=\"https://blog-web.modulabs.co.kr/wp-content/uploads/2024/04/bleu-equation.png\" width=\"500\">\n",
    "\n",
    "1. **n-gram**이라는 단위로 문장을 잘라요.  \n",
    "   - 예: \"I love AI\" → 2-gram = \"I love\", \"love AI\"\n",
    "\n",
    "2. 기계 번역 결과가 사람이 만든 문장과 얼마나 **같은 n-gram을 가졌는지** 확인해요.  \n",
    "   → 이걸 **정확도(Precision)** 라고 해요.\n",
    "\n",
    "3. 문장이 너무 짧으면 점수를 **감점(Brevity Penalty)** 해요.  \n",
    "   → 짧다고 무조건 좋은 번역은 아니니까요.\n",
    "\n",
    "4. 정리하면:  \n",
    "   **BLEU = 정밀도 × 패널티**\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### 🧪 예시로 이해하기\n",
    "\n",
    "- 참조 번역: `I adore machine translation`  \n",
    "- 기계 번역: `I love machine translation`  \n",
    "→ `machine translation`만 같으니까, 2-gram 정밀도는 1/3!\n",
    "\n",
    "→ BLEU는 **비슷하게 생긴 문장을 잘 찾아내는지** 보는 지표!\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### 🌐 왜 자주 쓰일까?\n",
    "\n",
    "- 계산이 **간단하고 빠름**  \n",
    "- **여러 언어** 에서도 비교 가능  \n",
    "- WMT 같은 국제 대회에서도 **기본 평가 기준**\n",
    "\n",
    "<img src=\"https://blog-web.modulabs.co.kr/wp-content/uploads/2024/04/wmt.png\" width=\"500\">\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### ⚠️ 근데 문제점도 있어요\n",
    "\n",
    "| 문제 | 설명 |\n",
    "|------|------|\n",
    "| 의미는 다 안 봄 | 문장은 다르게 써도 의미는 같을 수 있잖아요? 그걸 BLEU는 몰라요 |\n",
    "| 문장 구조 무시 | 순서만 바꿔도 점수는 확 떨어져요 |\n",
    "| 참조 번역에 의존 | 참조 번역이 이상하면 BLEU도 틀려요 |\n",
    "\n",
    "💬 예:  \n",
    "- \"The cat sat on the mat\"  \n",
    "- \"On the mat sat the cat\"  \n",
    "→ 둘 다 같은 뜻인데 BLEU는 낮은 점수 줘요.\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### 🧩 그래서 대안도 나왔어요\n",
    "\n",
    "<img src=\"https://blog-web.modulabs.co.kr/wp-content/uploads/2024/04/evaluation-meric-e1716364235553.png\" width=\"500\">\n",
    "\n",
    "- **METEOR**: 동의어나 어근도 봐요  \n",
    "- **TER**: 얼마나 고쳐야 할까? 보는 지표  \n",
    "- **BERTScore**: AI 모델(BERT)로 의미 비교  \n",
    "- **COMET**: 학습된 모델로 번역 평가\n",
    "\n",
    "→ BLEU보다 똑똑하고, 의미도 보는 지표들이에요.  \n",
    "하지만 계산이 어렵고 느릴 수 있어요.\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### ✅ 결론: 그래서 어떻게 써야 해?\n",
    "\n",
    "- **BLEU는 여전히 잘 쓰이는 지표** 예요.  \n",
    "  → 빠르고, 비교가 쉬우니까요.\n",
    "\n",
    "- 하지만 **BLEU만 보면 안 돼요.**  \n",
    "  → **의미, 문장 구조, 표현 다양성** 을 고려하려면 다른 지표도 함께 써야 해요.\n",
    "\n",
    "---\n",
    "\n",
    "#### 📌 한 줄 요약\n",
    "\n",
    "> BLEU는 \"기계 번역이 얼마나 사람 번역과 비슷한가\"를 보는 점수예요.  \n",
    "> 빠르고 편하지만, **의미나 문장 구조까지는 못 봐요.**  \n",
    "> 그래서 요즘은 **BLEU + 다른 평가 지표** 를 같이 써요.\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
