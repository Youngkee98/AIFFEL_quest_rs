{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy와 Confusion Matrix 이해하기\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Accuracy가 항상 좋은 Metric일까?\n",
    "\n",
    "분류 모델에서 자주 사용하는 지표인 **Accuracy(정확도)**는  \n",
    "전체 데이터 중에서 **모델이 정답을 맞힌 비율**을 나타냅니다.\n",
    "\n",
    "하지만 **Accuracy는 항상 좋은 성능 지표가 아닐 수 있습니다**,  \n",
    "Accuracy는 데이터의 불균형을 반영하지 못하기 때문입니다.\n",
    "특히 데이터가 한쪽으로 치우쳐 있는 **불균형 클래스 문제**에서는 주의가 필요합니다.\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### 🔢 Confusion Matrix (혼동 행렬)\n",
    "\n",
    "이진 분류 문제에서 모델의 예측 결과는 다음과 같은 4가지 경우로 나눌 수 있습니다:\n",
    "\n",
    "| 실제 / 예측 | Positive (양성) | Negative (음성) |\n",
    "|--------------|------------------|------------------|\n",
    "| **Positive** (양성) | **TP** (True Positive)<br>맞게 예측 | **FN** (False Negative)<br>암인데 정상이라 예측 |\n",
    "| **Negative** (음성) | **FP** (False Positive)<br>정상인데 암이라 예측 | **TN** (True Negative)<br>맞게 예측 |\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### 📐 Accuracy 수식\n",
    "\n",
    "**이진 분류일 때의 정확도는 다음과 같이 계산됩니다:**\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "- TP: 양성을 양성으로 맞힌 경우  \n",
    "- TN: 음성을 음성으로 맞힌 경우  \n",
    "- FP: 음성을 양성으로 틀리게 예측  \n",
    "- FN: 양성을 음성으로 틀리게 예측\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### 🎯 예시: 암 진단 모델\n",
    "\n",
    "병원에서 100명의 환자에 대해 모델을 테스트한 결과:\n",
    "\n",
    "- TP = 1 (실제 암 환자를 암이라고 예측함)  \n",
    "- TN = 90 (실제 정상인을 정상이라고 예측함)  \n",
    "- FP = 1 (실제 정상인을 암이라고 예측함)  \n",
    "- FN = 8 (실제 암 환자를 정상이라고 예측함)\n",
    "\n",
    "##### 👉 Accuracy 계산:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} = \\frac{1 + 90}{1 + 90 + 1 + 8} = \\frac{91}{100} = 91\\%\n",
    "$$\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "### ⚠️ 문제점\n",
    "\n",
    "- **정확도는 91%로 매우 높아 보이지만**, 실제로는 **암 환자 9명 중 8명을 놓쳤습니다.**\n",
    "- 중요한 질병을 놓치는 건 **치명적인 오류**일 수 있으므로, **정확도만 믿고 판단하면 위험**합니다.\n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n",
    "\n",
    "#### 💡 그래서 Precision, Recall 등의 다른 Metric이 필요합니다!\n",
    "\n",
    "정확도 외에도 다음과 같은 지표들을 함께 고려해야 합니다:\n",
    "- **Precision (정밀도)**: 예측한 것 중 진짜 맞춘 비율  \n",
    "    $$ \\text{Precision} = \\frac{TP}{TP + FP} $$  \n",
    "\n",
    "- **Recall (재현율)**: 실제 양성 중에서 모델이 찾아낸 비율  \n",
    "    $$ \\text{Recall} = \\frac{TP}{TP + FN} $$  \n",
    "\n",
    "- **F1 Score**: Precision과 Recall의 조화 평균  \n",
    "$$\n",
    "F_{\\beta} = (1 + \\beta^2) \\cdot \\frac{ \\text{Precision} \\cdot \\text{Recall} }{ \\beta^2 \\cdot \\text{Precision} + \\text{Recall} }\n",
    "$$\n",
    "\n",
    "\n",
    "이게 정확한 공식이며, β(베타)는 Precision과 Recall 중 어떤 쪽을 더 중요하게 볼 것인가를 조절하는 하이퍼파라미터입니다.\n",
    "\n",
    "- β = 1: F1 점수 (Precision과 Recall을 동일하게 반영)  \n",
    "\n",
    "- β > 1: Recall을 더 강조  \n",
    "\n",
    "- β < 1: Precision을 더 강조  \n",
    "\n",
    "필요하다면 그래프나 예제도 함께 정리해드릴 수 있어요!  \n",
    "\n",
    "<hr style=\"opacity:0.2;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
