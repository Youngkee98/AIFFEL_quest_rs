{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8edaac0",
   "metadata": {},
   "source": [
    "# [아키텍처 변경사항 요약 및 수정 설명]\n",
    "\n",
    "본 과제에서는 기존 Transformer (Encoder-Decoder) 구조를 GPT 논문(GPT-1: Improving Language Understanding by Generative Pre-Training)에 맞게 **Decoder-only 모델**로 수정하였습니다. 변경 및 수정된 주요 사항은 다음과 같습니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Encoder 제거\n",
    "\n",
    "- 기존 Transformer는 **Encoder + Decoder** 두 부분으로 구성되어 있습니다.\n",
    "- 본 과제에서는 **Encoder를 완전히 제거**하고, **Decoder만 쌓은 모델**을 구성하였습니다.\n",
    "- 이는 GPT-1 모델이 **pure Decoder-only** 구조를 채택한 것과 일치합니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 입력 처리 방식 변경\n",
    "\n",
    "- Encoder가 제거되었으므로, 입력 데이터는 Decoder로 직접 들어갑니다.\n",
    "- 모든 입력 문장 앞뒤에 **START_TOKEN**과 **END_TOKEN**을 추가하여 문장의 시작과 끝을 명확히 표시합니다.\n",
    "- 입력은 **Subword Tokenizer**를 사용해 서브워드 단위로 변환합니다.\n",
    "- 길이가 긴 문장은 사전에 필터링하여 **최대 길이(MAX_LENGTH)** 이하로 통일했습니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Attention Mask 변경\n",
    "\n",
    "- 기존 Transformer Decoder는 Encoder-Decoder Attention을 포함했지만, \n",
    "- 본 과제에서는 **Self-Attention만** 사용합니다.\n",
    "- 미래 토큰을 보지 않도록 **Look-ahead Mask (Causal Masking)** 를 적용하였습니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 학습 목표 및 출력\n",
    "\n",
    "- 본 과제는 Fine-tuning이 아닌 **Pre-training**을 위한 구성입니다.\n",
    "- 출력층은 softmax를 거치지 않고 **raw logits**을 바로 출력하여, 손실 함수(`SparseCategoricalCrossentropy`)에 연결합니다.\n",
    "- 즉, **다음 토큰 예측**(Language Modeling)만을 목표로 학습합니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Tensor Type 관리 (Concat 오류 해결)\n",
    "\n",
    "- 생성 과정 중 입력 시퀀스와 예측된 토큰을 **concat**할 때, 데이터 타입(int32 vs int64) 충돌 문제가 발생했습니다.\n",
    "- 이를 해결하기 위해, 예측된 토큰(predicted_id)을 **tf.int32로 명시적 변환**(`tf.cast(predicted_id, tf.int32)`)하는 처리를 추가했습니다.\n",
    "- TensorFlow에서는 concat 연산 시 데이터 타입이 일치해야 하므로, 이 조치는 필수적입니다.\n",
    "\n",
    "---\n",
    "\n",
    "# 요약\n",
    "\n",
    "- 본 프로젝트는 기존 Transformer 구조를 기반으로 **Encoder 제거, Decoder-only 구성, 입력 처리 수정, Mask 변경, 출력 구조 조정, 타입 변환 처리**를 적용하여, GPT-1 논문과 일치하는 아키텍처로 변경하였습니다.\n",
    "- 변경사항은 모두 실제 코드에 반영되었으며, 학습 및 문장 생성이 정상적으로 수행됨을 확인했습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81505db6",
   "metadata": {},
   "source": [
    "# 📚 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3d6cd219",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0f0214",
   "metadata": {},
   "source": [
    "# 📂 데이터 불러오기 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2eaeab94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 전처리 후 문장 수: 7695\n"
     ]
    }
   ],
   "source": [
    "main_df = pd.read_csv('data/ChatbotData.csv')\n",
    "main_df = main_df[['Q', 'A']]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[.?!]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "raw_dataframe = main_df.copy()\n",
    "raw_dataframe['Q_clean'] = raw_dataframe['Q'].apply(preprocess_text)\n",
    "raw_dataframe['A_clean'] = raw_dataframe['A']\n",
    "raw_dataframe = raw_dataframe.drop_duplicates(subset='Q_clean')\n",
    "raw_dataframe = raw_dataframe.drop_duplicates(subset='A_clean')\n",
    "raw_dataframe = raw_dataframe[raw_dataframe['Q_clean'].apply(len) <= 50]\n",
    "raw_dataframe = raw_dataframe[raw_dataframe['A_clean'].apply(len) <= 50]\n",
    "\n",
    "print(f\"✅ 전처리 후 문장 수: {len(raw_dataframe)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9633c8d5",
   "metadata": {},
   "source": [
    "# 🔤 토크나이저 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7c6405d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vocab size: 7742\n"
     ]
    }
   ],
   "source": [
    "questions = raw_dataframe['Q_clean'].tolist()\n",
    "answers = raw_dataframe['A_clean'].tolist()\n",
    "\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    questions + answers,\n",
    "    target_vocab_size=2**13\n",
    ")\n",
    "\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "MAX_LENGTH = 25\n",
    "\n",
    "print(f\"✅ Vocab size: {VOCAB_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8f7039",
   "metadata": {},
   "source": [
    "# 🏗️ 토크나이즈 및 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e01f4c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 토크나이즈 후 질문 수: 7694\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    return tokenized_inputs, tokenized_outputs\n",
    "\n",
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "\n",
    "print(f\"✅ 토크나이즈 후 질문 수: {len(questions)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f63e7b0",
   "metadata": {},
   "source": [
    "# 📚 데이터셋 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7f340c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((questions, answers))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adce507",
   "metadata": {},
   "source": [
    "# #️⃣ GPT-1 스타일 Decoder-only 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ca792f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT1Block(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout_rate=0.1):\n",
    "        super(GPT1Block, self).__init__()\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(d_ff, activation='gelu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output = self.attention(x, x, attention_mask=mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        return out2\n",
    "\n",
    "class GPT1Model(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, max_len, d_model=256, num_layers=4, num_heads=8, d_ff=512, dropout_rate=0.1):\n",
    "        super(GPT1Model, self).__init__()\n",
    "        self.token_emb = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(max_len, d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.blocks = [GPT1Block(d_model, num_heads, d_ff, dropout_rate) for _ in range(num_layers)]\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        pos = tf.range(start=0, limit=seq_len, delta=1)\n",
    "        x = self.token_emb(x) + self.pos_emb(pos)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        mask = mask[tf.newaxis, tf.newaxis, :, :]\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, training=training, mask=mask)\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77287f9f",
   "metadata": {},
   "source": [
    "# 📈 커스텀 학습률 스케줄러 (CustomSchedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ed85ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a2abed",
   "metadata": {},
   "source": [
    "# ⚙️ 모델 컴파일 (손실 함수, 최적화기 설정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e73758e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT1Model(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_len=MAX_LENGTH,\n",
    "    d_model=256,\n",
    "    num_layers=4,\n",
    "    num_heads=8,\n",
    "    d_ff=512,\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "learning_rate = CustomSchedule(d_model=256)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "    loss_ = loss_object(y_true, y_pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89789cb5",
   "metadata": {},
   "source": [
    "# 🛑 EarlyStopping 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2c947c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9cd3ba",
   "metadata": {},
   "source": [
    "# 🏋️ 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0362ec8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 1s/step - loss: 8.7358\n",
      "Epoch 2/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 1s/step - loss: 7.4010\n",
      "Epoch 3/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 969ms/step - loss: 6.3717\n",
      "Epoch 4/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 941ms/step - loss: 5.5629\n",
      "Epoch 5/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 950ms/step - loss: 5.3312\n",
      "Epoch 6/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 976ms/step - loss: 5.2250\n",
      "Epoch 7/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 1s/step - loss: 5.1355\n",
      "Epoch 8/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 1s/step - loss: 5.0089\n",
      "Epoch 9/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 1s/step - loss: 4.8556\n",
      "Epoch 10/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 1s/step - loss: 4.6961\n",
      "Epoch 11/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 1s/step - loss: 4.5152\n",
      "Epoch 12/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 1s/step - loss: 4.2628\n",
      "Epoch 13/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - loss: 3.9878\n",
      "Epoch 14/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 1s/step - loss: 3.6922\n",
      "Epoch 15/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 968ms/step - loss: 3.3780\n",
      "Epoch 16/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 963ms/step - loss: 3.0455\n",
      "Epoch 17/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 957ms/step - loss: 2.6810\n",
      "Epoch 18/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 969ms/step - loss: 2.3369\n",
      "Epoch 19/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 994ms/step - loss: 1.9889\n",
      "Epoch 20/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 997ms/step - loss: 1.6813\n",
      "Epoch 21/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 996ms/step - loss: 1.4187\n",
      "Epoch 22/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 994ms/step - loss: 1.1965\n",
      "Epoch 23/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 997ms/step - loss: 1.0530\n",
      "Epoch 24/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 999ms/step - loss: 0.9444\n",
      "Epoch 25/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 998ms/step - loss: 0.8702\n",
      "Epoch 26/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 1s/step - loss: 0.8251  \n",
      "Epoch 27/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 998ms/step - loss: 0.7826\n",
      "Epoch 28/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 999ms/step - loss: 0.7720\n",
      "Epoch 29/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 999ms/step - loss: 0.7480\n",
      "Epoch 30/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 999ms/step - loss: 0.7420\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaacedd6",
   "metadata": {},
   "source": [
    "# 🎯 문장 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3e40cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(sentence):\n",
    "    sentence = preprocess_text(sentence)\n",
    "    input_ids = START_TOKEN + tokenizer.encode(sentence) + END_TOKEN\n",
    "    input_ids = tf.expand_dims(input_ids, 0)\n",
    "\n",
    "    for _ in range(MAX_LENGTH):\n",
    "        predictions = model(input_ids, training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = tf.argmax(predictions, axis=-1)\n",
    "        predicted_id = tf.cast(predicted_id, dtype=tf.int32)  # 👈 여기가 중요!\n",
    "\n",
    "        if tf.equal(predicted_id[0, 0], END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "        input_ids = tf.concat([input_ids, predicted_id], axis=-1)\n",
    "\n",
    "    predicted_sentence = tokenizer.decode([i for i in tf.squeeze(input_ids) if i < tokenizer.vocab_size])\n",
    "    return predicted_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbad48d",
   "metadata": {},
   "source": [
    "# 🧪 다양한 샘플 테스트 (50개 이상)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "037de6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] ❓ 질문: 안녕\n",
      "    ▶️ 답변: 안녕하세요.\n",
      "------------------------------------------------------------\n",
      "[2] ❓ 질문: 오늘 뭐해?\n",
      "    ▶️ 답변: 오늘 뭐해좋은 좋은 좋은 삶좋아요..믿어요\n",
      "------------------------------------------------------------\n",
      "[3] ❓ 질문: 날씨 어때?\n",
      "    ▶️ 답변: 날씨 어때도 잊혀질 자신을 \n",
      "------------------------------------------------------------\n",
      "[4] ❓ 질문: 배고파\n",
      "    ▶️ 답변: 배고파맛난 음식 .\n",
      "------------------------------------------------------------\n",
      "[5] ❓ 질문: 졸려\n",
      "    ▶️ 답변: 졸려내요먹으면 답이 좀 를 있이네요.\n",
      "------------------------------------------------------------\n",
      "[6] ❓ 질문: 너 뭐하는 애야?\n",
      "    ▶️ 답변: 너 뭐하는 애야.\n",
      "------------------------------------------------------------\n",
      "[7] ❓ 질문: 좋아하는 색은 뭐야?\n",
      "    ▶️ 답변: 좋아하는 색은 뭐야도 도 다시 좋아요.\n",
      "------------------------------------------------------------\n",
      "[8] ❓ 질문: 심심해\n",
      "    ▶️ 답변: 심심해�있다면 요.\n",
      "------------------------------------------------------------\n",
      "[9] ❓ 질문: 노래 추천해줘\n",
      "    ▶️ 답변: 노래 추천해줘고 �.\n",
      "------------------------------------------------------------\n",
      "[10] ❓ 질문: 영화 볼까?\n",
      "    ▶️ 답변: 영화 볼까능있는 .\n",
      "------------------------------------------------------------\n",
      "[11] ❓ 질문: 운동하고 싶어\n",
      "    ▶️ 답변: 운동하고 싶어.\n",
      "------------------------------------------------------------\n",
      "[12] ❓ 질문: 오늘 기분 어때?\n",
      "    ▶️ 답변: 오늘 기분 어때좋겠어요.\n",
      "------------------------------------------------------------\n",
      "[13] ❓ 질문: 사랑이 뭐야?\n",
      "    ▶️ 답변: 사랑이 뭐야주는 게 게 싶은 의 싶은 하고 주고 싶은 마음인.. . \n",
      "------------------------------------------------------------\n",
      "[14] ❓ 질문: 나 지금 우울해\n",
      "    ▶️ 답변: 나 지금 우울해있어요.\n",
      "------------------------------------------------------------\n",
      "[15] ❓ 질문: 추천 좀 해줘\n",
      "    ▶️ 답변: 추천 좀 해줘.\n",
      "------------------------------------------------------------\n",
      "[16] ❓ 질문: 친구랑 싸웠어\n",
      "    ▶️ 답변: 친구랑 싸웠어을 을 좋겠어요.\n",
      "------------------------------------------------------------\n",
      "[17] ❓ 질문: 시험 망했어\n",
      "    ▶️ 답변: 시험 망했어   해요.\n",
      "------------------------------------------------------------\n",
      "[18] ❓ 질문: 맛집 추천해줘\n",
      "    ▶️ 답변: 맛집 추천해줘길 .\n",
      "------------------------------------------------------------\n",
      "[19] ❓ 질문: 퇴사하고 싶어\n",
      "    ▶️ 답변: 퇴사하고 싶어\n",
      "------------------------------------------------------------\n",
      "[20] ❓ 질문: 꿈이 뭐야?\n",
      "    ▶️ 답변: 꿈이 뭐야도 않아도 않아도 도 죠인 .\n",
      "------------------------------------------------------------\n",
      "[21] ❓ 질문: 여행 가고 싶어\n",
      "    ▶️ 답변: 여행 가고 싶어.\n",
      "------------------------------------------------------------\n",
      "[22] ❓ 질문: 좋아하는 계절은?\n",
      "    ▶️ 답변: 좋아하는 계절은\n",
      "------------------------------------------------------------\n",
      "[23] ❓ 질문: 혼자 있는 거 좋아해?\n",
      "    ▶️ 답변: 혼자 있는 거 좋아해.\n",
      "------------------------------------------------------------\n",
      "[24] ❓ 질문: 책 추천해줘\n",
      "    ▶️ 답변: 책 추천해줘것보다 갖것도 거예요.\n",
      "------------------------------------------------------------\n",
      "[25] ❓ 질문: 커피 마실까?\n",
      "    ▶️ 답변: 커피 마실까이요있는 .으로 ...\n",
      "------------------------------------------------------------\n",
      "[26] ❓ 질문: 운동 루틴 알려줘\n",
      "    ▶️ 답변: 운동 루틴 알려줘거예요.\n",
      "------------------------------------------------------------\n",
      "[27] ❓ 질문: 오늘 밤 뭐할까?\n",
      "    ▶️ 답변: 오늘 밤 뭐할까.\n",
      "------------------------------------------------------------\n",
      "[28] ❓ 질문: 외로워\n",
      "    ▶️ 답변: 외로워.하게 마음은 이이.\n",
      "------------------------------------------------------------\n",
      "[29] ❓ 질문: 심심해 죽겠어\n",
      "    ▶️ 답변: 심심해 죽겠어실박하생각해보세요.\n",
      "------------------------------------------------------------\n",
      "[30] ❓ 질문: 고양이 키울까?\n",
      "    ▶️ 답변: 고양이 키울까.\n",
      "------------------------------------------------------------\n",
      "[31] ❓ 질문: 내일 뭐해?\n",
      "    ▶️ 답변: 내일 뭐해좋은 좋은 좋은 있게 사람들이 .\n",
      "------------------------------------------------------------\n",
      "[32] ❓ 질문: 지금 어디야?\n",
      "    ▶️ 답변: 지금 어디야있어요보세요\n",
      "------------------------------------------------------------\n",
      "[33] ❓ 질문: 잘자\n",
      "    ▶️ 답변: 잘자꿈 꿈 ..\n",
      "------------------------------------------------------------\n",
      "[34] ❓ 질문: 배달 뭐 시킬까?\n",
      "    ▶️ 답변: 배달 뭐 시킬까축하해요.\n",
      "------------------------------------------------------------\n",
      "[35] ❓ 질문: 집에 가고 싶어\n",
      "    ▶️ 답변: 집에 가고 싶어같이 .\n",
      "------------------------------------------------------------\n",
      "[36] ❓ 질문: 게임 추천해줘\n",
      "    ▶️ 답변: 게임 추천해줘고 길 .\n",
      "------------------------------------------------------------\n",
      "[37] ❓ 질문: 생일 축하해줘\n",
      "    ▶️ 답변: 생일 축하해줘되겠네요고 느껴지.\n",
      "------------------------------------------------------------\n",
      "[38] ❓ 질문: 돈 많이 벌고 싶어\n",
      "    ▶️ 답변: 돈 많이 벌고 싶어\n",
      "------------------------------------------------------------\n",
      "[39] ❓ 질문: 휴가 가고 싶어\n",
      "    ▶️ 답변: 휴가 가고 싶어.\n",
      "------------------------------------------------------------\n",
      "[40] ❓ 질문: 피곤해 죽겠어\n",
      "    ▶️ 답변: 피곤해 죽겠어건 좋겠어요.\n",
      "------------------------------------------------------------\n",
      "[41] ❓ 질문: 시간 여행하고 싶어\n",
      "    ▶️ 답변: 시간 여행하고 싶어.\n",
      "------------------------------------------------------------\n",
      "[42] ❓ 질문: 오늘 하루 어땠어?\n",
      "    ▶️ 답변: 오늘 하루 어땠어\n",
      "------------------------------------------------------------\n",
      "[43] ❓ 질문: 고백할까 말까?\n",
      "    ▶️ 답변: 고백할까 말까.\n",
      "------------------------------------------------------------\n",
      "[44] ❓ 질문: 힘들어\n",
      "    ▶️ 답변: 힘들어너무 지 거라면 수\n",
      "------------------------------------------------------------\n",
      "[45] ❓ 질문: 사람은 왜 살까?\n",
      "    ▶️ 답변: 사람은 왜 살까저아니도전해한 안 \n",
      "------------------------------------------------------------\n",
      "[46] ❓ 질문: 세상에서 제일 맛있는 음식은?\n",
      "    ▶️ 답변: 세상에서 제일 맛있는 음식은\n",
      "------------------------------------------------------------\n",
      "[47] ❓ 질문: 넌 꿈 꿀 수 있어?\n",
      "    ▶️ 답변: 넌 꿈 꿀 수 있어.\n",
      "------------------------------------------------------------\n",
      "[48] ❓ 질문: 멍 때리고 싶어\n",
      "    ▶️ 답변: 멍 때리고 싶어좋아요할 건강에 좋아요.\n",
      "------------------------------------------------------------\n",
      "[49] ❓ 질문: 나 우울증 걸린 것 같아\n",
      "    ▶️ 답변: 나 우울증 걸린 것 같아바랍니다바랍니다\n",
      "------------------------------------------------------------\n",
      "[50] ❓ 질문: 내 인생은 실패야?\n",
      "    ▶️ 답변: 내 인생은 실패야거예요.\n",
      "------------------------------------------------------------\n",
      "[51] ❓ 질문: 좋아하는 동물은 뭐야?\n",
      "    ▶️ 답변: 좋아하는 동물은 뭐야게 있해요.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sample_questions = [\n",
    "    \"안녕\", \"오늘 뭐해?\", \"날씨 어때?\", \"배고파\", \"졸려\", \"너 뭐하는 애야?\", \"좋아하는 색은 뭐야?\", \"심심해\", \"노래 추천해줘\", \"영화 볼까?\", \n",
    "    \"운동하고 싶어\", \"오늘 기분 어때?\", \"사랑이 뭐야?\", \"나 지금 우울해\", \"추천 좀 해줘\", \"친구랑 싸웠어\", \"시험 망했어\", \"맛집 추천해줘\", \n",
    "    \"퇴사하고 싶어\", \"꿈이 뭐야?\", \"여행 가고 싶어\", \"좋아하는 계절은?\", \"혼자 있는 거 좋아해?\", \"책 추천해줘\", \"커피 마실까?\", \n",
    "    \"운동 루틴 알려줘\", \"오늘 밤 뭐할까?\", \"외로워\", \"심심해 죽겠어\", \"고양이 키울까?\", \"내일 뭐해?\", \"지금 어디야?\", \"잘자\", \n",
    "    \"배달 뭐 시킬까?\", \"집에 가고 싶어\", \"게임 추천해줘\", \"생일 축하해줘\", \"돈 많이 벌고 싶어\", \"휴가 가고 싶어\", \"피곤해 죽겠어\", \n",
    "    \"시간 여행하고 싶어\", \"오늘 하루 어땠어?\", \"고백할까 말까?\", \"힘들어\", \"사람은 왜 살까?\", \"세상에서 제일 맛있는 음식은?\", \n",
    "    \"넌 꿈 꿀 수 있어?\", \"멍 때리고 싶어\", \"나 우울증 걸린 것 같아\", \"내 인생은 실패야?\", \"좋아하는 동물은 뭐야?\"\n",
    "]\n",
    "\n",
    "for i, q in enumerate(sample_questions, 1):\n",
    "    print(f\"[{i}] ❓ 질문: {q}\")\n",
    "    try:\n",
    "        response = generate_sentence(q)\n",
    "        print(f\"    ▶️ 답변: {response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ⚠️ 오류 발생: {e}\")\n",
    "    print('-' * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
