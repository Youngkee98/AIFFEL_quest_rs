{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81505db6",
   "metadata": {},
   "source": [
    "# ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6cd219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0f0214",
   "metadata": {},
   "source": [
    "# ğŸ“‚ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaeab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.read_csv('data/ChatbotData.csv')\n",
    "main_df = main_df[['Q', 'A']]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[.?!]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "raw_dataframe = main_df.copy()\n",
    "raw_dataframe['Q_clean'] = raw_dataframe['Q'].apply(preprocess_text)\n",
    "raw_dataframe['A_clean'] = raw_dataframe['A']\n",
    "raw_dataframe = raw_dataframe.drop_duplicates(subset='Q_clean')\n",
    "raw_dataframe = raw_dataframe.drop_duplicates(subset='A_clean')\n",
    "raw_dataframe = raw_dataframe[raw_dataframe['Q_clean'].apply(len) <= 50]\n",
    "raw_dataframe = raw_dataframe[raw_dataframe['A_clean'].apply(len) <= 50]\n",
    "\n",
    "print(f\"âœ… ì „ì²˜ë¦¬ í›„ ë¬¸ì¥ ìˆ˜: {len(raw_dataframe)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9633c8d5",
   "metadata": {},
   "source": [
    "# ğŸ”¤ í† í¬ë‚˜ì´ì € í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6405d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = raw_dataframe['Q_clean'].tolist()\n",
    "answers = raw_dataframe['A_clean'].tolist()\n",
    "\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    questions + answers,\n",
    "    target_vocab_size=2**13\n",
    ")\n",
    "\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "MAX_LENGTH = 25\n",
    "\n",
    "print(f\"âœ… Vocab size: {VOCAB_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8f7039",
   "metadata": {},
   "source": [
    "# ğŸ—ï¸ í† í¬ë‚˜ì´ì¦ˆ ë° íŒ¨ë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f4c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    return tokenized_inputs, tokenized_outputs\n",
    "\n",
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "\n",
    "print(f\"âœ… í† í¬ë‚˜ì´ì¦ˆ í›„ ì§ˆë¬¸ ìˆ˜: {len(questions)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f63e7b0",
   "metadata": {},
   "source": [
    "# ğŸ“š ë°ì´í„°ì…‹ êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f340c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((questions, answers))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adce507",
   "metadata": {},
   "source": [
    "# #ï¸âƒ£ GPT-1 ìŠ¤íƒ€ì¼ Decoder-only ëª¨ë¸ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca792f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT1Block(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout_rate=0.1):\n",
    "        super(GPT1Block, self).__init__()\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(d_ff, activation='gelu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output = self.attention(x, x, attention_mask=mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        return out2\n",
    "\n",
    "class GPT1Model(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, max_len, d_model=256, num_layers=4, num_heads=8, d_ff=512, dropout_rate=0.1):\n",
    "        super(GPT1Model, self).__init__()\n",
    "        self.token_emb = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(max_len, d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.blocks = [GPT1Block(d_model, num_heads, d_ff, dropout_rate) for _ in range(num_layers)]\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        pos = tf.range(start=0, limit=seq_len, delta=1)\n",
    "        x = self.token_emb(x) + self.pos_emb(pos)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        mask = mask[tf.newaxis, tf.newaxis, :, :]\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, training, mask)\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77287f9f",
   "metadata": {},
   "source": [
    "# ğŸ“ˆ ì»¤ìŠ¤í…€ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ (CustomSchedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed85ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a2abed",
   "metadata": {},
   "source": [
    "# âš™ï¸ ëª¨ë¸ ì»´íŒŒì¼ (ì†ì‹¤ í•¨ìˆ˜, ìµœì í™”ê¸° ì„¤ì •)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73758e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model=256)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "    loss_ = loss_object(y_true, y_pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89789cb5",
   "metadata": {},
   "source": [
    "# ğŸ›‘ EarlyStopping ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c947c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9cd3ba",
   "metadata": {},
   "source": [
    "# ğŸ‹ï¸ ëª¨ë¸ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0362ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaacedd6",
   "metadata": {},
   "source": [
    "# ğŸ¯ ë¬¸ì¥ ìƒì„± í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e40cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(sentence):\n",
    "    sentence = preprocess_text(sentence)\n",
    "    input_ids = START_TOKEN + tokenizer.encode(sentence) + END_TOKEN\n",
    "    input_ids = tf.expand_dims(input_ids, 0)\n",
    "\n",
    "    for _ in range(MAX_LENGTH):\n",
    "        predictions = model(input_ids, training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "        if tf.equal(predicted_id[0, 0], END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "        input_ids = tf.concat([input_ids, predicted_id], axis=-1)\n",
    "\n",
    "    predicted_sentence = tokenizer.decode([i for i in tf.squeeze(input_ids) if i < tokenizer.vocab_size])\n",
    "    return predicted_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbad48d",
   "metadata": {},
   "source": [
    "# ğŸ§ª ë‹¤ì–‘í•œ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ (50ê°œ ì´ìƒ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037de6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_questions = [\n",
    "    \"ì•ˆë…•\", \"ì˜¤ëŠ˜ ë­í•´?\", \"ë‚ ì”¨ ì–´ë•Œ?\", \"ë°°ê³ íŒŒ\", \"ì¡¸ë ¤\", \"ë„ˆ ë­í•˜ëŠ” ì• ì•¼?\", \"ì¢‹ì•„í•˜ëŠ” ìƒ‰ì€ ë­ì•¼?\", \"ì‹¬ì‹¬í•´\", \"ë…¸ë˜ ì¶”ì²œí•´ì¤˜\", \"ì˜í™” ë³¼ê¹Œ?\", \n",
    "    \"ìš´ë™í•˜ê³  ì‹¶ì–´\", \"ì˜¤ëŠ˜ ê¸°ë¶„ ì–´ë•Œ?\", \"ì‚¬ë‘ì´ ë­ì•¼?\", \"ë‚˜ ì§€ê¸ˆ ìš°ìš¸í•´\", \"ì¶”ì²œ ì¢€ í•´ì¤˜\", \"ì¹œêµ¬ë‘ ì‹¸ì› ì–´\", \"ì‹œí—˜ ë§í–ˆì–´\", \"ë§›ì§‘ ì¶”ì²œí•´ì¤˜\", \n",
    "    \"í‡´ì‚¬í•˜ê³  ì‹¶ì–´\", \"ê¿ˆì´ ë­ì•¼?\", \"ì—¬í–‰ ê°€ê³  ì‹¶ì–´\", \"ì¢‹ì•„í•˜ëŠ” ê³„ì ˆì€?\", \"í˜¼ì ìˆëŠ” ê±° ì¢‹ì•„í•´?\", \"ì±… ì¶”ì²œí•´ì¤˜\", \"ì»¤í”¼ ë§ˆì‹¤ê¹Œ?\", \n",
    "    \"ìš´ë™ ë£¨í‹´ ì•Œë ¤ì¤˜\", \"ì˜¤ëŠ˜ ë°¤ ë­í• ê¹Œ?\", \"ì™¸ë¡œì›Œ\", \"ì‹¬ì‹¬í•´ ì£½ê² ì–´\", \"ê³ ì–‘ì´ í‚¤ìš¸ê¹Œ?\", \"ë‚´ì¼ ë­í•´?\", \"ì§€ê¸ˆ ì–´ë””ì•¼?\", \"ì˜ì\", \n",
    "    \"ë°°ë‹¬ ë­ ì‹œí‚¬ê¹Œ?\", \"ì§‘ì— ê°€ê³  ì‹¶ì–´\", \"ê²Œì„ ì¶”ì²œí•´ì¤˜\", \"ìƒì¼ ì¶•í•˜í•´ì¤˜\", \"ëˆ ë§ì´ ë²Œê³  ì‹¶ì–´\", \"íœ´ê°€ ê°€ê³  ì‹¶ì–´\", \"í”¼ê³¤í•´ ì£½ê² ì–´\", \n",
    "    \"ì‹œê°„ ì—¬í–‰í•˜ê³  ì‹¶ì–´\", \"ì˜¤ëŠ˜ í•˜ë£¨ ì–´ë• ì–´?\", \"ê³ ë°±í• ê¹Œ ë§ê¹Œ?\", \"í˜ë“¤ì–´\", \"ì‚¬ëŒì€ ì™œ ì‚´ê¹Œ?\", \"ì„¸ìƒì—ì„œ ì œì¼ ë§›ìˆëŠ” ìŒì‹ì€?\", \n",
    "    \"ë„Œ ê¿ˆ ê¿€ ìˆ˜ ìˆì–´?\", \"ë© ë•Œë¦¬ê³  ì‹¶ì–´\", \"ë‚˜ ìš°ìš¸ì¦ ê±¸ë¦° ê²ƒ ê°™ì•„\", \"ë‚´ ì¸ìƒì€ ì‹¤íŒ¨ì•¼?\", \"ì¢‹ì•„í•˜ëŠ” ë™ë¬¼ì€ ë­ì•¼?\"\n",
    "]\n",
    "\n",
    "for i, q in enumerate(sample_questions, 1):\n",
    "    print(f\"[{i}] â“ ì§ˆë¬¸: {q}\")\n",
    "    try:\n",
    "        response = generate_sentence(q)\n",
    "        print(f\"    â–¶ï¸ ë‹µë³€: {response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    print('-' * 60)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
