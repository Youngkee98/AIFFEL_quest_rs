{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81505db6",
   "metadata": {},
   "source": [
    "# 📚 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6cd219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0f0214",
   "metadata": {},
   "source": [
    "# 📂 데이터 불러오기 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaeab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.read_csv('data/ChatbotData.csv')\n",
    "main_df = main_df[['Q', 'A']]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[.?!]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "raw_dataframe = main_df.copy()\n",
    "raw_dataframe['Q_clean'] = raw_dataframe['Q'].apply(preprocess_text)\n",
    "raw_dataframe['A_clean'] = raw_dataframe['A']\n",
    "raw_dataframe = raw_dataframe.drop_duplicates(subset='Q_clean')\n",
    "raw_dataframe = raw_dataframe.drop_duplicates(subset='A_clean')\n",
    "raw_dataframe = raw_dataframe[raw_dataframe['Q_clean'].apply(len) <= 50]\n",
    "raw_dataframe = raw_dataframe[raw_dataframe['A_clean'].apply(len) <= 50]\n",
    "\n",
    "print(f\"✅ 전처리 후 문장 수: {len(raw_dataframe)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9633c8d5",
   "metadata": {},
   "source": [
    "# 🔤 토크나이저 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6405d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = raw_dataframe['Q_clean'].tolist()\n",
    "answers = raw_dataframe['A_clean'].tolist()\n",
    "\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    questions + answers,\n",
    "    target_vocab_size=2**13\n",
    ")\n",
    "\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "MAX_LENGTH = 25\n",
    "\n",
    "print(f\"✅ Vocab size: {VOCAB_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8f7039",
   "metadata": {},
   "source": [
    "# 🏗️ 토크나이즈 및 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f4c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    return tokenized_inputs, tokenized_outputs\n",
    "\n",
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "\n",
    "print(f\"✅ 토크나이즈 후 질문 수: {len(questions)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f63e7b0",
   "metadata": {},
   "source": [
    "# 📚 데이터셋 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f340c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((questions, answers))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adce507",
   "metadata": {},
   "source": [
    "# #️⃣ GPT-1 스타일 Decoder-only 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca792f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT1Block(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout_rate=0.1):\n",
    "        super(GPT1Block, self).__init__()\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(d_ff, activation='gelu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output = self.attention(x, x, attention_mask=mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        return out2\n",
    "\n",
    "class GPT1Model(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, max_len, d_model=256, num_layers=4, num_heads=8, d_ff=512, dropout_rate=0.1):\n",
    "        super(GPT1Model, self).__init__()\n",
    "        self.token_emb = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(max_len, d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.blocks = [GPT1Block(d_model, num_heads, d_ff, dropout_rate) for _ in range(num_layers)]\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        pos = tf.range(start=0, limit=seq_len, delta=1)\n",
    "        x = self.token_emb(x) + self.pos_emb(pos)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        mask = mask[tf.newaxis, tf.newaxis, :, :]\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, training, mask)\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77287f9f",
   "metadata": {},
   "source": [
    "# 📈 커스텀 학습률 스케줄러 (CustomSchedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed85ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a2abed",
   "metadata": {},
   "source": [
    "# ⚙️ 모델 컴파일 (손실 함수, 최적화기 설정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73758e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model=256)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "    loss_ = loss_object(y_true, y_pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89789cb5",
   "metadata": {},
   "source": [
    "# 🛑 EarlyStopping 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c947c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9cd3ba",
   "metadata": {},
   "source": [
    "# 🏋️ 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0362ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaacedd6",
   "metadata": {},
   "source": [
    "# 🎯 문장 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e40cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(sentence):\n",
    "    sentence = preprocess_text(sentence)\n",
    "    input_ids = START_TOKEN + tokenizer.encode(sentence) + END_TOKEN\n",
    "    input_ids = tf.expand_dims(input_ids, 0)\n",
    "\n",
    "    for _ in range(MAX_LENGTH):\n",
    "        predictions = model(input_ids, training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "        if tf.equal(predicted_id[0, 0], END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "        input_ids = tf.concat([input_ids, predicted_id], axis=-1)\n",
    "\n",
    "    predicted_sentence = tokenizer.decode([i for i in tf.squeeze(input_ids) if i < tokenizer.vocab_size])\n",
    "    return predicted_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbad48d",
   "metadata": {},
   "source": [
    "# 🧪 다양한 샘플 테스트 (50개 이상)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037de6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_questions = [\n",
    "    \"안녕\", \"오늘 뭐해?\", \"날씨 어때?\", \"배고파\", \"졸려\", \"너 뭐하는 애야?\", \"좋아하는 색은 뭐야?\", \"심심해\", \"노래 추천해줘\", \"영화 볼까?\", \n",
    "    \"운동하고 싶어\", \"오늘 기분 어때?\", \"사랑이 뭐야?\", \"나 지금 우울해\", \"추천 좀 해줘\", \"친구랑 싸웠어\", \"시험 망했어\", \"맛집 추천해줘\", \n",
    "    \"퇴사하고 싶어\", \"꿈이 뭐야?\", \"여행 가고 싶어\", \"좋아하는 계절은?\", \"혼자 있는 거 좋아해?\", \"책 추천해줘\", \"커피 마실까?\", \n",
    "    \"운동 루틴 알려줘\", \"오늘 밤 뭐할까?\", \"외로워\", \"심심해 죽겠어\", \"고양이 키울까?\", \"내일 뭐해?\", \"지금 어디야?\", \"잘자\", \n",
    "    \"배달 뭐 시킬까?\", \"집에 가고 싶어\", \"게임 추천해줘\", \"생일 축하해줘\", \"돈 많이 벌고 싶어\", \"휴가 가고 싶어\", \"피곤해 죽겠어\", \n",
    "    \"시간 여행하고 싶어\", \"오늘 하루 어땠어?\", \"고백할까 말까?\", \"힘들어\", \"사람은 왜 살까?\", \"세상에서 제일 맛있는 음식은?\", \n",
    "    \"넌 꿈 꿀 수 있어?\", \"멍 때리고 싶어\", \"나 우울증 걸린 것 같아\", \"내 인생은 실패야?\", \"좋아하는 동물은 뭐야?\"\n",
    "]\n",
    "\n",
    "for i, q in enumerate(sample_questions, 1):\n",
    "    print(f\"[{i}] ❓ 질문: {q}\")\n",
    "    try:\n",
    "        response = generate_sentence(q)\n",
    "        print(f\"    ▶️ 답변: {response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ⚠️ 오류 발생: {e}\")\n",
    "    print('-' * 60)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
