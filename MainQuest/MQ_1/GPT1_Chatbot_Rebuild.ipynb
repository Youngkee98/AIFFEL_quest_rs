{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8edaac0",
   "metadata": {},
   "source": [
    "# [ì•„í‚¤í…ì²˜ ë³€ê²½ì‚¬í•­ ìš”ì•½ ë° ìˆ˜ì • ì„¤ëª…]\n",
    "\n",
    "ë³¸ ê³¼ì œì—ì„œëŠ” ê¸°ì¡´ Transformer (Encoder-Decoder) êµ¬ì¡°ë¥¼ GPT ë…¼ë¬¸(GPT-1: Improving Language Understanding by Generative Pre-Training)ì— ë§ê²Œ **Decoder-only ëª¨ë¸**ë¡œ ìˆ˜ì •í•˜ì˜€ìŠµë‹ˆë‹¤. ë³€ê²½ ë° ìˆ˜ì •ëœ ì£¼ìš” ì‚¬í•­ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Encoder ì œê±°\n",
    "\n",
    "- ê¸°ì¡´ TransformerëŠ” **Encoder + Decoder** ë‘ ë¶€ë¶„ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "- ë³¸ ê³¼ì œì—ì„œëŠ” **Encoderë¥¼ ì™„ì „íˆ ì œê±°**í•˜ê³ , **Decoderë§Œ ìŒ“ì€ ëª¨ë¸**ì„ êµ¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
    "- ì´ëŠ” GPT-1 ëª¨ë¸ì´ **pure Decoder-only** êµ¬ì¡°ë¥¼ ì±„íƒí•œ ê²ƒê³¼ ì¼ì¹˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. ì…ë ¥ ì²˜ë¦¬ ë°©ì‹ ë³€ê²½\n",
    "\n",
    "- Encoderê°€ ì œê±°ë˜ì—ˆìœ¼ë¯€ë¡œ, ì…ë ¥ ë°ì´í„°ëŠ” Decoderë¡œ ì§ì ‘ ë“¤ì–´ê°‘ë‹ˆë‹¤.\n",
    "- ëª¨ë“  ì…ë ¥ ë¬¸ì¥ ì•ë’¤ì— **START_TOKEN**ê³¼ **END_TOKEN**ì„ ì¶”ê°€í•˜ì—¬ ë¬¸ì¥ì˜ ì‹œì‘ê³¼ ëì„ ëª…í™•íˆ í‘œì‹œí•©ë‹ˆë‹¤.\n",
    "- ì…ë ¥ì€ **Subword Tokenizer**ë¥¼ ì‚¬ìš©í•´ ì„œë¸Œì›Œë“œ ë‹¨ìœ„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "- ê¸¸ì´ê°€ ê¸´ ë¬¸ì¥ì€ ì‚¬ì „ì— í•„í„°ë§í•˜ì—¬ **ìµœëŒ€ ê¸¸ì´(MAX_LENGTH)** ì´í•˜ë¡œ í†µì¼í–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Attention Mask ë³€ê²½\n",
    "\n",
    "- ê¸°ì¡´ Transformer DecoderëŠ” Encoder-Decoder Attentionì„ í¬í•¨í–ˆì§€ë§Œ, \n",
    "- ë³¸ ê³¼ì œì—ì„œëŠ” **Self-Attentionë§Œ** ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "- ë¯¸ë˜ í† í°ì„ ë³´ì§€ ì•Šë„ë¡ **Look-ahead Mask (Causal Masking)** ë¥¼ ì ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. í•™ìŠµ ëª©í‘œ ë° ì¶œë ¥\n",
    "\n",
    "- ë³¸ ê³¼ì œëŠ” Fine-tuningì´ ì•„ë‹Œ **Pre-training**ì„ ìœ„í•œ êµ¬ì„±ì…ë‹ˆë‹¤.\n",
    "- ì¶œë ¥ì¸µì€ softmaxë¥¼ ê±°ì¹˜ì§€ ì•Šê³  **raw logits**ì„ ë°”ë¡œ ì¶œë ¥í•˜ì—¬, ì†ì‹¤ í•¨ìˆ˜(`SparseCategoricalCrossentropy`)ì— ì—°ê²°í•©ë‹ˆë‹¤.\n",
    "- ì¦‰, **ë‹¤ìŒ í† í° ì˜ˆì¸¡**(Language Modeling)ë§Œì„ ëª©í‘œë¡œ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Tensor Type ê´€ë¦¬ (Concat ì˜¤ë¥˜ í•´ê²°)\n",
    "\n",
    "- ìƒì„± ê³¼ì • ì¤‘ ì…ë ¥ ì‹œí€€ìŠ¤ì™€ ì˜ˆì¸¡ëœ í† í°ì„ **concat**í•  ë•Œ, ë°ì´í„° íƒ€ì…(int32 vs int64) ì¶©ëŒ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n",
    "- ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì˜ˆì¸¡ëœ í† í°(predicted_id)ì„ **tf.int32ë¡œ ëª…ì‹œì  ë³€í™˜**(`tf.cast(predicted_id, tf.int32)`)í•˜ëŠ” ì²˜ë¦¬ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.\n",
    "- TensorFlowì—ì„œëŠ” concat ì—°ì‚° ì‹œ ë°ì´í„° íƒ€ì…ì´ ì¼ì¹˜í•´ì•¼ í•˜ë¯€ë¡œ, ì´ ì¡°ì¹˜ëŠ” í•„ìˆ˜ì ì…ë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "# ìš”ì•½\n",
    "\n",
    "- ë³¸ í”„ë¡œì íŠ¸ëŠ” ê¸°ì¡´ Transformer êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **Encoder ì œê±°, Decoder-only êµ¬ì„±, ì…ë ¥ ì²˜ë¦¬ ìˆ˜ì •, Mask ë³€ê²½, ì¶œë ¥ êµ¬ì¡° ì¡°ì •, íƒ€ì… ë³€í™˜ ì²˜ë¦¬**ë¥¼ ì ìš©í•˜ì—¬, GPT-1 ë…¼ë¬¸ê³¼ ì¼ì¹˜í•˜ëŠ” ì•„í‚¤í…ì²˜ë¡œ ë³€ê²½í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
    "- ë³€ê²½ì‚¬í•­ì€ ëª¨ë‘ ì‹¤ì œ ì½”ë“œì— ë°˜ì˜ë˜ì—ˆìœ¼ë©°, í•™ìŠµ ë° ë¬¸ì¥ ìƒì„±ì´ ì •ìƒì ìœ¼ë¡œ ìˆ˜í–‰ë¨ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81505db6",
   "metadata": {},
   "source": [
    "# ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3d6cd219",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0f0214",
   "metadata": {},
   "source": [
    "# ğŸ“‚ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2eaeab94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì „ì²˜ë¦¬ í›„ ë¬¸ì¥ ìˆ˜: 7695\n"
     ]
    }
   ],
   "source": [
    "main_df = pd.read_csv('data/ChatbotData.csv')\n",
    "main_df = main_df[['Q', 'A']]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[.?!]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "raw_dataframe = main_df.copy()\n",
    "raw_dataframe['Q_clean'] = raw_dataframe['Q'].apply(preprocess_text)\n",
    "raw_dataframe['A_clean'] = raw_dataframe['A']\n",
    "raw_dataframe = raw_dataframe.drop_duplicates(subset='Q_clean')\n",
    "raw_dataframe = raw_dataframe.drop_duplicates(subset='A_clean')\n",
    "raw_dataframe = raw_dataframe[raw_dataframe['Q_clean'].apply(len) <= 50]\n",
    "raw_dataframe = raw_dataframe[raw_dataframe['A_clean'].apply(len) <= 50]\n",
    "\n",
    "print(f\"âœ… ì „ì²˜ë¦¬ í›„ ë¬¸ì¥ ìˆ˜: {len(raw_dataframe)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9633c8d5",
   "metadata": {},
   "source": [
    "# ğŸ”¤ í† í¬ë‚˜ì´ì € í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7c6405d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vocab size: 7742\n"
     ]
    }
   ],
   "source": [
    "questions = raw_dataframe['Q_clean'].tolist()\n",
    "answers = raw_dataframe['A_clean'].tolist()\n",
    "\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    questions + answers,\n",
    "    target_vocab_size=2**13\n",
    ")\n",
    "\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "MAX_LENGTH = 25\n",
    "\n",
    "print(f\"âœ… Vocab size: {VOCAB_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8f7039",
   "metadata": {},
   "source": [
    "# ğŸ—ï¸ í† í¬ë‚˜ì´ì¦ˆ ë° íŒ¨ë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e01f4c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í† í¬ë‚˜ì´ì¦ˆ í›„ ì§ˆë¬¸ ìˆ˜: 7694\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    return tokenized_inputs, tokenized_outputs\n",
    "\n",
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "\n",
    "print(f\"âœ… í† í¬ë‚˜ì´ì¦ˆ í›„ ì§ˆë¬¸ ìˆ˜: {len(questions)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f63e7b0",
   "metadata": {},
   "source": [
    "# ğŸ“š ë°ì´í„°ì…‹ êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7f340c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((questions, answers))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adce507",
   "metadata": {},
   "source": [
    "# #ï¸âƒ£ GPT-1 ìŠ¤íƒ€ì¼ Decoder-only ëª¨ë¸ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ca792f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT1Block(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout_rate=0.1):\n",
    "        super(GPT1Block, self).__init__()\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(d_ff, activation='gelu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output = self.attention(x, x, attention_mask=mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        return out2\n",
    "\n",
    "class GPT1Model(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, max_len, d_model=256, num_layers=4, num_heads=8, d_ff=512, dropout_rate=0.1):\n",
    "        super(GPT1Model, self).__init__()\n",
    "        self.token_emb = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(max_len, d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.blocks = [GPT1Block(d_model, num_heads, d_ff, dropout_rate) for _ in range(num_layers)]\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        pos = tf.range(start=0, limit=seq_len, delta=1)\n",
    "        x = self.token_emb(x) + self.pos_emb(pos)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        mask = mask[tf.newaxis, tf.newaxis, :, :]\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, training=training, mask=mask)\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77287f9f",
   "metadata": {},
   "source": [
    "# ğŸ“ˆ ì»¤ìŠ¤í…€ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ (CustomSchedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ed85ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a2abed",
   "metadata": {},
   "source": [
    "# âš™ï¸ ëª¨ë¸ ì»´íŒŒì¼ (ì†ì‹¤ í•¨ìˆ˜, ìµœì í™”ê¸° ì„¤ì •)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e73758e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT1Model(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_len=MAX_LENGTH,\n",
    "    d_model=256,\n",
    "    num_layers=4,\n",
    "    num_heads=8,\n",
    "    d_ff=512,\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "learning_rate = CustomSchedule(d_model=256)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "    loss_ = loss_object(y_true, y_pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89789cb5",
   "metadata": {},
   "source": [
    "# ğŸ›‘ EarlyStopping ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2c947c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9cd3ba",
   "metadata": {},
   "source": [
    "# ğŸ‹ï¸ ëª¨ë¸ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0362ec8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 1s/step - loss: 8.7358\n",
      "Epoch 2/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 1s/step - loss: 7.4010\n",
      "Epoch 3/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 969ms/step - loss: 6.3717\n",
      "Epoch 4/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 941ms/step - loss: 5.5629\n",
      "Epoch 5/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 950ms/step - loss: 5.3312\n",
      "Epoch 6/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 976ms/step - loss: 5.2250\n",
      "Epoch 7/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 1s/step - loss: 5.1355\n",
      "Epoch 8/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 1s/step - loss: 5.0089\n",
      "Epoch 9/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 1s/step - loss: 4.8556\n",
      "Epoch 10/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 1s/step - loss: 4.6961\n",
      "Epoch 11/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 1s/step - loss: 4.5152\n",
      "Epoch 12/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 1s/step - loss: 4.2628\n",
      "Epoch 13/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - loss: 3.9878\n",
      "Epoch 14/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 1s/step - loss: 3.6922\n",
      "Epoch 15/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 968ms/step - loss: 3.3780\n",
      "Epoch 16/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 963ms/step - loss: 3.0455\n",
      "Epoch 17/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 957ms/step - loss: 2.6810\n",
      "Epoch 18/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 969ms/step - loss: 2.3369\n",
      "Epoch 19/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 994ms/step - loss: 1.9889\n",
      "Epoch 20/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 997ms/step - loss: 1.6813\n",
      "Epoch 21/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 996ms/step - loss: 1.4187\n",
      "Epoch 22/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 994ms/step - loss: 1.1965\n",
      "Epoch 23/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 997ms/step - loss: 1.0530\n",
      "Epoch 24/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 999ms/step - loss: 0.9444\n",
      "Epoch 25/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 998ms/step - loss: 0.8702\n",
      "Epoch 26/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 1s/step - loss: 0.8251  \n",
      "Epoch 27/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 998ms/step - loss: 0.7826\n",
      "Epoch 28/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 999ms/step - loss: 0.7720\n",
      "Epoch 29/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 999ms/step - loss: 0.7480\n",
      "Epoch 30/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 999ms/step - loss: 0.7420\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaacedd6",
   "metadata": {},
   "source": [
    "# ğŸ¯ ë¬¸ì¥ ìƒì„± í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3e40cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(sentence):\n",
    "    sentence = preprocess_text(sentence)\n",
    "    input_ids = START_TOKEN + tokenizer.encode(sentence) + END_TOKEN\n",
    "    input_ids = tf.expand_dims(input_ids, 0)\n",
    "\n",
    "    for _ in range(MAX_LENGTH):\n",
    "        predictions = model(input_ids, training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = tf.argmax(predictions, axis=-1)\n",
    "        predicted_id = tf.cast(predicted_id, dtype=tf.int32)  # ğŸ‘ˆ ì—¬ê¸°ê°€ ì¤‘ìš”!\n",
    "\n",
    "        if tf.equal(predicted_id[0, 0], END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "        input_ids = tf.concat([input_ids, predicted_id], axis=-1)\n",
    "\n",
    "    predicted_sentence = tokenizer.decode([i for i in tf.squeeze(input_ids) if i < tokenizer.vocab_size])\n",
    "    return predicted_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbad48d",
   "metadata": {},
   "source": [
    "# ğŸ§ª ë‹¤ì–‘í•œ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ (50ê°œ ì´ìƒ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "037de6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] â“ ì§ˆë¬¸: ì•ˆë…•\n",
      "    â–¶ï¸ ë‹µë³€: ì•ˆë…•í•˜ì„¸ìš”.\n",
      "------------------------------------------------------------\n",
      "[2] â“ ì§ˆë¬¸: ì˜¤ëŠ˜ ë­í•´?\n",
      "    â–¶ï¸ ë‹µë³€: ì˜¤ëŠ˜ ë­í•´ì¢‹ì€ ì¢‹ì€ ì¢‹ì€ ì‚¶ì¢‹ì•„ìš”..ë¯¿ì–´ìš”\n",
      "------------------------------------------------------------\n",
      "[3] â“ ì§ˆë¬¸: ë‚ ì”¨ ì–´ë•Œ?\n",
      "    â–¶ï¸ ë‹µë³€: ë‚ ì”¨ ì–´ë•Œë„ ìŠí˜€ì§ˆ ìì‹ ì„ \n",
      "------------------------------------------------------------\n",
      "[4] â“ ì§ˆë¬¸: ë°°ê³ íŒŒ\n",
      "    â–¶ï¸ ë‹µë³€: ë°°ê³ íŒŒë§›ë‚œ ìŒì‹ .\n",
      "------------------------------------------------------------\n",
      "[5] â“ ì§ˆë¬¸: ì¡¸ë ¤\n",
      "    â–¶ï¸ ë‹µë³€: ì¡¸ë ¤ë‚´ìš”ë¨¹ìœ¼ë©´ ë‹µì´ ì¢€ ë¥¼ ìˆì´ë„¤ìš”.\n",
      "------------------------------------------------------------\n",
      "[6] â“ ì§ˆë¬¸: ë„ˆ ë­í•˜ëŠ” ì• ì•¼?\n",
      "    â–¶ï¸ ë‹µë³€: ë„ˆ ë­í•˜ëŠ” ì• ì•¼.\n",
      "------------------------------------------------------------\n",
      "[7] â“ ì§ˆë¬¸: ì¢‹ì•„í•˜ëŠ” ìƒ‰ì€ ë­ì•¼?\n",
      "    â–¶ï¸ ë‹µë³€: ì¢‹ì•„í•˜ëŠ” ìƒ‰ì€ ë­ì•¼ë„ ë„ ë‹¤ì‹œ ì¢‹ì•„ìš”.\n",
      "------------------------------------------------------------\n",
      "[8] â“ ì§ˆë¬¸: ì‹¬ì‹¬í•´\n",
      "    â–¶ï¸ ë‹µë³€: ì‹¬ì‹¬í•´ï¿½ìˆë‹¤ë©´ ìš”.\n",
      "------------------------------------------------------------\n",
      "[9] â“ ì§ˆë¬¸: ë…¸ë˜ ì¶”ì²œí•´ì¤˜\n",
      "    â–¶ï¸ ë‹µë³€: ë…¸ë˜ ì¶”ì²œí•´ì¤˜ê³  ï¿½.\n",
      "------------------------------------------------------------\n",
      "[10] â“ ì§ˆë¬¸: ì˜í™” ë³¼ê¹Œ?\n",
      "    â–¶ï¸ ë‹µë³€: ì˜í™” ë³¼ê¹ŒëŠ¥ìˆëŠ” .\n",
      "------------------------------------------------------------\n",
      "[11] â“ ì§ˆë¬¸: ìš´ë™í•˜ê³  ì‹¶ì–´\n",
      "    â–¶ï¸ ë‹µë³€: ìš´ë™í•˜ê³  ì‹¶ì–´.\n",
      "------------------------------------------------------------\n",
      "[12] â“ ì§ˆë¬¸: ì˜¤ëŠ˜ ê¸°ë¶„ ì–´ë•Œ?\n",
      "    â–¶ï¸ ë‹µë³€: ì˜¤ëŠ˜ ê¸°ë¶„ ì–´ë•Œì¢‹ê² ì–´ìš”.\n",
      "------------------------------------------------------------\n",
      "[13] â“ ì§ˆë¬¸: ì‚¬ë‘ì´ ë­ì•¼?\n",
      "    â–¶ï¸ ë‹µë³€: ì‚¬ë‘ì´ ë­ì•¼ì£¼ëŠ” ê²Œ ê²Œ ì‹¶ì€ ì˜ ì‹¶ì€ í•˜ê³  ì£¼ê³  ì‹¶ì€ ë§ˆìŒì¸.. . \n",
      "------------------------------------------------------------\n",
      "[14] â“ ì§ˆë¬¸: ë‚˜ ì§€ê¸ˆ ìš°ìš¸í•´\n",
      "    â–¶ï¸ ë‹µë³€: ë‚˜ ì§€ê¸ˆ ìš°ìš¸í•´ìˆì–´ìš”.\n",
      "------------------------------------------------------------\n",
      "[15] â“ ì§ˆë¬¸: ì¶”ì²œ ì¢€ í•´ì¤˜\n",
      "    â–¶ï¸ ë‹µë³€: ì¶”ì²œ ì¢€ í•´ì¤˜.\n",
      "------------------------------------------------------------\n",
      "[16] â“ ì§ˆë¬¸: ì¹œêµ¬ë‘ ì‹¸ì› ì–´\n",
      "    â–¶ï¸ ë‹µë³€: ì¹œêµ¬ë‘ ì‹¸ì› ì–´ì„ ì„ ì¢‹ê² ì–´ìš”.\n",
      "------------------------------------------------------------\n",
      "[17] â“ ì§ˆë¬¸: ì‹œí—˜ ë§í–ˆì–´\n",
      "    â–¶ï¸ ë‹µë³€: ì‹œí—˜ ë§í–ˆì–´   í•´ìš”.\n",
      "------------------------------------------------------------\n",
      "[18] â“ ì§ˆë¬¸: ë§›ì§‘ ì¶”ì²œí•´ì¤˜\n",
      "    â–¶ï¸ ë‹µë³€: ë§›ì§‘ ì¶”ì²œí•´ì¤˜ê¸¸ .\n",
      "------------------------------------------------------------\n",
      "[19] â“ ì§ˆë¬¸: í‡´ì‚¬í•˜ê³  ì‹¶ì–´\n",
      "    â–¶ï¸ ë‹µë³€: í‡´ì‚¬í•˜ê³  ì‹¶ì–´\n",
      "------------------------------------------------------------\n",
      "[20] â“ ì§ˆë¬¸: ê¿ˆì´ ë­ì•¼?\n",
      "    â–¶ï¸ ë‹µë³€: ê¿ˆì´ ë­ì•¼ë„ ì•Šì•„ë„ ì•Šì•„ë„ ë„ ì£ ì¸ .\n",
      "------------------------------------------------------------\n",
      "[21] â“ ì§ˆë¬¸: ì—¬í–‰ ê°€ê³  ì‹¶ì–´\n",
      "    â–¶ï¸ ë‹µë³€: ì—¬í–‰ ê°€ê³  ì‹¶ì–´.\n",
      "------------------------------------------------------------\n",
      "[22] â“ ì§ˆë¬¸: ì¢‹ì•„í•˜ëŠ” ê³„ì ˆì€?\n",
      "    â–¶ï¸ ë‹µë³€: ì¢‹ì•„í•˜ëŠ” ê³„ì ˆì€\n",
      "------------------------------------------------------------\n",
      "[23] â“ ì§ˆë¬¸: í˜¼ì ìˆëŠ” ê±° ì¢‹ì•„í•´?\n",
      "    â–¶ï¸ ë‹µë³€: í˜¼ì ìˆëŠ” ê±° ì¢‹ì•„í•´.\n",
      "------------------------------------------------------------\n",
      "[24] â“ ì§ˆë¬¸: ì±… ì¶”ì²œí•´ì¤˜\n",
      "    â–¶ï¸ ë‹µë³€: ì±… ì¶”ì²œí•´ì¤˜ê²ƒë³´ë‹¤ ê°–ê²ƒë„ ê±°ì˜ˆìš”.\n",
      "------------------------------------------------------------\n",
      "[25] â“ ì§ˆë¬¸: ì»¤í”¼ ë§ˆì‹¤ê¹Œ?\n",
      "    â–¶ï¸ ë‹µë³€: ì»¤í”¼ ë§ˆì‹¤ê¹Œì´ìš”ìˆëŠ” .ìœ¼ë¡œ ...\n",
      "------------------------------------------------------------\n",
      "[26] â“ ì§ˆë¬¸: ìš´ë™ ë£¨í‹´ ì•Œë ¤ì¤˜\n",
      "    â–¶ï¸ ë‹µë³€: ìš´ë™ ë£¨í‹´ ì•Œë ¤ì¤˜ê±°ì˜ˆìš”.\n",
      "------------------------------------------------------------\n",
      "[27] â“ ì§ˆë¬¸: ì˜¤ëŠ˜ ë°¤ ë­í• ê¹Œ?\n",
      "    â–¶ï¸ ë‹µë³€: ì˜¤ëŠ˜ ë°¤ ë­í• ê¹Œ.\n",
      "------------------------------------------------------------\n",
      "[28] â“ ì§ˆë¬¸: ì™¸ë¡œì›Œ\n",
      "    â–¶ï¸ ë‹µë³€: ì™¸ë¡œì›Œ.í•˜ê²Œ ë§ˆìŒì€ ì´ì´.\n",
      "------------------------------------------------------------\n",
      "[29] â“ ì§ˆë¬¸: ì‹¬ì‹¬í•´ ì£½ê² ì–´\n",
      "    â–¶ï¸ ë‹µë³€: ì‹¬ì‹¬í•´ ì£½ê² ì–´ì‹¤ë°•í•˜ìƒê°í•´ë³´ì„¸ìš”.\n",
      "------------------------------------------------------------\n",
      "[30] â“ ì§ˆë¬¸: ê³ ì–‘ì´ í‚¤ìš¸ê¹Œ?\n",
      "    â–¶ï¸ ë‹µë³€: ê³ ì–‘ì´ í‚¤ìš¸ê¹Œ.\n",
      "------------------------------------------------------------\n",
      "[31] â“ ì§ˆë¬¸: ë‚´ì¼ ë­í•´?\n",
      "    â–¶ï¸ ë‹µë³€: ë‚´ì¼ ë­í•´ì¢‹ì€ ì¢‹ì€ ì¢‹ì€ ìˆê²Œ ì‚¬ëŒë“¤ì´ .\n",
      "------------------------------------------------------------\n",
      "[32] â“ ì§ˆë¬¸: ì§€ê¸ˆ ì–´ë””ì•¼?\n",
      "    â–¶ï¸ ë‹µë³€: ì§€ê¸ˆ ì–´ë””ì•¼ìˆì–´ìš”ë³´ì„¸ìš”\n",
      "------------------------------------------------------------\n",
      "[33] â“ ì§ˆë¬¸: ì˜ì\n",
      "    â–¶ï¸ ë‹µë³€: ì˜ìê¿ˆ ê¿ˆ ..\n",
      "------------------------------------------------------------\n",
      "[34] â“ ì§ˆë¬¸: ë°°ë‹¬ ë­ ì‹œí‚¬ê¹Œ?\n",
      "    â–¶ï¸ ë‹µë³€: ë°°ë‹¬ ë­ ì‹œí‚¬ê¹Œì¶•í•˜í•´ìš”.\n",
      "------------------------------------------------------------\n",
      "[35] â“ ì§ˆë¬¸: ì§‘ì— ê°€ê³  ì‹¶ì–´\n",
      "    â–¶ï¸ ë‹µë³€: ì§‘ì— ê°€ê³  ì‹¶ì–´ê°™ì´ .\n",
      "------------------------------------------------------------\n",
      "[36] â“ ì§ˆë¬¸: ê²Œì„ ì¶”ì²œí•´ì¤˜\n",
      "    â–¶ï¸ ë‹µë³€: ê²Œì„ ì¶”ì²œí•´ì¤˜ê³  ê¸¸ .\n",
      "------------------------------------------------------------\n",
      "[37] â“ ì§ˆë¬¸: ìƒì¼ ì¶•í•˜í•´ì¤˜\n",
      "    â–¶ï¸ ë‹µë³€: ìƒì¼ ì¶•í•˜í•´ì¤˜ë˜ê² ë„¤ìš”ê³  ëŠê»´ì§€.\n",
      "------------------------------------------------------------\n",
      "[38] â“ ì§ˆë¬¸: ëˆ ë§ì´ ë²Œê³  ì‹¶ì–´\n",
      "    â–¶ï¸ ë‹µë³€: ëˆ ë§ì´ ë²Œê³  ì‹¶ì–´\n",
      "------------------------------------------------------------\n",
      "[39] â“ ì§ˆë¬¸: íœ´ê°€ ê°€ê³  ì‹¶ì–´\n",
      "    â–¶ï¸ ë‹µë³€: íœ´ê°€ ê°€ê³  ì‹¶ì–´.\n",
      "------------------------------------------------------------\n",
      "[40] â“ ì§ˆë¬¸: í”¼ê³¤í•´ ì£½ê² ì–´\n",
      "    â–¶ï¸ ë‹µë³€: í”¼ê³¤í•´ ì£½ê² ì–´ê±´ ì¢‹ê² ì–´ìš”.\n",
      "------------------------------------------------------------\n",
      "[41] â“ ì§ˆë¬¸: ì‹œê°„ ì—¬í–‰í•˜ê³  ì‹¶ì–´\n",
      "    â–¶ï¸ ë‹µë³€: ì‹œê°„ ì—¬í–‰í•˜ê³  ì‹¶ì–´.\n",
      "------------------------------------------------------------\n",
      "[42] â“ ì§ˆë¬¸: ì˜¤ëŠ˜ í•˜ë£¨ ì–´ë• ì–´?\n",
      "    â–¶ï¸ ë‹µë³€: ì˜¤ëŠ˜ í•˜ë£¨ ì–´ë• ì–´\n",
      "------------------------------------------------------------\n",
      "[43] â“ ì§ˆë¬¸: ê³ ë°±í• ê¹Œ ë§ê¹Œ?\n",
      "    â–¶ï¸ ë‹µë³€: ê³ ë°±í• ê¹Œ ë§ê¹Œ.\n",
      "------------------------------------------------------------\n",
      "[44] â“ ì§ˆë¬¸: í˜ë“¤ì–´\n",
      "    â–¶ï¸ ë‹µë³€: í˜ë“¤ì–´ë„ˆë¬´ ì§€ ê±°ë¼ë©´ ìˆ˜\n",
      "------------------------------------------------------------\n",
      "[45] â“ ì§ˆë¬¸: ì‚¬ëŒì€ ì™œ ì‚´ê¹Œ?\n",
      "    â–¶ï¸ ë‹µë³€: ì‚¬ëŒì€ ì™œ ì‚´ê¹Œì €ì•„ë‹ˆë„ì „í•´í•œ ì•ˆ \n",
      "------------------------------------------------------------\n",
      "[46] â“ ì§ˆë¬¸: ì„¸ìƒì—ì„œ ì œì¼ ë§›ìˆëŠ” ìŒì‹ì€?\n",
      "    â–¶ï¸ ë‹µë³€: ì„¸ìƒì—ì„œ ì œì¼ ë§›ìˆëŠ” ìŒì‹ì€\n",
      "------------------------------------------------------------\n",
      "[47] â“ ì§ˆë¬¸: ë„Œ ê¿ˆ ê¿€ ìˆ˜ ìˆì–´?\n",
      "    â–¶ï¸ ë‹µë³€: ë„Œ ê¿ˆ ê¿€ ìˆ˜ ìˆì–´.\n",
      "------------------------------------------------------------\n",
      "[48] â“ ì§ˆë¬¸: ë© ë•Œë¦¬ê³  ì‹¶ì–´\n",
      "    â–¶ï¸ ë‹µë³€: ë© ë•Œë¦¬ê³  ì‹¶ì–´ì¢‹ì•„ìš”í•  ê±´ê°•ì— ì¢‹ì•„ìš”.\n",
      "------------------------------------------------------------\n",
      "[49] â“ ì§ˆë¬¸: ë‚˜ ìš°ìš¸ì¦ ê±¸ë¦° ê²ƒ ê°™ì•„\n",
      "    â–¶ï¸ ë‹µë³€: ë‚˜ ìš°ìš¸ì¦ ê±¸ë¦° ê²ƒ ê°™ì•„ë°”ëë‹ˆë‹¤ë°”ëë‹ˆë‹¤\n",
      "------------------------------------------------------------\n",
      "[50] â“ ì§ˆë¬¸: ë‚´ ì¸ìƒì€ ì‹¤íŒ¨ì•¼?\n",
      "    â–¶ï¸ ë‹µë³€: ë‚´ ì¸ìƒì€ ì‹¤íŒ¨ì•¼ê±°ì˜ˆìš”.\n",
      "------------------------------------------------------------\n",
      "[51] â“ ì§ˆë¬¸: ì¢‹ì•„í•˜ëŠ” ë™ë¬¼ì€ ë­ì•¼?\n",
      "    â–¶ï¸ ë‹µë³€: ì¢‹ì•„í•˜ëŠ” ë™ë¬¼ì€ ë­ì•¼ê²Œ ìˆí•´ìš”.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sample_questions = [\n",
    "    \"ì•ˆë…•\", \"ì˜¤ëŠ˜ ë­í•´?\", \"ë‚ ì”¨ ì–´ë•Œ?\", \"ë°°ê³ íŒŒ\", \"ì¡¸ë ¤\", \"ë„ˆ ë­í•˜ëŠ” ì• ì•¼?\", \"ì¢‹ì•„í•˜ëŠ” ìƒ‰ì€ ë­ì•¼?\", \"ì‹¬ì‹¬í•´\", \"ë…¸ë˜ ì¶”ì²œí•´ì¤˜\", \"ì˜í™” ë³¼ê¹Œ?\", \n",
    "    \"ìš´ë™í•˜ê³  ì‹¶ì–´\", \"ì˜¤ëŠ˜ ê¸°ë¶„ ì–´ë•Œ?\", \"ì‚¬ë‘ì´ ë­ì•¼?\", \"ë‚˜ ì§€ê¸ˆ ìš°ìš¸í•´\", \"ì¶”ì²œ ì¢€ í•´ì¤˜\", \"ì¹œêµ¬ë‘ ì‹¸ì› ì–´\", \"ì‹œí—˜ ë§í–ˆì–´\", \"ë§›ì§‘ ì¶”ì²œí•´ì¤˜\", \n",
    "    \"í‡´ì‚¬í•˜ê³  ì‹¶ì–´\", \"ê¿ˆì´ ë­ì•¼?\", \"ì—¬í–‰ ê°€ê³  ì‹¶ì–´\", \"ì¢‹ì•„í•˜ëŠ” ê³„ì ˆì€?\", \"í˜¼ì ìˆëŠ” ê±° ì¢‹ì•„í•´?\", \"ì±… ì¶”ì²œí•´ì¤˜\", \"ì»¤í”¼ ë§ˆì‹¤ê¹Œ?\", \n",
    "    \"ìš´ë™ ë£¨í‹´ ì•Œë ¤ì¤˜\", \"ì˜¤ëŠ˜ ë°¤ ë­í• ê¹Œ?\", \"ì™¸ë¡œì›Œ\", \"ì‹¬ì‹¬í•´ ì£½ê² ì–´\", \"ê³ ì–‘ì´ í‚¤ìš¸ê¹Œ?\", \"ë‚´ì¼ ë­í•´?\", \"ì§€ê¸ˆ ì–´ë””ì•¼?\", \"ì˜ì\", \n",
    "    \"ë°°ë‹¬ ë­ ì‹œí‚¬ê¹Œ?\", \"ì§‘ì— ê°€ê³  ì‹¶ì–´\", \"ê²Œì„ ì¶”ì²œí•´ì¤˜\", \"ìƒì¼ ì¶•í•˜í•´ì¤˜\", \"ëˆ ë§ì´ ë²Œê³  ì‹¶ì–´\", \"íœ´ê°€ ê°€ê³  ì‹¶ì–´\", \"í”¼ê³¤í•´ ì£½ê² ì–´\", \n",
    "    \"ì‹œê°„ ì—¬í–‰í•˜ê³  ì‹¶ì–´\", \"ì˜¤ëŠ˜ í•˜ë£¨ ì–´ë• ì–´?\", \"ê³ ë°±í• ê¹Œ ë§ê¹Œ?\", \"í˜ë“¤ì–´\", \"ì‚¬ëŒì€ ì™œ ì‚´ê¹Œ?\", \"ì„¸ìƒì—ì„œ ì œì¼ ë§›ìˆëŠ” ìŒì‹ì€?\", \n",
    "    \"ë„Œ ê¿ˆ ê¿€ ìˆ˜ ìˆì–´?\", \"ë© ë•Œë¦¬ê³  ì‹¶ì–´\", \"ë‚˜ ìš°ìš¸ì¦ ê±¸ë¦° ê²ƒ ê°™ì•„\", \"ë‚´ ì¸ìƒì€ ì‹¤íŒ¨ì•¼?\", \"ì¢‹ì•„í•˜ëŠ” ë™ë¬¼ì€ ë­ì•¼?\"\n",
    "]\n",
    "\n",
    "for i, q in enumerate(sample_questions, 1):\n",
    "    print(f\"[{i}] â“ ì§ˆë¬¸: {q}\")\n",
    "    try:\n",
    "        response = generate_sentence(q)\n",
    "        print(f\"    â–¶ï¸ ë‹µë³€: {response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    print('-' * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
