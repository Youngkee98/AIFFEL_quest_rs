# ğŸ“˜ LSTM ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸ì—ì„œì˜ Vocab Sizeì™€ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì˜ ì˜í–¥ ë¶„ì„

ì´ ì €ì¥ì†ŒëŠ” ë‹¤ìŒ ë…¼ë¬¸ì˜ ì½”ë“œ, ë°ì´í„° ì„¤ì •, ì‹¤í—˜ ê²°ê³¼ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤:

**"LSTM ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸ì—ì„œì˜ Vocab Sizeì™€ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì˜ ì˜í–¥ ë¶„ì„"**

## ğŸ“Œ ê°œìš”

ë³¸ ì—°êµ¬ëŠ” Reuters-21578 ë‰´ìŠ¤ ë°ì´í„°ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ, ë‹¨ì–´ ì§‘í•© í¬ê¸°(vocabulary size)ì™€ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ LSTM ê³„ì—´ ëª¨ë¸(LSTM, BiLSTM, GRU)ì˜ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì‹¤ì¦ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.

í•µì‹¬ ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
- ì „ì²´ ë‹¨ì–´ ë¶„í¬ì˜ ëˆ„ì  ë¹ˆë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ vocab sizeë¥¼ 10%~100% ë²”ìœ„ë¡œ ì¡°ì •
- í´ë˜ìŠ¤ ë¶ˆê· í˜• ëŒ€ì‘ì„ ìœ„í•œ ê°€ì¤‘ì¹˜ ì†ì‹¤ í•¨ìˆ˜ ë° ë°ì´í„° ì¦ê°• ê¸°ë²• í‰ê°€
- Accuracy ë° macro F1-scoreë¥¼ ì£¼ìš” ì„±ëŠ¥ ì§€í‘œë¡œ ì‚¬ìš©
- F1-score ê¸°ë°˜ Stability ì§€í‘œë¥¼ í™œìš©í•œ vocab sizeë³„ ëª¨ë¸ ì•ˆì •ì„± ë¶„ì„

---

## ğŸ§  ì‹¤í—˜ ëª¨ë¸

- LSTM (Long Short-Term Memory) [1]
- BiLSTM (Bidirectional LSTM) [2]
- GRU (Gated Recurrent Unit) [3]

ëª¨ë“  ëª¨ë¸ì€ ì•„ë˜ì™€ ê°™ì€ ê³µí†µ ì•„í‚¤í…ì²˜ë¥¼ ë”°ë¦…ë‹ˆë‹¤:
- Embedding â†’ RNN ê³„ì¸µ (64, 32 ìœ ë‹›) â†’ Dropout â†’ Dense (64) â†’ Softmax ì¶œë ¥ (46ê°œ í´ë˜ìŠ¤)

---

## ğŸ“ˆ í•µì‹¬ ê²°ê³¼ ìš”ì•½
![F1_Comparison](https://github.com/user-attachments/assets/4ce0ee58-bac2-4163-a250-820866cf1f42)
![Accuracy_Comparison](https://github.com/user-attachments/assets/f93bec66-3965-4849-920f-2bfc4281c711)



- BiLSTM ëª¨ë¸ì´ ì •í™•ë„ ë° F1 ì ìˆ˜ ëª¨ë‘ì—ì„œ ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„
- Vocab Sizeê°€ 436 ì´ìƒ(43.6%)ì¼ ë•Œ BiLSTMì€ ê°€ì¥ ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ ìœ ì§€
- ë°ì´í„° ì¦ê°•ì´ í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ ì™„í™”ì— íš¨ê³¼ì 
- Vocab size ì„ íƒ ì „ëµì€ ê³¼ì í•©ê³¼ ì •ë³´ ì†ì‹¤ì„ ëª¨ë‘ ë°©ì§€í•˜ëŠ” ë° ì¤‘ìš”í•¨

---

## ğŸ“¬ ë¬¸ì˜

**ê¹€ì˜ê¸° (Youngkee Kim)**  
zerokee98@naver.com  
AI Research 13th, MODU LABS

---

# ğŸ“˜ Effect of Vocabulary Size and Class Imbalance on LSTM-based Text Classification Models

This repository contains the code, data settings, and experimental results for our paper:

**"Effect of Vocabulary Size and Class Imbalance on LSTM-based Text Classification Models"**

## ğŸ“Œ Overview

This study explores how vocabulary size and class imbalance influence the performance of RNN-based modelsâ€”specifically LSTM, BiLSTM, and GRUâ€”in multi-class text classification tasks using the Reuters-21578 dataset.

Key aspects include:
- Systematic variation of vocabulary size based on cumulative token coverage (from 10% to 100%)
- Evaluation of class imbalance handling methods: weighted loss and data augmentation
- Use of accuracy and macro F1-score as primary evaluation metrics
- Stability analysis across vocab sizes using the F1-based stability index

---

## ğŸ§  Models Evaluated

- Long Short-Term Memory (LSTM) [1]
- Bidirectional LSTM (BiLSTM) [2]
- Gated Recurrent Unit (GRU) [3]

All models share a common architecture:
- Embedding â†’ RNN Layers (64, 32 units) â†’ Dropout â†’ Dense (64) â†’ Output (Softmax over 46 classes)

---

## ğŸ“ˆ Key Findings

- BiLSTM outperforms LSTM and GRU in both accuracy and F1
- From vocab size 436 (43.6% coverage) and above, BiLSTM shows robust and stable performance
- Data augmentation effectively mitigates the impact of class imbalance
- Vocabulary size selection plays a key role in performance and generalization

---

## ğŸ“¬ Contact

**Youngkee Kim**  
zerokee98@naver.com  
AI Research 13th, MODU LABS
