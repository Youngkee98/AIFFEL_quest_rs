{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e08b488",
   "metadata": {},
   "source": [
    "# 1️⃣ 라이브러리 및 패키지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a20f25fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "4.11.3\n",
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import numpy\n",
    "import transformers\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "print(tensorflow.__version__)\n",
    "print(numpy.__version__)\n",
    "print(transformers.__version__)\n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "177d4356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b41227923f24cd8ab3583c23e0157b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 150000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 로딩 중 로그 출력 최소화 (로딩바 포함)\n",
    "import datasets\n",
    "datasets.logging.set_verbosity_error()  # 모든 로깅 출력 (로딩바 포함)을 끔\n",
    "\n",
    "# 실제 데이터셋 불러오기\n",
    "nsmc_dataset = load_dataset('e9t/nsmc')\n",
    "\n",
    "# 데이터셋 전체 구조 출력 (train/validation/test로 나뉜 dict 형태)\n",
    "print(nsmc_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d0c0f7",
   "metadata": {},
   "source": [
    "### ✅ 체크포인트 : 데이터 구성요소 분석\n",
    "\n",
    "---\n",
    "\n",
    "데터셋은 DatasetDict 형태로 train, test 가 존재하는 것을 알 수 있음.\n",
    "\n",
    "`train` : 150000개의 데이터, `id`,`document`,`label` 이 존재한다.\n",
    "`test` : 50000개의 데이터, `id`,`document`,`label` 이 존재한다.\n",
    "\n",
    "**Validation Set을 나눠줘야겠다. 학습용으로**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc72305f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': Dataset({\n",
      "    features: ['id', 'document', 'label'],\n",
      "    num_rows: 135000\n",
      "}), 'validation': Dataset({\n",
      "    features: ['id', 'document', 'label'],\n",
      "    num_rows: 15000\n",
      "}), 'test': Dataset({\n",
      "    features: ['id', 'document', 'label'],\n",
      "    num_rows: 50000\n",
      "})}\n"
     ]
    }
   ],
   "source": [
    "# train 데이터를 90% 학습용 / 10% 검증용으로 나눔\n",
    "# 일반적으로 10% 정도 사용한다고 한다.\n",
    "nsmc_split = nsmc_dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# 새롭게 구성된 데이터셋 딕셔너리로 저장\n",
    "nsmc_dataset = {\n",
    "    'train': nsmc_split['train'],\n",
    "    'validation': nsmc_split['test'],\n",
    "    'test': nsmc_dataset['test']\n",
    "}\n",
    "\n",
    "print(nsmc_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c14df3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative (0): 67703\n",
      "Positive (1): 67297\n"
     ]
    }
   ],
   "source": [
    "# 클래스 불균형 확인하기\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# train split에서 label 추출\n",
    "train_labels = nsmc_dataset['train']['label']\n",
    "\n",
    "# 각 클래스 개수 세기\n",
    "label_counts = Counter(train_labels)\n",
    "\n",
    "# 출력\n",
    "print(f\"Negative (0): {label_counts[0]}\")\n",
    "print(f\"Positive (1): {label_counts[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1540a2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. 문장 길이 측정\n",
    "train_docs = nsmc_dataset['train']['document']\n",
    "lengths = [len(doc) for doc in train_docs]  # 각 문장의 문자 수\n",
    "\n",
    "# 2. 히스토그램 그리기\n",
    "plt.hist(lengths, bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title(\"NSMC\")\n",
    "plt.xlabel(\"doc length\")\n",
    "plt.ylabel(\"doc num\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa04c667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 평균 문장 길이: 35.19\n",
      "📌 표준편차: 29.57\n",
      "📌 90%의 문장을 포함하기 위한 최소 길이: 75\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. 길이 리스트 생성 \n",
    "lengths = [len(doc) for doc in nsmc_dataset['train']['document']]\n",
    "\n",
    "# 2. 평균 및 표준편차 계산\n",
    "mean_length = np.mean(lengths)\n",
    "std_length = np.std(lengths)\n",
    "\n",
    "print(f\"📌 평균 문장 길이: {mean_length:.2f}\")\n",
    "print(f\"📌 표준편차: {std_length:.2f}\")\n",
    "\n",
    "# 3. 누적 분포를 기준으로 90% 커버하는 길이 구하기\n",
    "percentile_90 = np.percentile(lengths, 90)  # 전체 문장 중 90%가 이 길이 이내\n",
    "\n",
    "print(f\"📌 90%의 문장을 포함하기 위한 최소 길이: {percentile_90:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65ad9e4",
   "metadata": {},
   "source": [
    "### ✅ 체크포인트 : NSMC 데이터가 어떤 데이터인지 파악\n",
    "\n",
    "---\n",
    "#### 데이터 구조\n",
    "NSMC는 다음과 같은 데이터 특성을 가지고 있다.\n",
    "\n",
    "`ID` : 고유 번호\n",
    "`Document` : 리뷰 텍스트 데이터\n",
    "`Label` : 감성분류 1은 긍정 0은 부정\n",
    "\n",
    "#### 클래스 불균형 여부\n",
    "클래스 불균형은 거의 없다고 보면 된다.\n",
    "\n",
    "#### 문장길이 분포 \n",
    "\n",
    "📌 평균 문장 길이: 35.20\n",
    "\n",
    "📌 표준편차: 29.58\n",
    "\n",
    "📌 90%의 문장을 포함하기 위한 최소 길이: 75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efb2992",
   "metadata": {},
   "source": [
    "# 2️⃣ Tokenizer & Model\n",
    "\n",
    "---\n",
    "\n",
    "만약에 허깅페이스에 데이터셋이 없다면 Tensorflow_dataset에서 불러와서 Huggingface 에 맞게 바꾸는 과정을 하겠지만 충분히 있으므로 허깅페이스 데이터로 그래도 진행한다.\n",
    "\n",
    "현재 노드에서는 `Klue/Bert Model` 및 토크나이저를 `AutoTokenizer`에 맞게 진행할거임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca6f6f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ./nsmc-klue-bert-finetuned/added_tokens.json. We won't load it.\n",
      "loading file ./nsmc-klue-bert-finetuned/vocab.txt\n",
      "loading file ./nsmc-klue-bert-finetuned/tokenizer.json\n",
      "loading file None\n",
      "loading file ./nsmc-klue-bert-finetuned/special_tokens_map.json\n",
      "loading file ./nsmc-klue-bert-finetuned/tokenizer_config.json\n",
      "loading configuration file ./nsmc-klue-bert-finetuned/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"klue/bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file ./nsmc-klue-bert-finetuned/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./nsmc-klue-bert-finetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face의 transformers 라이브러리에서 필요한 클래스 불러오기\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# 🔹 1. Tokenizer 불러오기\n",
    "\n",
    "huggingface_tokenizer = AutoTokenizer.from_pretrained('./nsmc-klue-bert-finetuned')\n",
    "\n",
    "# 🔹 2. 모델 불러오기 (분류용)\n",
    "\n",
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    './nsmc-klue-bert-finetuned',  # 사전 학습된 klue/bert-base 모델 사용\n",
    "    num_labels=2                # 출력 클래스 수: 2개 (0 또는 1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5667e2",
   "metadata": {},
   "source": [
    "### ✅ 체크포인트 : `AutoTokenizer`와 `AutoModel` 의 기능\n",
    "---\n",
    "\n",
    "`AutoTokenizer`, `AutoModel` 기능을 한번 사용해 본거다\n",
    "\n",
    "```python\n",
    "huggingface_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "```\n",
    "\n",
    " 이 명령은 'distilbert-base-uncased' 모델에 최적화된 토크나이저 클래스 (예: DistilBertTokenizerFast)를 자동으로 로드해서 `huggingface_tokenizer`에 할당합니다.\n",
    "\n",
    "```python\n",
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased', \n",
    "    num_labels=2                \n",
    ")\n",
    "```\n",
    "\n",
    "여기서는 `AutoModelForSequenceClassification` 해당 사전학습 모델 위에 분류용 헤드가 붙은 구조를 불러온다.\n",
    "\n",
    "이 부분은 `AutoModelForTokenClassification`, `AutoModelForQuestionAnswering` 등 다양한 task에 따라 모델이 존재한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dd4f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    # data['document']는 문자열 리스트\n",
    "    # → batched=True일 경우 한 번에 여러 문장이 들어옴\n",
    "\n",
    "    # 1. 텍스트 정제 (필요 시)\n",
    "    texts = data['document']\n",
    "    # texts = [clean_text(t) for t in texts]\n",
    "\n",
    "    # 2. 토크나이징\n",
    "    tokenized = huggingface_tokenizer(\n",
    "        texts,                         # 리스트 그대로 전달\n",
    "        truncation=True,\n",
    "        max_length=75,                # 이전 분석 기반 길이 설정\n",
    "        padding='max_length',\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "    # 3. 레이블 추가 (그대로 매칭됨)\n",
    "    tokenized['label'] = data['label']\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08627553",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsmc_dataset = DatasetDict({\n",
    "    'train': nsmc_split['train'],\n",
    "    'validation': nsmc_split['test'],\n",
    "    'test': nsmc_dataset['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c724885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e257b15c3c9e4fb99f45ccd7cc9f8c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67745631953d43f296c8621fb2123960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff8ff30d7a14c74924714e8ee5db096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_dataset = nsmc_dataset.map(transform, batched=True)\n",
    "\n",
    "hf_train_dataset = hf_dataset['train']\n",
    "hf_val_dataset = hf_dataset['validation']\n",
    "hf_test_dataset = hf_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fe80d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1}\n",
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "print(set(hf_dataset['train']['label']))  # → {0, 1}이면 OK, {-1}이면 문제\n",
    "print(set(hf_dataset['test']['label']))  # → {0, 1}이면 OK, {-1}이면 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0581c6ac",
   "metadata": {},
   "source": [
    "### ✅ 체크포인트: map() 함수의 역할\n",
    "---\n",
    "\n",
    "🔹 **map() 함수는 무엇을 하나요**\n",
    "\n",
    "Hugging Face datasets.Dataset 객체에 있는 모든 샘플에 특정 함수를 자동 반복 적용합니다.\n",
    "\n",
    "반복문을 직접 쓰지 않아도, 내부적으로 최적화된 방식으로 빠르게 데이터를 처리해줍니다.\n",
    "\n",
    "🔹 **왜 batched=True를 써야 하나요?**\n",
    "\n",
    "토크나이저(huggingface_tokenizer)는 문장 여러 개를 한꺼번에 처리할 수 있을 때 더 빠르고 효율적입니다.\n",
    "\n",
    "batched=True를 쓰면 transform() 함수에 여러 샘플이 list 형태로 전달되므로,\n",
    "\n",
    "→ data['sentence1']는 단일 문장이 아니라 여러 문장의 리스트가 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf36291",
   "metadata": {},
   "source": [
    "# 4️⃣ Train & Evaluation & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b19d240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "output_dir = os.getenv('HOME') + '/aiffel/transformers/nsmc-klue-bert' \n",
    "\n",
    "# TrainingArguments: Trainer에 넘겨줄 학습 설정값 정의\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir,                          # 학습 결과가 저장될 디렉토리\n",
    "    evaluation_strategy=\"epoch\",        # 매 epoch이 끝날 때마다 평가 실행\n",
    "    learning_rate=2e-5,                 # 학습률 (learning rate)\n",
    "    per_device_train_batch_size=64,      # 각 디바이스당 학습 배치 크기\n",
    "    per_device_eval_batch_size=64,       # 평가 시 배치 크기\n",
    "    num_train_epochs=3,                 # 학습할 전체 에폭 수\n",
    "    weight_decay=0.01                   # 가중치 감소 (L2 정규화에 사용)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ae8a317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)  # logits → class index\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),           # 전체 정확도\n",
    "        'precision': precision_score(labels, preds),         # 정밀도 (양성으로 예측한 것 중 실제 양성 비율)\n",
    "        'recall': recall_score(labels, preds),               # 재현율 (실제 양성 중에서 맞춘 비율)\n",
    "        'f1': f1_score(labels, preds)                        # 정밀도와 재현율의 조화 평균\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d279c7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 객체 생성\n",
    "trainer = Trainer(\n",
    "    model=huggingface_model,             # KLUE-BERT\n",
    "    args=training_arguments,             # 학습 설정 인자들\n",
    "    train_dataset=hf_train_dataset,      # 학습 데이터셋 (토크나이즈 완료)\n",
    "    eval_dataset=hf_val_dataset,         # 검증 데이터셋\n",
    "    compute_metrics=compute_metrics      # 평가 함수 (accuracy, F1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44f0f165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 135000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6330\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6330' max='6330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6330/6330 1:56:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.248600</td>\n",
       "      <td>0.246010</td>\n",
       "      <td>0.899533</td>\n",
       "      <td>0.881637</td>\n",
       "      <td>0.923904</td>\n",
       "      <td>0.902276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.189800</td>\n",
       "      <td>0.239139</td>\n",
       "      <td>0.905267</td>\n",
       "      <td>0.890451</td>\n",
       "      <td>0.925100</td>\n",
       "      <td>0.907445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.131500</td>\n",
       "      <td>0.269185</td>\n",
       "      <td>0.908467</td>\n",
       "      <td>0.906403</td>\n",
       "      <td>0.911819</td>\n",
       "      <td>0.909103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2000\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2500\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3000\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3500\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4000\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4500\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5000\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5500\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-6000\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-6000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-6000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6330, training_loss=0.2004366641172677, metrics={'train_runtime': 6965.4757, 'train_samples_per_second': 58.144, 'train_steps_per_second': 0.909, 'total_flos': 1.56093716925e+16, 'train_loss': 0.2004366641172677, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() # 학습 시작"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982aa4e4",
   "metadata": {},
   "source": [
    "### ✅ 체크포인트: 학습 결과\n",
    "---\n",
    "\n",
    "결과 적으로 과적합되기전에 완료가 되었지만\n",
    "\n",
    "`Accuracy` : 0.90846 \n",
    "\n",
    "`F1`       : 0.909103\n",
    "\n",
    "요구조건인 정확도 90% 이상은 충족했다.\n",
    "\n",
    "---\n",
    "\n",
    "이후에는 현재 Finetunning 된 모델과 가중치들을 저장해 이다음 test 셋에 대해 진행해보아 Acc, F1 을 출력해볼 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eda06cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./nsmc-klue-bert-finetuned\n",
      "Configuration saved in ./nsmc-klue-bert-finetuned/config.json\n",
      "Model weights saved in ./nsmc-klue-bert-finetuned/pytorch_model.bin\n",
      "tokenizer config file saved in ./nsmc-klue-bert-finetuned/tokenizer_config.json\n",
      "Special tokens file saved in ./nsmc-klue-bert-finetuned/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./nsmc-klue-bert-finetuned/tokenizer_config.json',\n",
       " './nsmc-klue-bert-finetuned/special_tokens_map.json',\n",
       " './nsmc-klue-bert-finetuned/vocab.txt',\n",
       " './nsmc-klue-bert-finetuned/added_tokens.json',\n",
       " './nsmc-klue-bert-finetuned/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model('./nsmc-klue-bert-finetuned')\n",
    "huggingface_tokenizer.save_pretrained('./nsmc-klue-bert-finetuned')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27c17b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('./nsmc-klue-bert-finetuned')\n",
    "tokenizer = AutoTokenizer.from_pretrained('./nsmc-klue-bert-finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2510610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'f1': f1_score(labels, preds)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d3f7d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 05:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2749262750148773, 'eval_accuracy': 0.90454, 'eval_f1': 0.9059303494353456, 'eval_runtime': 322.0676, 'eval_samples_per_second': 155.247, 'eval_steps_per_second': 19.406}\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "test_trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# 테스트셋 평가\n",
    "test_result = test_trainer.evaluate(hf_test_dataset)\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1486566b",
   "metadata": {},
   "source": [
    "### ✅ 체크포인트: Eval 결과\n",
    "---\n",
    "\n",
    "결과 적으로 과적합되기전에 완료가 되었지만\n",
    "\n",
    "`Accuracy` : 0.90454\n",
    "\n",
    "`F1`       : 0.90593\n",
    "\n",
    "요구조건인 정확도 90% 이상은 충족했다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a95d9",
   "metadata": {},
   "source": [
    "# 5️⃣ Bucketing & Dynamic Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb5f34",
   "metadata": {},
   "source": [
    "#### ✅ Dynamic Padding 이란?\n",
    "\n",
    "동적 패딩(Dynamic Padding)은 각 미니배치(batch)에서 가장 긴 문장을 기준으로 padding을 수행하는 방식입니다.\n",
    "\n",
    "전체 max_length를 기준으로 하지 않고, 매 배치마다 필요한 만큼만 padding합니다.\n",
    "\n",
    "`batch = [\"나는 간다\", \"나는 오늘 도서관에 갑니다\", \"갑니다\"]`\n",
    "\n",
    "- 가장 긴 문장 길이가 8 토큰이면 → batch 내 모든 문장을 8에 맞춰 padding\n",
    "\n",
    "- 다음 배치가 더 짧은 문장으로 구성되면 → 그 배치에서는 5만큼만 padding\n",
    "\n",
    "#### ✅ Bucketing이란?\n",
    "\n",
    "버킷팅(Bucketing)은 문장 길이가 비슷한 샘플끼리 묶어서 batch를 구성하는 전략입니다.\n",
    "Dynamic Padding의 효율을 더 극대화하기 위한 보조 전략입니다.\n",
    "\n",
    "`데이터셋 전체: [5토큰, 7토큰, 100토큰, 105토큰, 110토큰, 10토큰...]`\n",
    "\n",
    "`Bucket 1: [5, 7, 10]`\n",
    "\n",
    "`Bucket 2: [100, 105, 110]`\n",
    "\n",
    "→ 비슷한 길이끼리 batch를 구성하면 padding이 거의 필요 없음 → 학습 속도 향상 및 메모리 낭비 감소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c067990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 토크나이저 기반으로 dynamic padding 수행하는 Data Collator 생성\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# → 각 배치마다 가장 긴 문장의 길이에 맞춰 padding을 동적으로 수행\n",
    "# → Padding 효율을 높여 GPU 메모리 낭비를 줄이고 학습 속도 향상 기대"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b068e9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "import os\n",
    "\n",
    "# 3. TrainingArguments 정의 (group_by_length 포함)\n",
    "output_dir = os.getenv('HOME') + '/aiffel/transformers/nsmc-klue-bert' \n",
    "\n",
    "training_arguments_bucketing = TrainingArguments(\n",
    "    output_dir=output_dir,              # 모델과 로그 파일 저장 위치\n",
    "    evaluation_strategy=\"epoch\",       # 매 epoch 끝날 때마다 validation 평가 수행\n",
    "    learning_rate=2e-5,                # 학습률\n",
    "    per_device_train_batch_size=64,    # 학습 시 배치 크기\n",
    "    per_device_eval_batch_size=64,     # 평가 시 배치 크기\n",
    "    num_train_epochs=3,                # 학습 epoch 수\n",
    "    weight_decay=0.01,                 # L2 정규화에 사용되는 가중치 감소 값\n",
    "    group_by_length=True               # 문장 길이에 따라 유사한 샘플끼리 버킷을 만들어 학습\n",
    "    # → 이로 인해 동적 패딩 효과가 극대화됨\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d3c52b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Trainer 객체 생성\n",
    "trainer_bucketing = Trainer(\n",
    "    model=model,                             # fine-tuned 된 KLUE-BERT 모델\n",
    "    args=training_arguments_bucketing,       # 위에서 정의한 학습 설정\n",
    "    train_dataset=hf_train_dataset,          # 학습 데이터셋\n",
    "    eval_dataset=hf_val_dataset,             # 검증 데이터셋\n",
    "    tokenizer=tokenizer,                     # tokenizer: 로그 기록, data_collator 등에 사용됨\n",
    "    data_collator=data_collator,             # 동적 padding 적용을 위한 collator\n",
    "    compute_metrics=compute_metrics          # 정확도, 정밀도, 재현율, F1 점수 평가 함수\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9a5cf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 135000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6330\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6330' max='6330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6330/6330 1:58:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>0.332428</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.903744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.410031</td>\n",
       "      <td>0.904733</td>\n",
       "      <td>0.906005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>0.467877</td>\n",
       "      <td>0.905800</td>\n",
       "      <td>0.906788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2000\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2500\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3000\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3500\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4000\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4500\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4500/special_tokens_map.json\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5000\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5500\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5500/special_tokens_map.json\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-6000\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-6000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-6000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6330, training_loss=0.06771285311682342, metrics={'train_runtime': 7083.0432, 'train_samples_per_second': 57.179, 'train_steps_per_second': 0.894, 'total_flos': 1.56093716925e+16, 'train_loss': 0.06771285311682342, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. 학습 시작 (Bucketing + Dynamic Padding 적용)\n",
    "trainer_bucketing.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8dc766ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 05:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Evaluation Result:\n",
      "{'eval_loss': 0.47763878107070923, 'eval_accuracy': 0.9001, 'eval_f1': 0.9015356107946146, 'eval_runtime': 324.192, 'eval_samples_per_second': 154.23, 'eval_steps_per_second': 19.279}\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# test set 평가용 Trainer 생성\n",
    "test_trainer = Trainer(\n",
    "    model=trainer_bucketing.model,  # 학습된 모델 사용\n",
    "    tokenizer=tokenizer,            # 이전에 저장해둔 tokenizer\n",
    "    compute_metrics=compute_metrics # 평가 지표 함수\n",
    ")\n",
    "\n",
    "# 테스트셋 평가\n",
    "test_result = test_trainer.evaluate(hf_test_dataset)\n",
    "print(\"📌 Test Evaluation Result:\")\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50ae0e2",
   "metadata": {},
   "source": [
    "# ✅체크포인트 : Bucket + Dynamic Padding\n",
    "\n",
    "---\n",
    "\n",
    "**Bucket + Dynamic Padding**\n",
    "\n",
    "`eval_accuracy: 0.9001\n",
    "eval_f1      : 0.9015\n",
    "eval_runtime : 324.192`\n",
    "\n",
    "**기존**\n",
    "\n",
    "`'eval_accuracy': 0.90454\n",
    "'eval_f1': 0.9059\n",
    "'eval_runtime': 322.0676`\n",
    "\n",
    "큰차이가 없음.\n",
    "\n",
    "그런데 팀원들과 얘기를 해보았을 때 달라서 Config 확인만 해보자\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18611c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/bert-base/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/05b36ee62545d769939a7746eca739b844a40a7a7553700f110b58b28ed6a949.7cb231256a5dbe886e12b902d05cb1241f330d8c19428508f91b2b28c1cfe0b6\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_name_or_path': 'klue/bert-base',\n",
      " 'add_cross_attention': False,\n",
      " 'architectures': ['BertForMaskedLM'],\n",
      " 'attention_probs_dropout_prob': 0.1,\n",
      " 'bad_words_ids': None,\n",
      " 'bos_token_id': None,\n",
      " 'chunk_size_feed_forward': 0,\n",
      " 'classifier_dropout': None,\n",
      " 'decoder_start_token_id': None,\n",
      " 'diversity_penalty': 0.0,\n",
      " 'do_sample': False,\n",
      " 'early_stopping': False,\n",
      " 'encoder_no_repeat_ngram_size': 0,\n",
      " 'eos_token_id': None,\n",
      " 'finetuning_task': None,\n",
      " 'forced_bos_token_id': None,\n",
      " 'forced_eos_token_id': None,\n",
      " 'hidden_act': 'gelu',\n",
      " 'hidden_dropout_prob': 0.1,\n",
      " 'hidden_size': 768,\n",
      " 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},\n",
      " 'initializer_range': 0.02,\n",
      " 'intermediate_size': 3072,\n",
      " 'is_decoder': False,\n",
      " 'is_encoder_decoder': False,\n",
      " 'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
      " 'layer_norm_eps': 1e-12,\n",
      " 'length_penalty': 1.0,\n",
      " 'max_length': 20,\n",
      " 'max_position_embeddings': 512,\n",
      " 'min_length': 0,\n",
      " 'model_type': 'bert',\n",
      " 'no_repeat_ngram_size': 0,\n",
      " 'num_attention_heads': 12,\n",
      " 'num_beam_groups': 1,\n",
      " 'num_beams': 1,\n",
      " 'num_hidden_layers': 12,\n",
      " 'num_return_sequences': 1,\n",
      " 'output_attentions': False,\n",
      " 'output_hidden_states': False,\n",
      " 'output_scores': False,\n",
      " 'pad_token_id': 0,\n",
      " 'position_embedding_type': 'absolute',\n",
      " 'prefix': None,\n",
      " 'problem_type': None,\n",
      " 'pruned_heads': {},\n",
      " 'remove_invalid_values': False,\n",
      " 'repetition_penalty': 1.0,\n",
      " 'return_dict': True,\n",
      " 'return_dict_in_generate': False,\n",
      " 'sep_token_id': None,\n",
      " 'task_specific_params': None,\n",
      " 'temperature': 1.0,\n",
      " 'tie_encoder_decoder': False,\n",
      " 'tie_word_embeddings': True,\n",
      " 'tokenizer_class': None,\n",
      " 'top_k': 50,\n",
      " 'top_p': 1.0,\n",
      " 'torch_dtype': None,\n",
      " 'torchscript': False,\n",
      " 'transformers_version': '4.11.3',\n",
      " 'type_vocab_size': 2,\n",
      " 'use_bfloat16': False,\n",
      " 'use_cache': True,\n",
      " 'vocab_size': 32000}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\")\n",
    "config = model.config\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(config.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f27f93",
   "metadata": {},
   "source": [
    "# ✅Out of Memory \n",
    "\n",
    "---\n",
    "\n",
    "1. 비운상태\n",
    "\n",
    "`\n",
    "Total memory     : 14.56 GB\n",
    "Allocated memory : 0.83 GB\n",
    "Reserved memory  : 1.27 GB\n",
    "Usage ratio      : 5.7 %\n",
    "`\n",
    "\n",
    "2. 데이터 로드 후\n",
    "\n",
    "\n",
    "`\n",
    "Total memory     : 14.56 GB\n",
    "Allocated memory : 0.83 GB\n",
    "Reserved memory  : 1.27 GB\n",
    "Usage ratio      : 5.7 %\n",
    "`\n",
    "\n",
    "3. 리스타트해서 마지막에 필요한 것들만 쏙쏙 빼먹음.\n",
    "\n",
    "```python\n",
    "# Hugging Face의 transformers 라이브러리에서 필요한 클래스 불러오기\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# 🔹 1. Tokenizer 불러오기\n",
    "\n",
    "huggingface_tokenizer = AutoTokenizer.from_pretrained('./nsmc-klue-bert-finetuned')\n",
    "\n",
    "# 🔹 2. 모델 불러오기 (분류용)\n",
    "\n",
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    './nsmc-klue-bert-finetuned',  # 사전 학습된 klue/bert-base 모델 사용\n",
    "    num_labels=2                # 출력 클래스 수: 2개 (0 또는 1)\n",
    ")\n",
    "```\n",
    "\n",
    "여기서 klue-bert-finetuned 만 가져옴.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1676e88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import torch\n",
    "\n",
    "# # 삭제하려는 변수 리스트\n",
    "# vars_to_delete = [\n",
    "#     'huggingface_model', 'model',\n",
    "#     'trainer', 'trainer_bucketing',\n",
    "#     'hf_train_dataset', 'hf_val_dataset', 'hf_test_dataset'\n",
    "# ]\n",
    "\n",
    "# # 존재하는 변수만 안전하게 제거\n",
    "# for var_name in vars_to_delete:\n",
    "#     try:\n",
    "#         del globals()[var_name]\n",
    "#     except KeyError:\n",
    "#         pass  # 존재하지 않는 변수면 무시\n",
    "\n",
    "# # Python 가비지 컬렉션 실행\n",
    "# gc.collect()\n",
    "\n",
    "# # PyTorch 캐시 메모리 비우기\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d85b4ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory     : 14.56 GB\n",
      "Allocated memory : 1.66 GB\n",
      "Reserved memory  : 5.57 GB\n",
      "Usage ratio      : 11.4 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 현재 GPU의 ID (보통 0번)\n",
    "gpu_id = 0\n",
    "\n",
    "# 총 메모리\n",
    "total = torch.cuda.get_device_properties(gpu_id).total_memory\n",
    "\n",
    "# 현재 할당된 메모리\n",
    "allocated = torch.cuda.memory_allocated(gpu_id)\n",
    "\n",
    "# 현재 캐시된 메모리 (reserved)\n",
    "reserved = torch.cuda.memory_reserved(gpu_id)\n",
    "\n",
    "print(f\"Total memory     : {total / 1024**3:.2f} GB\")\n",
    "print(f\"Allocated memory : {allocated / 1024**3:.2f} GB\")\n",
    "print(f\"Reserved memory  : {reserved / 1024**3:.2f} GB\")\n",
    "print(f\"Usage ratio      : {allocated / total * 100:.1f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c0e4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
