{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c93c7233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# 미래 버전과의 호환성을 위한 설정 (Python 2/3 코드 혼용 방지용)\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# TensorFlow 및 Keras 백엔드 모듈 불러오기\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# 기본적인 파이썬 표준 라이브러리\n",
    "import os        # 파일 및 디렉토리 경로 처리\n",
    "import re        # 정규 표현식 처리\n",
    "import math      # 수학 함수들\n",
    "import numpy as np      # 수치 계산 및 배열 처리\n",
    "import pandas as pd     # 데이터프레임 처리\n",
    "import random           # 무작위 수 생성\n",
    "import collections      # Counter, defaultdict 등 데이터 구조\n",
    "import json             # JSON 파일 읽기/쓰기\n",
    "import shutil           # 파일 복사, 이동 등\n",
    "import zipfile          # zip 압축 파일 처리\n",
    "import copy             # 객체 깊은 복사\n",
    "from datetime import datetime  # 현재 시각 기록\n",
    "\n",
    "# 시각화를 위한 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SentencePiece: 서브워드 토크나이저 (BPE, Unigram 등)\n",
    "import sentencepiece as spm\n",
    "\n",
    "# tqdm: 진행률 표시 바 (노트북 환경용)\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# 무작위성 제어: 재현 가능한 실험을 위해 시드 고정\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)           # 파이썬 random 모듈 시드 고정\n",
    "np.random.seed(random_seed)        # 넘파이 시드 고정\n",
    "tf.random.set_seed(random_seed)    # 텐서플로우 시드 고정\n",
    "\n",
    "# TensorFlow 버전 확인\n",
    "print(tf.__version__)\n",
    "\n",
    "# 현재 사용 가능한 GPU 디바이스 목록 확인\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# 현재 연결된 GPU 이름 출력 (없을 경우 공백 출력)\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7d98215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터와 모델 경로 설정\n",
    "# os.getenv('HOME')은 현재 사용자의 홈 디렉토리 경로를 불러옵니b다.\n",
    "# 예: '/home/username' 또는 '/Users/username'\n",
    "data_dir = os.getenv('HOME') + '/aiffel/bert_pretrain/data'     # 학습용 데이터가 저장될 경로\n",
    "model_dir = os.getenv('HOME') + '/aiffel/bert_pretrain/models'  # 사전학습된 토크나이저 모델(.model, .vocab)이 저장된 경로\n",
    "\n",
    "# SentencePiece 토크나이저 불러오기\n",
    "vocab = spm.SentencePieceProcessor()  # SentencePieceProcessor 객체 생성\n",
    "vocab.load(f\"{model_dir}/ko_32000.model\")  # 학습된 서브워드 모델(.model 파일)을 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31dd4b2",
   "metadata": {},
   "source": [
    "# 1. Sentence Piece 코드를 이해하기 위한 코드\n",
    "\n",
    "**목표**\n",
    "\n",
    "1. 토큰과 인덱스를 자유자재로 변환할수 있는지\n",
    "2. 특수 토큰들은 7개임을 확인해볼 수 있었음.\n",
    "3. encode -> decode 흐름을 알아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6be74513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1️⃣ 예시 단어 '_기' → ID 확인\n",
      "단어 '_기'의 ID: 1\n",
      "\n",
      "2️⃣ ID → 서브워드 매핑 확인 (ID 0~10)\n",
      "ID 0의 토큰: [PAD]\n",
      "ID 1의 토큰: [UNK]\n",
      "ID 2의 토큰: [BOS]\n",
      "ID 3의 토큰: [EOS]\n",
      "ID 4의 토큰: [SEP]\n",
      "ID 5의 토큰: [CLS]\n",
      "ID 6의 토큰: [MASK]\n",
      "ID 7의 토큰: ▁1\n",
      "ID 8의 토큰: ▁이\n",
      "ID 9의 토큰: 으로\n",
      "ID 10의 토큰: 에서\n",
      "\n",
      "3️⃣ 서브워드 '_1'의 ID → 다시 토큰 확인\n",
      "'_1'의 ID: 1035\n",
      "ID → 토큰: _1\n",
      "\n",
      "4️⃣ ID 0이 UNK([UNK])인지 확인\n",
      "ID 0이 UNK인가?: False\n",
      "\n",
      "5️⃣ 전체 encode → decode 흐름\n",
      "입력 문장: 나는 오늘 기분이 좋다\n",
      "서브워드 (문자 토큰): ['▁나는', '▁오늘', '▁기', '분이', '▁좋다']\n",
      "ID 목록: [4344, 1613, 24, 4054, 9328]\n",
      "복원된 문장: 나는 오늘 기분이 좋다\n"
     ]
    }
   ],
   "source": [
    "#===================================================\n",
    "# 토크나이저 Sentence Piece 사용법을 알아보기\n",
    "#===================================================\n",
    "\n",
    "# 1️⃣ 예시 단어 → ID 확인\n",
    "print(\"1️⃣ 예시 단어 '_기' → ID 확인\")\n",
    "print(\"단어 '_기'의 ID:\", vocab.piece_to_id(\"_기\"))  # '_'는 공백을 의미 (▁기)\n",
    "\n",
    "# 2️⃣ ID → 서브워드 확인\n",
    "print(\"\\n2️⃣ ID → 서브워드 매핑 확인 (ID 0~10)\")\n",
    "for i in range(11):\n",
    "    print(f\"ID {i}의 토큰:\", vocab.id_to_piece(i))\n",
    "\n",
    "# 3️⃣ 서브워드 → ID → 다시 서브워드\n",
    "print(\"\\n3️⃣ 서브워드 '_1'의 ID → 다시 토큰 확인\")\n",
    "piece = \"_1\"\n",
    "piece_id = vocab.piece_to_id(piece)\n",
    "print(f\"'{piece}'의 ID:\", piece_id)\n",
    "print(\"ID → 토큰:\", vocab.id_to_piece(piece_id))\n",
    "\n",
    "# 4️⃣ UNK 여부 확인\n",
    "print(\"\\n4️⃣ ID 0이 UNK([UNK])인지 확인\")\n",
    "print(\"ID 0이 UNK인가?:\", vocab.is_unknown(0))\n",
    "\n",
    "# 5️⃣ 인코딩 & 디코딩 전체 흐름 확인\n",
    "print(\"\\n5️⃣ 전체 encode → decode 흐름\")\n",
    "text = \"나는 오늘 기분이 좋다\"\n",
    "ids = vocab.encode(text)\n",
    "tokens = vocab.encode(text, out_type=str)\n",
    "reconstructed = vocab.decode(ids)\n",
    "\n",
    "print(\"입력 문장:\", text)\n",
    "print(\"서브워드 (문자 토큰):\", tokens)\n",
    "print(\"ID 목록:\", ids)\n",
    "print(\"복원된 문장:\", reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d59a960",
   "metadata": {},
   "source": [
    "# 2. Vocab_list를 만들어보기\n",
    "\n",
    "## **왜 Vocab_list를 만들어야 하는걸까?**\n",
    "\n",
    "BERT에서의 Pretrained 에서는 두가지 주요 학습 과제가 존재한다. \n",
    "\n",
    "문장 간 관계를 예측하는 **NSP(Next Sentence Prediction)** \n",
    "\n",
    "문장 내 일부 토큰을 가려놓고 원래의 단어를 예측하는 **MLM(Masked Language Modeling)** 이다.\n",
    "\n",
    "이 중 MLM은 입력 문장에서 일부 토큰을 무작위로 선택하여 [MASK]로 가리고 모델이 그 위치에 어떤 단어가 들어갈지 예측한다.\n",
    "\n",
    "\n",
    "이 과정을 구현할 때 중요한 점은, 무작위로 선택된 토큰이 모두 예측 가능한 '일반적인 단어'여야 한다는 것. \n",
    "\n",
    "만약 [MASK]로 가린 자리에 원래 들어있던 단어가 [PAD]나 [UNK] 같은 특수 토큰이라면, \n",
    "\n",
    "모델은 아무리 학습해도 유의미한 결과를 낼 수 없습니다. \n",
    "\n",
    "이런 특수 토큰은 문장의 구조나 입력 처리에는 필요하지만, \n",
    "\n",
    "실제 의미 예측의 대상이 될 수는 없기 때문입니다.\n",
    "\n",
    "---\n",
    "\n",
    "이러한 이유로, 일반적인 토큰만 모아둔 리스트가 필요하며, \n",
    "\n",
    "그것이 바로 vocab_list이다. \n",
    "\n",
    "vocab_list는 SentencePiece로부터 얻은 전체 토큰 집합에서 [PAD], [UNK], [CLS], [SEP], [MASK] 등과 같이 모델 구조나 문장 경계를 처리하기 위한 특수 토큰들을 제외한 나머지 토큰들만 추출하여 만든 리스트입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0d53c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁1', '▁이', '으로', '에서', '▁있', '▁2', '▁그', '▁대', '▁사', '이다']\n"
     ]
    }
   ],
   "source": [
    "vocab_list = []\n",
    "\n",
    "for id in range(7, 8007): # 8007을 입력해 8000개의 vocab_size를 확보한다.\n",
    "        if not vocab.is_unknown(id): # 특수 토큰이 아니라면\n",
    "            token = vocab.id_to_piece(id) # 토큰 문자열로 변환하고\n",
    "            vocab_list.append(token) # vocab_list에 append 한다.\n",
    "\n",
    "print(vocab_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bb9cec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "string_a = \"추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에\"\n",
    "string_b = \"손바닥 위엔 기쁨의 눈물이 흘러 컬컬한 목에 모주 한잔을 적셔 몇 달 포 전부터 콜록거리는 아내 생각에 그토록 먹고 싶다던\"\n",
    "tokens_org = [\"[CLS]\"] + vocab.encode_as_pieces(string_a) + [\"[SEP]\"] + vocab.encode_as_pieces(string_b) + [\"[SEP]\"]\n",
    "print(tokens_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d709d2e",
   "metadata": {},
   "source": [
    "# 3. 데이터 전처리 (1) MASK 생성\n",
    "\n",
    "## 3.1 원활한 MLM을 위한 띄어쓰기 기준으로 마스킹하기 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "720931c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokens_org)\n",
    "\n",
    "# 전체 token의 15% mask\n",
    "mask_cnt = int((len(tokens_org) - 3) * 0.15)\n",
    "mask_cnt# 마스크해야하는 단어의 수만 알 수 있는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d7446",
   "metadata": {},
   "source": [
    "Subword 기반으로 토크나이징 했을 때 띄어쓰기가 없다면 한 단어 내의 것들이 마킹될 수 있기 때문에 먼저 **띄어쓰기 단위로 마스킹**해주는 작업을 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34feb4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3] ['▁추적', '추', '적']\n",
      "[4] ['▁비가']\n",
      "[5] ['▁내리는']\n",
      "[6, 7, 8] ['▁날', '이었', '어']\n",
      "[9, 10] ['▁그날', '은']\n",
      "[11, 12, 13] ['▁', '왠', '지']\n",
      "[14, 15] ['▁손', '님이']\n",
      "[16] ['▁많아']\n",
      "[17] ['▁첫']\n",
      "[18] ['▁번에']\n",
      "[19, 20] ['▁삼', '십']\n",
      "[21] ['▁전']\n",
      "[22, 23] ['▁둘째', '번']\n",
      "[24, 25] ['▁오', '십']\n",
      "[26] ['▁전']\n",
      "[27, 28] ['▁오랜', '만에']\n",
      "[29, 30] ['▁받아', '보는']\n",
      "[31] ['▁십']\n",
      "[32, 33] ['▁전', '짜리']\n",
      "[34, 35, 36] ['▁백', '통', '화']\n",
      "[37, 38, 39] ['▁서', '푼', '에']\n",
      "[41] ['▁손바닥']\n",
      "[42, 43] ['▁위', '엔']\n",
      "[44, 45] ['▁기쁨', '의']\n",
      "[46, 47] ['▁눈', '물이']\n",
      "[48] ['▁흘러']\n",
      "[49, 50, 51] ['▁컬', '컬', '한']\n",
      "[52] ['▁목에']\n",
      "[53, 54] ['▁모', '주']\n",
      "[55, 56, 57] ['▁한', '잔', '을']\n",
      "[58, 59] ['▁적', '셔']\n",
      "[60] ['▁몇']\n",
      "[61] ['▁달']\n",
      "[62] ['▁포']\n",
      "[63] ['▁전부터']\n",
      "[64, 65, 66] ['▁콜', '록', '거리는']\n",
      "[67] ['▁아내']\n",
      "[68] ['▁생각에']\n",
      "[69, 70] ['▁그', '토록']\n",
      "[71] ['▁먹고']\n",
      "[72, 73] ['▁싶다', '던']\n"
     ]
    }
   ],
   "source": [
    "cand_idx = []  # word 단위의 토큰 인덱스를 담는 리스트. 예: [[1,2], [3], [4,5,6], ...]\n",
    "\n",
    "for (i, token) in enumerate(tokens_org):  # 전체 토큰을 하나씩 순회하면서\n",
    "    if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "        continue  # 특수 토큰은 마스킹 후보에서 제외\n",
    "\n",
    "    # u\"\\u2581\"는 '▁' 문자 → SentencePiece에서 단어의 시작을 의미\n",
    "    # 현재 토큰이 '▁'로 시작하지 않으면 이전 단어의 연속 토큰으로 간주\n",
    "    \n",
    "    # 지금 candidate_idx 라는 리스트에 하나 이상의 단어리스트가 있는지, 그리고 _로 시작하지 않았을 경우에는\n",
    "    if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"): \n",
    "        cand_idx[-1].append(i)  # 이전 단어에 이어 붙임 (예: '▁추적', '추', '적' → 같은 단어)\n",
    "    else: # _로 시작했을 경우에는\n",
    "        cand_idx.append([i])  # 새로운 단어의 시작 → 새 리스트 생성하여 추가\n",
    "\n",
    "# 결과 확인: 단어 단위로 묶인 인덱스와 대응하는 토큰들 출력\n",
    "for cand in cand_idx:\n",
    "    print(cand, [tokens_org[i] for i in cand])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc06ed1",
   "metadata": {},
   "source": [
    "## 3.2 Random_mask 적용하기\n",
    "\n",
    "### ❓ 왜 index shuffle 을 해야하냐?\n",
    "\n",
    "vocab_index : 랜덤한 다른 단어로 바꿀 때 선택 대상이 되는 토큰 리스트\n",
    "\n",
    "idex_shuffle을 하는 대상은 **cand_idx**임\n",
    "\n",
    "cand_idx는, 한 문장 내의 마스킹 후보 단어 인덱스를 섞는 작업이다.\n",
    "\n",
    "목적은 문장 내 다양한 위치의 단어들을 무작위로 선택하기 위해서이다.\n",
    "\n",
    "자세한건 콬드를 통해 한번 보자\n",
    "\n",
    "\n",
    "### ❓ 왜 Random_mask 를 적용해야하냐?\n",
    "\n",
    "Random_mask는 단순히 [MASK]로만 가리는 것이 아니라\n",
    "\n",
    "마스킹 대상 토큰 중 일부를 랜덤 토큰 또는 그대로 유지하는 거라서 확률에 따라 결정된다.\n",
    "\n",
    "확률에서 특이한건 일정 비율을 주는게 아니라, dice라는 진짜 주사위를 굴려버린다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b12eb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1️⃣ 셔플이 돌아간 cand_idx : 이 중에서 앞에서 15% 만큼 마스킹하기 때문에 셔플해줘야한다. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[18],\n",
       " [44, 45],\n",
       " [24, 25],\n",
       " [49, 50, 51],\n",
       " [31],\n",
       " [63],\n",
       " [41],\n",
       " [52],\n",
       " [22, 23],\n",
       " [71],\n",
       " [17],\n",
       " [19, 20],\n",
       " [60],\n",
       " [32, 33],\n",
       " [62],\n",
       " [46, 47],\n",
       " [29, 30],\n",
       " [72, 73],\n",
       " [6, 7, 8],\n",
       " [64, 65, 66],\n",
       " [67],\n",
       " [9, 10],\n",
       " [26],\n",
       " [55, 56, 57],\n",
       " [61],\n",
       " [34, 35, 36],\n",
       " [37, 38, 39],\n",
       " [21],\n",
       " [58, 59],\n",
       " [48],\n",
       " [69, 70],\n",
       " [4],\n",
       " [27, 28],\n",
       " [42, 43],\n",
       " [14, 15],\n",
       " [68],\n",
       " [5],\n",
       " [11, 12, 13],\n",
       " [1, 2, 3],\n",
       " [16],\n",
       " [53, 54]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random mask를 위해서 index 순서를 섞음\n",
    "# 순서를 섞는 이유는 dice는 마스킹 방식을 정하는 랜덤임. \n",
    "random.shuffle(cand_idx)\n",
    "\n",
    "print(\"1️⃣ 셔플이 돌아간 cand_idx : 이 중에서 앞에서 15% 만큼 마스킹하기 때문에 셔플해줘야한다. \")\n",
    "\n",
    "cand_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a8eee22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '[MASK]', '▁삼', '십', '▁전', '▁둘째', '번', '[MASK]', '[MASK]', '▁전', '▁오랜', '만에', '▁받아', '보는', 'ud', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '[MASK]', '[MASK]', '▁눈', '물이', '▁흘러', '[MASK]', '[MASK]', '[MASK]', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# tokens가 마스킹되므로, 원본 tokens_org를 복사해서 사용 (테스트 반복용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "# mask_lms: 마스킹된 토큰들의 인덱스와 정답(label) 저장용 리스트\n",
    "mask_lms = []\n",
    "\n",
    "# cand_idx: 단어 단위로 묶인 토큰 인덱스 리스트\n",
    "for index_set in cand_idx:\n",
    "    # 현재까지 마스킹한 토큰 수가 mask_cnt(15%) 이상이면 중단\n",
    "    if len(mask_lms) >= mask_cnt:\n",
    "        break\n",
    "\n",
    "    # 이번에 마스킹하려는 단어(index_set 전체)를 추가했을 때 15%를 초과하면 skip <- 한 단어에서 추론하는 걸 방지하기 위함. 이거 없애도 되지 않나?\n",
    "    if len(mask_lms) + len(index_set) > mask_cnt:\n",
    "        continue\n",
    "\n",
    "    # 0.0 ~ 1.0 사이의 랜덤 확률 생성 → 어떤 방식으로 마스킹할지 결정하는 데 사용\n",
    "    dice = random.random()\n",
    "\n",
    "    # 단어(= index_set)를 구성하는 각 토큰에 대해 마스킹 적용\n",
    "    for index in index_set:\n",
    "        masked_token = None\n",
    "\n",
    "        if dice < 0.8:\n",
    "            # 80% 확률로 [MASK] 토큰으로 대체\n",
    "            masked_token = \"[MASK]\"\n",
    "        elif dice < 0.9:\n",
    "            # 10% 확률로 원래 토큰 유지 (identity)\n",
    "            masked_token = tokens[index]\n",
    "        else:\n",
    "            # 10% 확률로 다른 랜덤 토큰으로 대체 (vocab_list에서 선택)\n",
    "            masked_token = random.choice(vocab_list)\n",
    "\n",
    "        # 마스킹된 토큰 정보 저장: 원래 위치와 정답 레이블\n",
    "        mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "\n",
    "        # 실제 마스킹 적용\n",
    "        tokens[index] = masked_token\n",
    "\n",
    "# 결과 출력\n",
    "print(\"tokens_org\")  # 원본 (마스킹 전) 토큰\n",
    "print(tokens_org, \"\\n\")\n",
    "\n",
    "print(\"tokens\")  # 마스킹된 토큰\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f46394",
   "metadata": {},
   "source": [
    "### 토큰을 보게 되면 알 수 있는 것\n",
    "\n",
    "1. shuffle 을 했을 때 위에 있는 것 부터 하나씩 마스킹되어간다는 것을 알 수 있음. \n",
    "2. [34, 35, 36],[52],[44, 45],[9, 10],[55, 56, 57] 딱 이만큼 이제 마스킹된거임.\n",
    "3. 그래서 Shuffle 후 랜덤 다이스 돌려서 마스킹하는 거임!\n",
    "\n",
    "여기까지는 완벽 이해 완료 \n",
    "\n",
    "## 3.3 학습용 데이터로 정리하기\n",
    "\n",
    "마스킹이 적용된 토큰들에 대해, 어디 위치가 마스킹되었고, 정답을 알려주는 과정이다.\n",
    "\n",
    "1. 마스킹 리스트 정렬을 한다. (Merge Sort)\n",
    "2. 마스킹 인덱스만 추출한다.\n",
    "3. 마스킹된 정답 토큰을 추출한다. (원래 있었던 정답 토큰만 추출한다.) \n",
    "4. 최종적으로 학습에 쓰일 마스킹 위치와 정답 토큰을 출력하여 디버깅하거나 검토한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27b4861d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_idx   : [18, 24, 25, 31, 44, 45, 49, 50, 51, 63]\n",
      "mask_label : ['▁번에', '▁오', '십', '▁십', '▁기쁨', '의', '▁컬', '컬', '한', '▁전부터']\n"
     ]
    }
   ],
   "source": [
    "# 마스킹된 토큰 정보를 index 기준으로 정렬\n",
    "# 예: [{\"index\": 8, \"label\": \"다\"}, {\"index\": 2, \"label\": \"▁기\"}] → [{\"index\": 2, \"label\": \"▁기\"}, {\"index\": 8, \"label\": \"다\"}]\n",
    "mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "\n",
    "# 마스킹된 위치만 추출하여 리스트로 저장 (모델이 예측할 위치 정보)\n",
    "mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "\n",
    "# 마스킹된 위치에 원래 있었던 정답 토큰만 추출하여 리스트로 저장 (모델이 예측해야 할 정답)\n",
    "mask_label = [p[\"label\"] for p in mask_lms]\n",
    "\n",
    "# 결과 확인\n",
    "print(\"mask_idx   :\", mask_idx)    # 예: [2, 8]\n",
    "print(\"mask_label :\", mask_label)  # 예: ['▁기', '다']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae360691",
   "metadata": {},
   "source": [
    "## 3.4 모듈화 진행하기 \n",
    "group_tokens_by_word(): 서브워드 토큰을 단어 단위 인덱스로 묶기\n",
    "\n",
    "shuffle_candidates(): 마스킹 후보 인덱스 셔플\n",
    "\n",
    "apply_random_mask() : MLM 방식에 따라 실제 마스킹 적용\n",
    "\n",
    "extract_mask_targets() : 정답 레이블 및 인덱스 정리\n",
    "\n",
    "create_masked_tokens(): 전체 마스킹 파이프라인 통합 실행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0732d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_tokens_by_word(tokens_org):\n",
    "    cand_idx = []  # word 단위의 토큰 인덱스를 담는 리스트. 예: [[1,2], [3], [4,5,6], ...]\n",
    "    for (i, token) in enumerate(tokens_org):  # 전체 토큰을 하나씩 순회하면서\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue  # 특수 토큰은 마스킹 후보에서 제외\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):\n",
    "            cand_idx[-1].append(i)  # 이전 단어에 이어 붙임 (예: '▁추적', '추', '적' → 같은 단어)\n",
    "        else:\n",
    "            cand_idx.append([i])  # 새로운 단어의 시작 → 새 리스트 생성하여 추가\n",
    "\n",
    "#     # 결과 확인: 단어 단위로 묶인 인덱스와 대응하는 토큰들 출력\n",
    "#     for cand in cand_idx:\n",
    "#         print(cand, [tokens_org[i] for i in cand])\n",
    "    return cand_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca31b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_candidates(cand_idx):\n",
    "    random.shuffle(cand_idx)\n",
    "    return cand_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "934239b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_random_mask(tokens_org, cand_idx, mask_cnt, voacb_list):\n",
    "    \n",
    "    tokens = copy.deepcopy(tokens_org)                 # 복사용\n",
    "    mask_lms = []\n",
    "\n",
    "    for index_set in cand_idx:                        # 셔플된거에서 순회 시작\n",
    "        if len(mask_lms) >= mask_cnt:                 # 정지 조건\n",
    "            break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt: # 스킵 조건\n",
    "            continue\n",
    "\n",
    "        dice = random.random()                        # 다이스 굴리고\n",
    "    \n",
    "        for index in index_set:                       # MLM 규칙\n",
    "            if dice < 0.8:\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < 0.9:\n",
    "                masked_token = tokens[index]\n",
    "            else:\n",
    "                masked_token = random.choice(vocab_list)\n",
    "\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            tokens[index] = masked_token\n",
    "\n",
    "    return tokens, mask_lms                          # 토큰이랑, mask_lms에 딕셔너리를 가지고 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bb9e2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mask_targets(mask_lms):\n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]\n",
    "    return mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "883b52b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_tokens(tokens_org, mask_cnt, vocab_list):\n",
    "    cand_idx = group_tokens_by_word(tokens_org)\n",
    "    cand_idx = shuffle_candidates(cand_idx)\n",
    "    tokens, mask_lms = apply_random_mask(tokens_org, cand_idx, mask_cnt, vocab_list)\n",
    "    mask_idx, mask_label = extract_mask_targets(mask_lms)\n",
    "\n",
    "    return tokens, mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a0315ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본: ['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]']\n",
      "\n",
      "================================================================================================\n",
      "\n",
      "마스킹된: ['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '[MASK]', '[MASK]', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '[MASK]', '▁컬', '컬', '한', '[MASK]', '▁않기', '▁담고', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '[MASK]', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]']\n",
      "\n",
      "================================================================================================\n",
      "\n",
      "mask_idx: [16, 17, 37, 38, 39, 48, 52, 53, 54, 67]\n",
      "\n",
      "================================================================================================\n",
      "\n",
      "mask_label: ['▁많아', '▁첫', '▁서', '푼', '에', '▁흘러', '▁목에', '▁모', '주', '▁아내']\n"
     ]
    }
   ],
   "source": [
    "tokens, mask_idx, mask_label = create_masked_tokens(tokens_org, mask_cnt, vocab_list)\n",
    "\n",
    "print(\"원본:\", tokens_org)\n",
    "print(\"\")\n",
    "print(\"================================================================================================\")\n",
    "print(\"\")\n",
    "print(\"마스킹된:\", tokens)\n",
    "print(\"\")\n",
    "print(\"================================================================================================\")\n",
    "print(\"\")\n",
    "print(\"mask_idx:\", mask_idx)\n",
    "print(\"\")\n",
    "print(\"================================================================================================\")\n",
    "print(\"\")\n",
    "print(\"mask_label:\", mask_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47177115",
   "metadata": {},
   "source": [
    "# 4. 데이터 전처리(2) NSP pair 생성\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56f8f0d",
   "metadata": {},
   "source": [
    "**NSP pair란?**  BERT 사전학습 시 사용되는 **문장 A와 문장 B의 쌍**으로,  \n",
    "문장 B가 실제로 문장 A 다음에 오는 문장인지 (`IsNext`)  \n",
    "아니면 무작위로 끌어온 문장인지 (`NotNext`)를 모델이 예측하도록 구성된 데이터입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dc41db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"추적추적 비가 내리는 날이었어\n",
    "그날은 왠지 손님이 많아\n",
    "첫 번에 삼십 전 둘째 번 오십 전\n",
    "오랜만에 받아보는 십 전짜리 백통화 서푼에\n",
    "손바닥 위엔 기쁨의 눈물이 흘러\n",
    "컬컬한 목에 모주 한잔을 적셔\n",
    "몇 달 포 전부터 콜록거리는 아내\n",
    "생각에 그토록 먹고 싶다던\n",
    "설렁탕 한 그릇을 이제는 살 수 있어\n",
    "집으로 돌아가는 길 난 문득 떠올라\n",
    "아내의 목소리가 거칠어만 가는 희박한 숨소리가\n",
    "오늘은 왠지 나가지 말라던 내 옆에 있어 달라던\n",
    "그리도 나가고 싶으면 일찍이라도 들어와 달라던\n",
    "아내의 간절한 목소리가 들려와\n",
    "나를 원망하듯 비는 점점 거세져\n",
    "싸늘히 식어가는 아내가 떠올라 걱정은 더해져\n",
    "난 몰라 오늘은 운수 좋은 날\n",
    "난 맨날 이렇게 살 수 있으면 얼마나 좋을까\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8435dc2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어'],\n",
       " ['▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아'],\n",
       " ['▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 줄 단위로 tokenize\n",
    "doc = [vocab.encode_as_pieces(line) for line in string.split(\"\\n\")]\n",
    "# encode_as_pieces(line) 을 통해서 분리를 진행한다.\n",
    "doc[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "799cb59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 입력 시퀀스의 최대 길이 (예: BERT 모델이 처리할 수 있는 최대 길이)\n",
    "n_test_seq = 64\n",
    "\n",
    "# 너무 짧은 문장은 학습 효율이 낮기 때문에 최소 길이 설정\n",
    "min_seq = 8\n",
    "\n",
    "# 실제 입력 문장(tokens_a + tokens_b)의 최대 길이\n",
    "# BERT는 [CLS], [SEP], [SEP] 총 3개의 특수 토큰을 사용하므로 그만큼 빼줌\n",
    "max_seq = n_test_seq - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b695244",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_chunk: 7 66 [['▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어'], ['▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아'], ['▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전'], ['▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러'], ['▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내']]\n",
      "tokens_a: 16 ['▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아']\n",
      "tokens_b: 50 ['▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내']\n",
      "\n",
      "current_chunk: 7 65 [['▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던'], ['▁설', '렁', '탕', '▁한', '▁그릇', '을', '▁이제는', '▁살', '▁수', '▁있어'], ['▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라'], ['▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁가는', '▁희', '박한', '▁숨', '소', '리가'], ['▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던'], ['▁아내의', '▁간', '절한', '▁목소리가', '▁들려', '와']]\n",
      "tokens_a: 35 ['▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '▁설', '렁', '탕', '▁한', '▁그릇', '을', '▁이제는', '▁살', '▁수', '▁있어', '▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라', '▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁가는', '▁희', '박한', '▁숨', '소', '리가']\n",
      "tokens_b: 30 ['▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던', '▁아내의', '▁간', '절한', '▁목소리가', '▁들려', '와']\n",
      "\n",
      "current_chunk: 4 41 [['▁나를', '▁원', '망', '하', '듯', '▁비는', '▁점점', '▁거세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '▁걱', '정은', '▁더', '해져'], ['▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까']]\n",
      "tokens_a: 30 ['▁나를', '▁원', '망', '하', '듯', '▁비는', '▁점점', '▁거세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '▁걱', '정은', '▁더', '해져', '▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날']\n",
      "tokens_b: 11 ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_chunk = []   # 한 문단(doc) 안에서 라인(문장) 단위로 토큰을 모을 리스트\n",
    "current_length = 0   # current_chunk에 있는 모든 토큰의 총 길이\n",
    "\n",
    "for i in range(len(doc)):  # 문단(doc)의 각 문장을 하나씩 순회\n",
    "    current_chunk.append(doc[i])           # 해당 줄의 토큰을 current_chunk에 추가\n",
    "    current_length += len(doc[i])          # 해당 줄의 토큰 수를 current_length에 누적\n",
    "\n",
    "    # 조건: 문장이 2줄 이상 쌓였고, \n",
    "    # (1) 현재 줄이 문단의 마지막이거나, \n",
    "    # (2) 누적된 토큰 수가 최대 허용 길이(max_seq)를 넘으면\n",
    "    # 학습 데이터 생성을 위해 쪼개기 시작\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):\n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        ##########################################\n",
    "        # ⬇ NSP를 위한 token A/B 분할 과정 시작\n",
    "\n",
    "        # token_a: 앞부분 문장들 (1개 이상)\n",
    "        a_end = 1  # 최소 한 문장은 A에 포함\n",
    "        if 1 < len(current_chunk):  # 총 문장이 2줄 이상이면\n",
    "            # 랜덤한 지점에서 문장을 자름 (0 ~ a_end-1 → A, 나머지 → B)\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "\n",
    "        tokens_a = []  # token A 초기화\n",
    "        for j in range(a_end):  # 0부터 a_end 직전까지\n",
    "            tokens_a.extend(current_chunk[j])  # 각 문장의 토큰들을 token_a에 붙이기\n",
    "\n",
    "        # token_b: 나머지 뒷부분 문장들\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):  # a_end부터 끝까지\n",
    "            tokens_b.extend(current_chunk[j])  # 토큰들을 token_b에 이어붙임\n",
    "\n",
    "        # 결과 출력\n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        ##########################################\n",
    "\n",
    "        print()  # 구분을 위한 빈 줄 출력\n",
    "\n",
    "        # 다음 NSP 샘플 생성을 위해 초기화\n",
    "        current_chunk = []\n",
    "        current_length = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1b6840",
   "metadata": {},
   "source": [
    "**trim_tokens(tokens_a, tokens_b, max_seq)**\n",
    "\n",
    "전체 길이가 max_seq를 넘으면 긴 쪽에서 토큰을 삭제합니다.\n",
    "\n",
    "삭제 방식:\n",
    "\n",
    "1. tokens_a가 더 길면 앞에서 삭제 (del tokens_a[0])\n",
    "\n",
    "2. tokens_b가 더 길면 뒤에서 삭제 (tokens_b.pop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "118cd526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
    "    :param tokens_a: tokens A\n",
    "    :param tokens_b: tokens B\n",
    "    :param max_seq: 두 tokens 길이의 최대 값\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            del tokens_a[0]\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa87f8",
   "metadata": {},
   "source": [
    "**create_nsp_pairs_from_doc(doc, max_seq):**\n",
    "\n",
    "1. 여러 문장을 모아서 current_chunk에 누적합니다.\n",
    "\n",
    "2. 문장 누적 길이가 max_seq를 넘거나 문단 끝에 도달하면 다음 단계로 넘어갑니다.\n",
    "\n",
    "3. 누적된 문장들을 랜덤 지점에서 tokens_a와 tokens_b로 나눕니다.\n",
    "\n",
    "4. 50% 확률로 tokens_a와 tokens_b를 바꿔치기 → 이때 is_next = 0 (False)\n",
    "\n",
    "5. 총 길이가 max_seq보다 길면 잘라냄 → trim_tokens 호출\n",
    "\n",
    "6. 둘 다 비어있지 않은 경우 NSP 데이터로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "060980a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nsp_pairs_from_doc(doc, max_seq):\n",
    "    \"\"\"\n",
    "    하나의 문단(doc)을 받아 NSP 학습용 문장쌍 생성\n",
    "    :param doc: 문단 (토큰화된 문장 리스트들의 리스트)\n",
    "    :param max_seq: 최대 길이 ([CLS], [SEP], [SEP] 제외한 길이)\n",
    "    :return: NSP 쌍 리스트 [(tokens_a, tokens_b, is_next), ...]\n",
    "    \"\"\"\n",
    "    nsp_data = []\n",
    "\n",
    "    current_chunk = []   # 현재 모으고 있는 문장들\n",
    "    current_length = 0   # 모인 토큰 길이 누적\n",
    "\n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])\n",
    "        current_length += len(doc[i])\n",
    "\n",
    "        if len(current_chunk) > 1 and (i == len(doc) - 1 or current_length >= max_seq):\n",
    "            # 랜덤 지점에서 문장 나누기\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "            tokens_b = []\n",
    "            for j in range(a_end, len(current_chunk)):\n",
    "                tokens_b.extend(current_chunk[j])\n",
    "\n",
    "            # 50% 확률로 NotNext 만들기\n",
    "            if random.random() < 0.5:\n",
    "                is_next = 0  # False\n",
    "                tokens_a, tokens_b = tokens_b, tokens_a  # 순서 뒤집기\n",
    "            else:\n",
    "                is_next = 1  # True\n",
    "\n",
    "            # 최대 길이 초과 시 잘라내기\n",
    "            trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "\n",
    "            # 최소 길이 보장\n",
    "            if len(tokens_a) > 0 and len(tokens_b) > 0:\n",
    "                nsp_data.append((tokens_a, tokens_b, is_next))\n",
    "\n",
    "            # 초기화\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "\n",
    "    return nsp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8d54278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "\n",
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    하나의 문단(doc)을 받아 BERT pretrain용 인스턴스 생성 (NSP + MLM)\n",
    "    :param vocab: {token: id} 형태의 vocab 딕셔너리\n",
    "    :param doc: 문장 단위 토큰 리스트 예: [['나는', '학생이다'], ['오늘은', '날씨가', '좋다']]\n",
    "    :param n_seq: 전체 시퀀스 최대 길이 (ex: 128)\n",
    "    :param mask_prob: 마스킹 확률 (ex: 0.15)\n",
    "    :param vocab_list: 마스킹 대상이 될 수 있는 일반 단어 리스트\n",
    "    :return: instance 리스트 (dict 형태)\n",
    "    \"\"\"\n",
    "    max_seq = n_seq - 3  # [CLS], [SEP], [SEP] 제외한 순수 토큰 최대 길이\n",
    "    instances = []\n",
    "\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])\n",
    "        current_length += len(doc[i])\n",
    "\n",
    "        if len(current_chunk) > 1 and (i == len(doc) - 1 or current_length >= max_seq):\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "            tokens_b = []\n",
    "            for j in range(a_end, len(current_chunk)):\n",
    "                tokens_b.extend(current_chunk[j])\n",
    "\n",
    "            # 50% 확률로 NSP False\n",
    "            if random.random() < 0.5:\n",
    "                is_next = 0\n",
    "                tokens_a, tokens_b = tokens_b, tokens_a\n",
    "            else:\n",
    "                is_next = 1\n",
    "\n",
    "            # 길이 초과 시 자르기\n",
    "            trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "            if len(tokens_a) == 0 or len(tokens_b) == 0:\n",
    "                continue\n",
    "\n",
    "            # tokens, segment 생성\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "            # 마스킹 (deepcopy로 원본 보존)\n",
    "            tokens_masked, mask_idx, mask_label = create_masked_tokens(copy.deepcopy(tokens), int((len(tokens) - 3) * mask_prob), vocab_list)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens_masked,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "\n",
    "    return instances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55dbde6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '▁둘째', '조차', '▁오', '십', '▁꺼', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '[MASK]', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '[MASK]', '[MASK]', '▁콜', '록', '거리는', '▁아내', '[SEP]', '▁추적', '추', '적', '[MASK]', '[MASK]', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁노벨', '헝가', '▁많아', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [2, 5, 26, 40, 41, 50, 51, 60, 61], 'mask_label': ['▁번', '▁전', '▁흘러', '▁포', '▁전부터', '▁비가', '▁내리는', '▁손', '님이']}\n",
      "{'tokens': ['[CLS]', '▁생각에', '▁사용', '▁이로써', '▁먹고', '▁싶다', '던', '▁설', '렁', '탕', '▁한', '▁그릇', '을', '▁이제는', '▁살', '[MASK]', '▁있어', '▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라', '[SEP]', '▁아내의', '▁목소리가', '▁이들은', '계는', '(15', '▁가는', '▁희', '박한', '▁숨', '소', '리가', '▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '[MASK]', '▁있어', '▁달라', '던', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던', '▁아내의', '▁간', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [2, 3, 15, 28, 29, 30, 46, 50, 51], 'mask_label': ['▁그', '토록', '▁수', '▁거칠', '어', '만', '▁옆에', '▁그리', '도']}\n",
      "{'tokens': ['[CLS]', '▁싸', '늘', '히', '[MASK]', '[MASK]', '[MASK]', '▁아내가', '[MASK]', '[MASK]', '▁걱', '정은', '▁더', '해져', '▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까', '[SEP]', '▁나를', '▁원', '망', '하', '듯', '▁비는', '▁점점', '▁거세', '져', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [4, 5, 6, 8, 9, 34], 'mask_label': ['▁식', '어', '가는', '▁떠올', '라', '▁나를']}\n"
     ]
    }
   ],
   "source": [
    "instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "\n",
    "# 최종 데이터셋 결과 확인\n",
    "for instance in instances:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a9459d",
   "metadata": {},
   "source": [
    "# 5. 데이터 전처리(3) 데이터셋 완성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e997b444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3957761"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "\n",
    "# line count 확인\n",
    "total = 0\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    for line in in_f:\n",
    "        total += 1\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62bde99",
   "metadata": {},
   "source": [
    "**이 다음 코드가 존재하는 이유**\n",
    "\n",
    "각 문단의 문장 수와 대표 문장을 확인하여,\n",
    "\n",
    "실제로 NSP 학습이 가능한 문단인지\n",
    "\n",
    "너무 짧거나 문장이 없는 문단이 많은지\n",
    "\n",
    "주제별로 분리된 자연스러운 문단인지 확인하기 위해 샘플 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7fe8da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def pretty_print_instance(instance, max_len=30):\n",
    "    print(\"▶ is_next:\", instance['is_next'])\n",
    "    \n",
    "    # tokens 예쁘게 출력\n",
    "    tokens_preview = instance['tokens'][:max_len]\n",
    "    if len(instance['tokens']) > max_len:\n",
    "        tokens_preview.append('...')\n",
    "    print(\"▶ tokens:\", tokens_preview)\n",
    "    \n",
    "    # segment 예쁘게 출력\n",
    "    segment_preview = instance['segment'][:max_len]\n",
    "    if len(instance['segment']) > max_len:\n",
    "        segment_preview.append('...')\n",
    "    print(\"▶ segment:\", segment_preview)\n",
    "\n",
    "    # 마스킹 정보\n",
    "    print(\"▶ mask_idx:\", instance['mask_idx'])\n",
    "    print(\"▶ mask_label:\", instance['mask_label'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fecc0f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a6e899bfb34e4ca54820da55b2d2da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 lines : ['▁지미', '▁카터']\n",
      "['▁제임스', '▁얼', '▁\"', '지', '미', '\"', '▁카터', '▁주니어', '(,', '▁1924', '년', '▁10', '월', '▁1', '일', '▁~', '▁)', '는', '▁민주당', '▁출신', '▁미국', '▁39', '번째', '▁대통령', '▁(19', '77', '년', '▁~', '▁1981', '년', ')', '이다', '.']\n",
      "['▁그는', '▁2002', '년', '▁말', '▁인권', '과', '▁중재', '▁역할에', '▁대한', '▁공로를', '▁인정받아', '▁노벨', '▁평화', '상을', '▁받게', '▁되었다', '.']\n",
      "\n",
      "14 lines : ['▁수학']\n",
      "['▁수학', '(', '數', '學', ',', '▁)', '은', '▁양', ',', '▁구조', ',', '▁공간', ',', '▁변화', ',', '▁미', '적', '분', '▁등의', '▁개념을', '▁다루는', '▁학문이다', '.', '▁현대', '▁수학', '은', '▁형식', '▁논', '리를', '▁이용해서', '▁공', '리로', '▁구성된', '▁추상', '적', '▁구조를', '▁연구하는', '▁학문', '으로', '▁여겨', '지기도', '▁한다', '.', '▁수학', '은', '▁그', '▁구조와', '▁발전', '▁과정', '에서는', '▁자연', '과학', '에', '▁속하는', '▁물리', '학을', '▁비롯한', '▁다른', '▁학문', '들과', '▁깊은', '▁연', '관을', '▁맺고', '▁있다', '.', '▁하지만', ',', '▁어느', '▁과학의', '▁분야', '들과는', '▁달리', ',', '▁자연', '계에서', '▁관측', '되지', '▁않는', '▁개념', '들에', '▁대해서', '까지', '▁이론을', '▁일반화', '▁및', '▁추상', '화', '시킬', '▁수', '▁있다는', '▁차이가', '▁있다고', '▁한다', '.', '▁수', '학자들은', '▁그러한', '▁개념', '들에', '▁대해서', '▁추측', '을', '▁하고', ',', '▁적절', '하게', '▁선택', '된', '▁정의', '와', '▁공리', '로부터의', '▁엄', '밀한', '▁연', '역을', '▁통해서', '▁추측', '들의', '▁진', '위를', '▁파악', '한다', '.']\n",
      "['▁수', '학의', '▁기초를', '▁확실히', '▁세우', '기', '▁위해', ',', '▁수리', '논', '리', '학과', '▁집합', '론이', '▁발전', '하였고', ',', '▁이와', '▁더불어', '▁범주', '론이', '▁최근', '에도', '▁발전', '되고', '▁있다', '.', '▁“', '근', '본', '▁위기', '”', '라는', '▁말은', '▁대략', '▁1900', '년에서', '▁1930', '년', '▁사이에', '▁일어난', ',', '▁수', '학의', '▁엄', '밀한', '▁기초', '에', '▁대한', '▁탐', '구를', '▁상징', '적으로', '▁보여주는', '▁말이다', '.', '▁수', '학의', '▁엄', '밀한', '▁기초', '에', '▁대한', '▁몇', '▁가지', '▁의견', '▁불', '일', '치는', '▁오늘날에도', '▁계속되고', '▁있다', '.', '▁수', '학의', '▁기초', '에', '▁대한', '▁위', '기는', '▁그', '▁당시', '▁수많은', '▁논쟁', '에', '▁의해', '▁촉발', '되었으며', ',', '▁그', '▁논쟁', '에는', '▁칸', '토', '어의', '▁집합', '론과', '▁브라우', '어', '-', '힐', '베르트', '▁논쟁이', '▁포함되었다', '.']\n",
      "\n",
      "4 lines : ['▁수학', '▁상수']\n",
      "['▁수학에서', '▁상수', '란', '▁그', '▁값이', '▁변하지', '▁않는', '▁불변', '량으로', ',', '▁변', '수의', '▁반대', '말', '이다', '.', '▁물리', '▁상수', '와는', '▁달리', ',', '▁수학', '▁상', '수는', '▁물리적', '▁측정', '과는', '▁상관없이', '▁정의된다', '.']\n",
      "['▁특정', '▁수학', '▁상수', ',', '▁예를', '▁들면', '▁골', '롬', '-', '딕', '맨', '▁상수', ',', '▁프랑', '세', '즈', '-', '로', '빈', '슨', '▁상수', ',', '▁formula', '_1', ',', '▁레', '비', '▁상수', '같은', '▁상', '수는', '▁다른', '▁수학', '상수', '▁또는', '▁함수', '와', '▁약한', '▁상관', '관계', '▁또는', '▁강한', '▁상관', '관계를', '▁갖는다', '.']\n",
      "\n",
      "10 lines : ['▁문학']\n",
      "['▁문학', '(', '文', '學', ')', '은', '▁언어를', '▁예술적', '▁표현의', '▁제', '재로', '▁삼아', '▁새로운', '▁의미를', '▁창출', '하여', ',', '▁인간과', '▁사회를', '▁진실', '되게', '▁묘사', '하는', '▁예술의', '▁하위', '분야', '이다', '.', '▁간단하게', '▁설명', '하면', ',', '▁언어를', '▁통해', '▁인간의', '▁삶을', '▁미', '적', '(', '美', '的', ')', '으로', '▁형상', '화한', '▁것이라고', '▁볼', '▁수', '▁있다', '.', '▁문학', '은', '▁원래', '▁문예', '(', '文', '藝', ')', '라고', '▁부르는', '▁것이', '▁옳', '으며', ',', '▁문학을', '▁학문의', '▁대상', '으로서', '▁탐구', '하는', '▁학문의', '▁명칭', '▁역시', '▁문예', '학', '이다', '.', '▁문예', '학은', '▁음악', '사', '학', ',', '▁미술', '사', '학', '▁등과', '▁함께', '▁예술', '학의', '▁핵심', '분야', '로서', '▁인문', '학의', '▁하위', '범', '주에', '▁포함된다', '.']\n",
      "['▁반영', '론적', '▁관', '점에', '▁의한', '▁감', '상은', '▁작품을', '▁창작', '된', '▁당시', '▁시대', '▁정', '황', '과', '▁연결', '시켜', '▁감상', '하는', '▁입장', '이고', ',', '▁내재', '적', '▁관', '점의', '▁감', '상은', '▁작품의', '▁형식', ',', '▁내용에', '▁국한', '하여', '▁감상', '하는', '▁것이다', '.', '▁표현', '론적', '▁관', '점의', '▁감', '상은', '▁작가의', '▁전기', '적', '▁사실과', '▁작품을', '▁연결', '시켜', '▁감상', '하는', '▁것이고', ',', '▁수용', '론적', '▁관', '점의', '▁감', '상은', '▁독', '자와', '▁작품을', '▁연결', '시켜', '▁감상', '하는', '▁것을', '▁말한다', '.']\n",
      "\n",
      "10 lines : ['▁나라', '▁목록']\n",
      "['▁이', '▁문서는', '▁나라', '▁목록', '이며', ',', '▁전', '▁세계', '▁20', '6', '개', '▁나라의', '▁각', '▁현황', '과', '▁주권', '▁승인', '▁정보를', '▁개', '요', '▁형태로', '▁나열', '하고', '▁있다', '.']\n",
      "['▁위', '▁목록에', '▁포함되지', '▁않은', '▁다음', '▁국가는', '▁몬테', '비', '데오', '▁협약', '의', '▁모든', '▁조건을', '▁만족', '하지', '▁못', '하거나', ',', '▁자주', '적이고', '▁독립', '적', '임을', '▁주장', '하지', '▁않는', '▁국가이다', '.']\n",
      "\n",
      "['▁화학']\n",
      "['▁화학', '(', '化', '學', ',', '▁)', '은', '▁물질의', '▁성질', ',', '▁조성', ',', '▁구조', ',', '▁변화', '▁및', '▁그에', '▁수반', '하는', '▁에너지의', '▁변화를', '▁연구하는', '▁자연과', '학의', '▁한', '▁분야이다', '.', '▁물리학', '도', '▁역시', '▁물질을', '▁다루는', '▁학문', '이지만', ',', '▁물리학', '이', '▁원', '소와', '▁화합', '물을', '▁모두', '▁포함한', '▁물체의', '▁운동과', '▁에너지', ',', '▁열', '적', '·', '전기', '적', '·', '광', '학적', '·', '기계', '적', '▁속', '성을', '▁다루고', '▁이러한', '▁현상', '으로부터', '▁통일된', '▁이론을', '▁구축', '하려는', '▁것과는', '▁달리', '▁화학', '에서는', '▁물질', '▁자체를', '▁연구', '▁대상으로', '▁한다', '.', '▁화학', '은', '▁이미', '▁존재하는', '▁물질을', '▁이용하여', '▁특정한', '▁목적에', '▁맞는', '▁새로운', '▁물질을', '▁합성', '하는', '▁길을', '▁제공하며', ',', '▁이는', '▁농작', '물의', '▁증', '산', ',', '▁질병의', '▁치료', '▁및', '▁예방', ',', '▁에너지', '▁효율', '▁증대', ',', '▁환경', '오', '염', '▁감소', '▁등', '▁여러', '▁가지', '▁이', '점을', '▁제공한다', '.']\n",
      "['▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '▁화합', '물의', '▁범위가', '▁크게', '▁넓', '어져', '▁탄소', '▁사슬', '▁또는', '▁탄소', '▁고', '리를', '▁가진', '▁모든', '▁화합', '물을', '▁뜻한다', '.', '▁유기', '화', '학의', '▁오랜', '▁관심', '사는', '▁유기', '▁화합', '물의', '▁합성', '▁메커니즘', '이다', '.', '▁현대에', '▁들어서', '▁핵', '자기', '▁공명', '법과', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 문단 샘플 5개만 출력해보기\n",
    "count = 5  # 5개 문단만 출력\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 한 문단 단위로 저장될 리스트\n",
    "    for line in tqdm(in_f, total=total):  # tqdm으로 진행률 표시\n",
    "        line = line.strip()  # 줄 끝 개행 문자 제거\n",
    "        if line == \"\":  # 빈 줄 = 문단 구분자\n",
    "            if 0 < len(doc):  # 누적된 문장이 있을 때만 처리\n",
    "                if 0 < count:\n",
    "                    count -= 1\n",
    "                    # 문단 정보 출력 (몇 줄? 첫 줄, 중간 줄, 마지막 줄)\n",
    "                    print(len(doc), \"lines :\", doc[0])\n",
    "                    print(doc[1])\n",
    "                    print(doc[-1])\n",
    "                    print()\n",
    "                else:\n",
    "                    break  # 5개 문단만 확인하고 종료\n",
    "                doc = []  # 다음 문단을 위한 초기화\n",
    "        else:\n",
    "            # 빈 줄이 아닌 경우: 토크나이저로 토큰화 후 저장\n",
    "            pieces = vocab.encode_as_pieces(line)  # 문장을 subword로 쪼갬\n",
    "            if 0 < len(pieces):  # 토큰이 비어있지 않으면\n",
    "                doc.append(pieces)\n",
    "\n",
    "    # 파일 끝나고 마지막 문단이 남아 있는 경우\n",
    "    if 0 < len(doc):\n",
    "        print(doc[0])\n",
    "        print(doc[1])\n",
    "        print(doc[-1])\n",
    "        doc = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5507d05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "545910ba504c43b99a86ee9d749082a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: 21 instances: 9\n",
      "▶ is_next: 1\n",
      "▶ tokens: ['[CLS]', '[MASK]', '▁카터', '[SEP]', '▁제임스', '▁얼', '▁\"', '지', '미', '\"', '▁카터', '▁주니어', '(,', '[MASK]', '[MASK]', '▁10', '월', '[MASK]', '[MASK]', '▁~', '▁)', '는', '▁민주당', '▁출신', '▁미국', '▁39', '번째', '[MASK]', '▁(19', '77', '...']\n",
      "▶ segment: [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, '...']\n",
      "▶ mask_idx: [1, 13, 14, 17, 18, 27, 42, 43, 57]\n",
      "▶ mask_label: ['▁지미', '▁1924', '년', '▁1', '일', '▁대통령', '▁섬', '터', '▁후']\n",
      "\n",
      "doc: 14 instances: 7\n",
      "▶ is_next: 0\n",
      "▶ tokens: ['[CLS]', '.', '▁하지만', ',', '[MASK]', '▁과학의', '[MASK]', '[MASK]', '▁달리', ',', '▁자연', '계에서', '▁관측', '되지', '▁않는', '▁개념', '들에', '▁대해서', '까지', '▁이론을', '▁일반화', '▁및', '▁추상', '화', '시킬', '▁수', '▁표시', '▁차이가', '▁있다고', '▁한다', '...']\n",
      "▶ segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, '...']\n",
      "▶ mask_idx: [4, 6, 7, 26, 36, 39, 40, 47, 48]\n",
      "▶ mask_label: ['▁어느', '▁분야', '들과는', '▁있다는', '▁대해서', '▁하고', ',', '▁공리', '로부터의']\n",
      "\n",
      "doc: 4 instances: 1\n",
      "▶ is_next: 1\n",
      "▶ tokens: ['[CLS]', '▁수학', '▁상수', '[SEP]', '[MASK]', '▁상수', '란', '▁그', '[MASK]', '▁변하지', '▁지명', '▁불변', '량으로', ',', '▁변', '수의', '▁반대', '말', '이다', '.', '▁물리', '▁상수', '와는', '▁달리', ',', '▁수학', '▁상', '수는', '▁물리적', '▁측정', '...']\n",
      "▶ segment: [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, '...']\n",
      "▶ mask_idx: [4, 8, 10, 35, 36, 37, 50, 59, 60]\n",
      "▶ mask_label: ['▁수학에서', '▁값이', '▁않는', '▁상', '수는', '▁대개', '▁있는', '▁정의', '가능한']\n",
      "\n",
      "doc: 10 instances: 4\n",
      "▶ is_next: 1\n",
      "▶ tokens: ['[CLS]', '▁문학', '[SEP]', '▁문학', '(', '文', '學', ')', '은', '▁언어를', '▁예술적', '▁표현의', '▁제', '재로', '▁삼아', '▁새로운', '▁의미를', '▁창출', '하여', ',', '[MASK]', '[MASK]', '▁진실', '되게', '▁묘사', '하는', '▁예술의', '[MASK]', '[MASK]', '[MASK]', '...']\n",
      "▶ segment: [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, '...']\n",
      "▶ mask_idx: [20, 21, 27, 28, 29, 30, 38, 49, 50]\n",
      "▶ mask_label: ['▁인간과', '▁사회를', '▁하위', '분야', '이다', '.', '▁삶을', '▁볼', '▁수']\n",
      "\n",
      "doc: 10 instances: 3\n",
      "▶ is_next: 0\n",
      "▶ tokens: ['[CLS]', '▁목록을', '▁구성하고', '▁빵', '▁국가를', '▁선정', '하는', '▁기준에', '▁대한', '▁정보는', '▁\"', '포', '함', '▁기준', '\"', '[MASK]', '[MASK]', '▁통해', '▁설명하였다', '.', '▁나라에', '▁대한', '▁일반적인', '▁정보는', '▁\"', '국가', '\"', '▁문서', '에서', '▁설명하고', '...']\n",
      "▶ segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, '...']\n",
      "▶ mask_idx: [3, 15, 16, 55, 56, 57, 58, 59, 62]\n",
      "▶ mask_label: ['▁있는', '▁단', '락을', '▁형태로', '▁나열', '하고', '▁있다', '.', '▁명']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 5  # 테스트용 문단 개수 제한\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":\n",
    "            if 0 < len(doc):\n",
    "                # NSP+MLM instance 생성\n",
    "                instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "                print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "                if len(instances) > 0:\n",
    "                    pretty_print_instance(instances[0])\n",
    "                doc = []\n",
    "                count -= 1\n",
    "                if 0 < count:\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            # 여기가 빠졌던 부분입니다!\n",
    "            pieces = vocab.encode_as_pieces(line)\n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "\n",
    "    # 마지막 문단 처리\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        instances = create_pretrain_instances(doc, 128)\n",
    "        # save\n",
    "        print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "        print(instances[0])\n",
    "        print(instances[-1])\n",
    "        print()\n",
    "        doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f9b6e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def make_pretrain_data(vocab, vocab_list, in_file, out_file, n_seq, mask_prob=0.15):\n",
    "    \"\"\"\n",
    "    BERT pretraining 학습용 데이터를 생성하여 파일로 저장\n",
    "    - vocab: SubwordTokenizer 객체\n",
    "    - vocab_list: 마스킹 가능한 일반 토큰 리스트\n",
    "    - in_file: 입력 파일 경로 (.txt)\n",
    "    - out_file: 출력 파일 경로 (.jsonl)\n",
    "    - n_seq: 최대 시퀀스 길이\n",
    "    - mask_prob: MLM 마스킹 확률\n",
    "    \"\"\"\n",
    "\n",
    "    def save_pretrain_instances(out_f, doc):\n",
    "        # 문단(doc)으로부터 학습용 인스턴스들을 생성하고 저장\n",
    "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # 전체 라인 수 계산 (진행률 표시용)\n",
    "    total_lines = 0\n",
    "    with open(in_file, 'r') as f:\n",
    "        for _ in f:\n",
    "            total_lines += 1\n",
    "\n",
    "    with open(in_file, 'r') as in_f:\n",
    "        with open(out_file, 'w') as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=total_lines):\n",
    "                line = line.strip()\n",
    "\n",
    "                if line == \"\":\n",
    "                    if len(doc) > 0:\n",
    "                        save_pretrain_instances(out_f, doc)\n",
    "                        doc = []\n",
    "                else:\n",
    "                    pieces = vocab.encode_as_pieces(line)\n",
    "                    if len(pieces) > 0:\n",
    "                        doc.append(pieces)\n",
    "\n",
    "            # 마지막 문단 처리\n",
    "            if len(doc) > 0:\n",
    "                save_pretrain_instances(out_f, doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a5c2ce1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3957761/3957761 [06:36<00:00, 9994.12it/s] \n"
     ]
    }
   ],
   "source": [
    "pretrain_json_path = os.getenv('HOME') + '/aiffel/bert_pretrain/data/bert_pre_train.json'\n",
    "\n",
    "make_pretrain_data(vocab, vocab_list, corpus_file, pretrain_json_path, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a352ea96",
   "metadata": {},
   "source": [
    "# 6. 메모리 사용량 최소화하는 방법(memmap)\n",
    "\n",
    "### np.memmap이란?\n",
    "\n",
    "`np.memmap`은 NumPy의 특수한 배열 객체로, **전체 데이터를 메모리에 모두 올리지 않고**  \n",
    "**디스크 파일에 저장된 데이터를 필요한 부분만 메모리에 불러와서 사용하는 방식**입니다.\n",
    "\n",
    "---\n",
    "\n",
    "### 핵심 동작 원리\n",
    "\n",
    "- 일반적인 `np.array`는 모든 데이터를 RAM에 올려야 하므로,\n",
    "  대용량일 경우 **메모리 부족** 문제가 발생합니다.\n",
    "\n",
    "- 반면 `np.memmap`은 파일의 일부분만 **슬라이스 단위로 로딩**하기 때문에,\n",
    "  **RAM에 필요한 조각만 올라오고**, 나머지는 **디스크에 유지**됩니다.\n",
    "\n",
    "\n",
    "### 사용 예시\n",
    "\n",
    "```python\n",
    "arr = np.memmap('data.memmap', dtype=np.float32, mode='w+', shape=(1000000,))\n",
    "arr[0] = 42.0  # 이 때 0번 인덱스만 메모리에 올라옴\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8195e9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    \"\"\"\n",
    "    학습에 필요한 데이터를 로드\n",
    "    :param vocab: vocab\n",
    "    :param filename: 전처리된 json 파일\n",
    "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
    "    :param count: 데이터 수 제한 (None이면 전체)\n",
    "    :return enc_tokens: encoder inputs\n",
    "    :return segments: segment inputs\n",
    "    :return labels_nsp: nsp labels\n",
    "    :return labels_mlm: mlm labels\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            # 데이터 수 제한\n",
    "            if count is not None and count <= total:\n",
    "                break\n",
    "    \n",
    "    # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if total <= i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # encoder token\n",
    "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            # segment\n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            # nsp label\n",
    "            label_nsp = data[\"is_next\"]\n",
    "            # mlm label\n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b11a1c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/128000 [00:00<?, ?it/s]/tmp/ipykernel_62/2049745891.py:42: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
      "/tmp/ipykernel_62/2049745891.py:43: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
      "/tmp/ipykernel_62/2049745891.py:44: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
      "100%|██████████| 128000/128000 [00:24<00:00, 5200.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load early stop 128000 128000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 128000건만 메모리에 로딩\n",
    "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=128000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "accd91d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([    5,  2386,  2163,     6,     6,     6,  8047,   173,   607,\n",
       "          2387,   317, 27604,  3926, 27625,  5551,    37, 18995,  8198,\n",
       "          9858,  1447,     6,     6,  5551,    37,    18,   451,     6,\n",
       "             6,  4864,  6436,    25,  5551, 27646, 18205,   928,   157,\n",
       "         27821,    61, 27773,   530, 27604,  3372,   523,  3409,   673,\n",
       "          5551,    18,   982, 13264, 27599,  5551,  5053,   982,  4739,\n",
       "           151, 27604,  6945,  3554,  6719, 12788,  2046,  5890,  1853,\n",
       "         27599,     4, 16415, 25250,  3324,  1042,   103, 27610, 27686,\n",
       "         27718, 25250,  7504,   416,  5708, 27625,   131, 27662,     7,\n",
       "         27629,   203,   241, 27602,  4867,   788,   243,  5898,   796,\n",
       "           663,  1647,  4630, 27625,   203,  3008, 27625, 27616,    16,\n",
       "         27599, 16415,   207,  4612,  5551, 27646,   630, 27714,  4269,\n",
       "             6,     6,     6, 14406,  1605, 27599,  3050, 14146, 15991,\n",
       "          8637, 27599,    13,    81,   729,  2247, 15033, 27873, 14475,\n",
       "         27813,     4], dtype=int32),\n",
       " memmap([    5,   639,   254,   238,   787,  2654,   784, 27604,  1925,\n",
       "           259,     6,     6,  3954, 27669, 10672, 16018, 27599,  5010,\n",
       "          4082,  4871, 15405,  5113, 27646, 28274,  2604, 27599,     6,\n",
       "           442, 27667, 27645, 27604,  5838,    65,  5611,   129, 27655,\n",
       "         27881, 28017, 10214,  8262,    28, 27599, 11232,   165, 27604,\n",
       "            13,  1326,  5611,    29, 28018, 27793,  1326, 27787, 27617,\n",
       "         27977,  8199,  1326, 27787, 27616,   287, 11484,  7908,   171,\n",
       "         27599,     4,  3220, 27635,  4937,    19,  4791,  3220, 27635,\n",
       "             6,     6,    98,  5947, 27616, 27602, 13069, 26245,     6,\n",
       "          8801, 27600, 19200,  3220, 27635, 21271,     6,     6,  5699,\n",
       "          5846, 16230,  2191, 27599,  5078,    81, 27604,   342,  4812,\n",
       "         27625,     6, 14456,    89, 24930,  2540, 27600,   488,  4871,\n",
       "          2524, 13358,   171, 27599,   330, 27604, 14456,     6,  4842,\n",
       "         27682, 27625,  2131, 14135,  9943,   761, 28254,   658,   171,\n",
       "         27599,     4], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " 0,\n",
       " 1,\n",
       " memmap([    0,     0,     0, 27596, 27671,   969,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,  1921, 27625,     0,     0,     0,     0,  4267,\n",
       "         27599,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           429,  5346, 27626,     0,     0,     0,  5551,     0,     0,\n",
       "             0,     0,     0,     0, 25987,     0,     0,     0,     0,\n",
       "             0,     0], dtype=int32),\n",
       " memmap([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,  1326, 13317,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,  5147,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "          7462, 13054,     0,     0,     0,     0,     0,     0,  7203,\n",
       "             0,     0,     0,     0,     0,     0,     8, 13676,    68,\n",
       "          7336,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,   294,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0, 15170,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0], dtype=int32))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 처음과 마지막 확인\n",
    "pre_train_inputs[0][0], pre_train_inputs[0][-1], pre_train_inputs[1][0], pre_train_inputs[1][-1], pre_train_labels[0][0], pre_train_labels[0][-1], pre_train_labels[1][0], pre_train_labels[1][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2967156d",
   "metadata": {},
   "source": [
    "# 7. BERT 모델 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6a2d4b",
   "metadata": {},
   "source": [
    "## 7.1 `get_pad_mask`\n",
    "\n",
    "입력 시퀀스 중에서 PAD(=0) 위치를 찾아서 마스킹합니다.\n",
    "\n",
    "마스킹된 위치는 attention 점수 계산 시 무시됩니다.\n",
    "\n",
    "시퀀스 길이가 5지만, 실제 내용은 3개\n",
    "\n",
    "```python\n",
    "\n",
    "tokens = [[7, 21, 30, 0, 0]]   # 시퀀스 길이가 5지만, 실제 내용은 3개\n",
    "get_pad_mask(tokens)\n",
    "→ [[[[0, 0, 0, 1, 1]]]]       # 0 위치가 PAD로 마스킹됨  \n",
    "\n",
    "```\n",
    "\n",
    "✔ 함수의 목적\n",
    "\n",
    "이 마스크를 적용하면 모델은 **\"실제 단어들끼리만\"** 의미 있는 관계를 학습할 수 있습니다.\n",
    "\n",
    "만약 패딩까지 attention을 하게 되면, 의미 없는 정보에까지 weight(가중치)를 분산하게 되어 모델의 학습이 불안정하거나 부정확해질 수 있어요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fe0753c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    패딩 마스크 생성 함수\n",
    "    - 입력 시퀀스에서 PAD 토큰 위치를 1로 표시\n",
    "    - 나머지 위치는 0으로 유지\n",
    "\n",
    "    Args:\n",
    "        tokens: 입력 토큰 시퀀스 (batch_size, n_seq)\n",
    "        i_pad: PAD 토큰의 ID (기본값 0)\n",
    "\n",
    "    Returns:\n",
    "        mask: 패딩 마스크 (batch_size, 1, n_seq)\n",
    "              → True(PAD 위치): 1.0, 나머지: 0.0\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)  # PAD인 위치만 1\n",
    "    mask = tf.expand_dims(mask, axis=1)  # shape: (batch_size, 1, n_seq)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c35ae0b",
   "metadata": {},
   "source": [
    "## 7.2 `get_ahead_mask`\n",
    "\n",
    "입력 시퀀스에서  \n",
    "- **자기보다 뒤에 있는 토큰(미래)**  \n",
    "- 그리고 **PAD 위치**를 함께 마스킹합니다.\n",
    "\n",
    "이 마스크는 **디코더(Decoder)** 또는 **MLM(Masked Language Modeling)** 과제에서 주로 사용됩니다.  \n",
    "즉, 모델이 미래 단어를 미리 보지 못하게 하고, 패딩 또한 무시하게 만듭니다.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 예시 코드\n",
    "\n",
    "```python\n",
    "tokens = [[10, 25, 0]]   # 시퀀스 길이 3, 실제 단어는 2개\n",
    "get_ahead_mask(tokens)\n",
    "\n",
    "→ [[[0, 1, 1],\n",
    "     [0, 0, 1],\n",
    "     [0, 0, 1]]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "41fe7670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    미래 정보 + PAD 마스킹 생성 함수\n",
    "    - 자기보다 뒤에 있는 토큰과 PAD 위치를 모두 마스킹\n",
    "    - 주로 Decoder나 Masked LM 등에서 사용\n",
    "\n",
    "    Args:\n",
    "        tokens: 입력 토큰 시퀀스 (batch_size, n_seq)\n",
    "        i_pad: PAD 토큰의 ID (기본값 0)\n",
    "\n",
    "    Returns:\n",
    "        mask: 어헤드 마스크 (batch_size, n_seq, n_seq)\n",
    "              → 미래 위치나 PAD 위치: 1.0, 나머지: 0.0\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "\n",
    "    # 상삼각 행렬: 자기보다 미래 위치는 1, 나머지는 0\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)  # shape: (n_seq, n_seq)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)  # (1, n_seq, n_seq)\n",
    "\n",
    "    # 패딩 마스크: (batch_size, 1, n_seq)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "\n",
    "    # 둘 중 하나라도 1이면 마스킹: 미래 또는 PAD 위치\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)  # shape: (batch_size, n_seq, n_seq)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172a243e",
   "metadata": {},
   "source": [
    "#### PAD 마스킹을 해야하는 이유?\n",
    "\n",
    "[PAD] 토큰은 실제 단어가 아니고 자리만 채우는 용도입니다. 모델이 [PAD]에 Attention을 주면 안됩니다.\n",
    "이 위치는 마스킹해야 된다.\n",
    "\n",
    "#### 자기 보다 미래 위치는 1로 해서 패딩을 시킴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1772ad9c",
   "metadata": {},
   "source": [
    "## 7.2 `GLEU 활성화 함수`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72c9957",
   "metadata": {},
   "source": [
    "### GELU (Gaussian Error Linear Unit) 활성화 함수 시각화\n",
    "\n",
    "GELU는 BERT에서 기본으로 사용되는 **비선형 활성화 함수**입니다.  \n",
    "ReLU와 달리 부드럽게 입력을 통과시키며, 입력의 크기에 따라 **부분적으로만 활성화**합니다.\n",
    "\n",
    "GELU의 정의는 다음과 같습니다:\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) = 0.5 \\cdot x \\cdot \\left(1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}} \\cdot (x + 0.044715x^3)\\right)\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "- 음수 입력: 0에 가깝게 수축되며 거의 활성화되지 않음\n",
    "\n",
    "- 0 근처 입력: 부드럽게 0을 통과하며 부분적으로 활성화됨\n",
    "\n",
    "- 양수 입력: 거의 선형적으로 출력에 반영됨\n",
    "\n",
    "- ReLU보다 더 부드러운 비선형성을 가짐\n",
    "\n",
    "- x의 크기에 따라 확률적으로 \"입력이 의미 있는지\"를 판단해 통과시킴 음수 -> 거의 통과 x, 0 일경우 절반 통과, 1일 경우 다 통과\n",
    "\n",
    "- BERT, GPT 계열 모델에서 표준 활성화 함수로 사용됨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8bf6a095",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# GELU 함수 정의 (TF 버전 그대로)\n",
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + tf.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "\n",
    "# 입력 범위 설정\n",
    "x = tf.linspace(-5.0, 5.0, 1000)\n",
    "y = gelu(x)\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y, label='GELU', color='blue')\n",
    "plt.title(\"GELU Activation Function\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b2f5a9",
   "metadata": {},
   "source": [
    "## 7.3 가중치, 편향치 초기화 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4a48ac",
   "metadata": {},
   "source": [
    "### 가중치  초기화 함수\n",
    "\n",
    "딥러닝 모델에서 **가중치(weight)와 바이어스(bias)의 초기화**는  \n",
    "학습의 안정성과 수렴 속도에 매우 중요한 영향을 미칩니다.  \n",
    "BERT 모델에서는 다음과 같은 초기화 전략을 사용합니다.\n",
    "\n",
    "`Truncated Normal(stddev=stddev)` 은 정규 분포에서 샘플링하되, 평균으로부터 +-2 표준편차 이상 벗어난 값은 버리고 다시 뽑는다.\n",
    "\n",
    "극단적인 값들을 제거해 초기값이 너무 커지는것을 방지한다.\n",
    "\n",
    "### 바이어스 초기화 함수\n",
    "\n",
    "모든 바이어스를 0으로 초기화 한다.\n",
    "\n",
    "딥러닝 아키텍처에서 바이어스는 기울기를 통해 학습 될 값이므로 처음에는 0으로 초기화 시켜줘야한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "915b8934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    가중치(weight) 초기화 함수 생성\n",
    "    - TruncatedNormal: 정규분포에서 값 생성하되, ±2 표준편차 바깥 값은 버림\n",
    "    - BERT 논문에서도 표준편차 0.02 사용\n",
    "    Args:\n",
    "        stddev: 생성할 랜덤값의 표준편차 (기본값 0.02)\n",
    "    Returns:\n",
    "        TruncatedNormal initializer 객체\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    바이어스(bias) 초기화 함수 생성\n",
    "    - 모든 값을 0으로 초기화\n",
    "    Returns:\n",
    "        Zeros initializer 객체\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a48192",
   "metadata": {},
   "source": [
    "## 7.4 Cofing는 Json 파일을 -> Python 처럼 쓸 수 있게 해주는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b04c6ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aa78aa",
   "metadata": {},
   "source": [
    "## 7.5 Token Embedding\n",
    "\n",
    "BERT 모델에서 Token Embedding은 입력 토큰 ID를 고차원 벡터 공간으로 변환하는 과정입니다.  \n",
    "이 과정을 통해 모델은 단순한 정수 ID가 아닌, **의미론적 정보**를 담고 있는 벡터로 입력을 처리할 수 있게 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3ab1e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    입력 임베딩과 출력 projection에 동일한 가중치를 사용하는 Shared Embedding Layer입니다.\n",
    "    BERT 구조에서 embedding lookup과 output linear projection이 동일한 weight를 공유합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        클래스 생성자\n",
    "        Args:\n",
    "            config: Config 객체 (모델 설정값 포함)\n",
    "            name: 레이어 이름\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab     # 전체 vocabulary 크기\n",
    "        self.d_model = config.d_model     # hidden dimension 크기 (임베딩 차원)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        실제 공유 가중치(shared_weights)를 생성하는 함수.\n",
    "        Keras Layer의 구조상, build()는 첫 호출 시 자동 실행됩니다.\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                name=\"weights\",                               # 가중치 이름\n",
    "                shape=[self.n_vocab, self.d_model],           # (vocab 크기, embedding 차원)\n",
    "                initializer=kernel_initializer()              # truncated normal 초기화\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        레이어를 호출할 때 동작하는 메서드\n",
    "        Args:\n",
    "            inputs: 입력 텐서 (token ID 또는 hidden vector)\n",
    "            mode: 실행 모드 - \"embedding\" 또는 \"linear\"\n",
    "        Returns:\n",
    "            임베딩 벡터 또는 linear projection 결과\n",
    "        \"\"\"\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")  # 잘못된 mode 입력 시 에러\n",
    "\n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        임베딩 룩업 수행 (입력: token ID, 출력: embedding vector)\n",
    "        Args:\n",
    "            inputs: (batch_size, seq_len)의 token ID 텐서\n",
    "        Returns:\n",
    "            (batch_size, seq_len, d_model)의 임베딩 벡터\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):\n",
    "        \"\"\"\n",
    "        hidden state를 vocabulary 차원으로 projection (출력단)\n",
    "        Args:\n",
    "            inputs: (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            (batch_size, seq_len, n_vocab)로 출력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "\n",
    "        # 먼저 2D로 reshape 후 행렬 곱 수행\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (batch_size * seq_len, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # 원래 shape로 복원\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418dcbad",
   "metadata": {},
   "source": [
    "## 7.6 Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6e304ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Embedding Layer\n",
    "    - 입력 시퀀스의 각 토큰 위치를 임베딩 벡터로 변환합니다.\n",
    "    - 위치 정보를 학습 가능한 embedding table로 처리합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자 (initializer)\n",
    "        Args:\n",
    "            config: 설정 객체 (Config), n_seq와 d_model을 포함\n",
    "            name: 레이어 이름\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        # 위치 임베딩을 위한 Embedding 레이어 생성\n",
    "        # 입력: 위치 인덱스 (0 ~ n_seq-1), 출력: d_model 차원의 벡터\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=config.n_seq,        # 위치 수 (시퀀스 최대 길이)\n",
    "            output_dim=config.d_model,     # 임베딩 차원\n",
    "            embeddings_initializer=kernel_initializer()\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        레이어 호출 시 실행되는 메서드\n",
    "        Args:\n",
    "            inputs: 입력 텐서 (batch_size, n_seq)\n",
    "                    → 값은 사용되지 않고 shape만 필요\n",
    "\n",
    "        Returns:\n",
    "            embed: 위치 임베딩 텐서 (batch_size, n_seq, d_model)\n",
    "        \"\"\"\n",
    "\n",
    "        # 각 위치마다 인덱스를 생성: [0, 1, 2, ..., n_seq-1]\n",
    "        # tf.ones_like → 입력 shape과 같은 텐서를 만들고, cumsum으로 인덱스 부여\n",
    "        position = tf.cast(\n",
    "            tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True),  # shape: (batch_size, n_seq)\n",
    "            tf.int32\n",
    "        )\n",
    "\n",
    "        # 위치 인덱스에 대한 임베딩 벡터 조회\n",
    "        embed = self.embedding(position)  # (batch_size, n_seq, d_model)\n",
    "\n",
    "        return embed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4e6d23",
   "metadata": {},
   "source": [
    "## 7.7 ScaleDotProduct Attetion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9e299253",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention 레이어\n",
    "    - Q, K, V를 입력으로 받아 어텐션 가중치를 계산하고 가중합을 반환합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        Args:\n",
    "            name: 레이어 이름\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        어텐션 계산\n",
    "        Args:\n",
    "            Q: Query 벡터 (bs, n_head, seq_q, d_head)\n",
    "            K: Key 벡터   (bs, n_head, seq_k, d_head)\n",
    "            V: Value 벡터 (bs, n_head, seq_k, d_head)\n",
    "            attn_mask: 마스킹 텐서 (bs, 1, seq_q, seq_k)\n",
    "        Returns:\n",
    "            attn_out: 어텐션 출력 (bs, n_head, seq_q, d_head)\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. 어텐션 스코어: Q × K^T (유사도)\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)  # (bs, n_head, seq_q, seq_k)\n",
    "\n",
    "        # 2. 스케일링: d_head의 루트로 나눠서 softmax 안정화\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = attn_score / scale\n",
    "\n",
    "        # 3. 마스킹 처리: 마스크가 1인 위치에는 매우 작은 음수를 더해 softmax 무시\n",
    "        attn_scale -= 1.e9 * attn_mask  # 마스킹된 위치는 softmax 후 거의 0이 됨\n",
    "\n",
    "        # 4. 확률로 변환: softmax\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "\n",
    "        # 5. 가중합: attention output = softmax(QK^T) × V\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "\n",
    "        return attn_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb241f64",
   "metadata": {},
   "source": [
    "## 7.8 MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e0eefb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention 레이어\n",
    "    - 여러 개의 Attention Head를 병렬로 계산한 후 하나로 합치는 구조\n",
    "    - 입력 Q, K, V에 대해 선형 변환 → 분할 → attention → 병합 → 최종 projection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        Args:\n",
    "            config: 설정값을 담은 Config 객체 (d_model, n_head, d_head 포함)\n",
    "            name: 레이어 이름\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model   # 전체 hidden size (예: 768)\n",
    "        self.n_head = config.n_head     # 헤드 수 (예: 12)\n",
    "        self.d_head = config.d_head     # 각 헤드당 차원 수 (예: 64) → d_model = n_head * d_head\n",
    "\n",
    "        # Q, K, V 각각을 위한 Dense projection 레이어\n",
    "        self.W_Q = tf.keras.layers.Dense(self.n_head * self.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(self.n_head * self.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(self.n_head * self.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "        # Scaled Dot-Product Attention 연산을 담당하는 서브 레이어\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "\n",
    "        # 여러 헤드의 attention 출력을 합쳐서 최종 output으로 변환\n",
    "        self.W_O = tf.keras.layers.Dense(self.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        레이어 실행\n",
    "        Args:\n",
    "            Q, K, V: (batch, seq_len, d_model) 입력\n",
    "            attn_mask: (batch, seq_len, seq_len) 어텐션 마스크\n",
    "        Returns:\n",
    "            attn_out: (batch, seq_len, d_model) 최종 attention 결과\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "\n",
    "        # 1. Q, K, V를 n_head 개로 분리 (multi-head 준비)\n",
    "        # W_Q/K/V → shape: (batch, seq_len, n_head * d_head)\n",
    "        # reshape + transpose → (batch, n_head, seq_len, d_head)\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])\n",
    "\n",
    "        # 2. 마스크에도 헤드 차원을 맞춰주기 위해 차원 추가\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)  # shape: (batch, 1, seq_q, seq_k)\n",
    "\n",
    "        # 3. Scaled Dot-Product Attention 실행 (각 head 별로 병렬 처리)\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (batch, n_head, seq_len, d_head)\n",
    "\n",
    "        # 4. (n_head, d_head)를 다시 합쳐 원래 차원으로 복원\n",
    "        attn_out = tf.transpose(attn_out, [0, 2, 1, 3])                      # → (batch, seq_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out, [batch_size, -1, self.d_model])     # → (batch, seq_len, d_model)\n",
    "\n",
    "        # 5. 최종 projection (출력 차원으로 다시 맵핑)\n",
    "        attn_out = self.W_O(attn_out)  # (batch, seq_len, d_model)\n",
    "\n",
    "        return attn_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad14dac7",
   "metadata": {},
   "source": [
    "## 7.9 PositionWiseFeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d3e59527",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position-wise Feed Forward Network (FFN)\n",
    "    - Transformer 각 토큰 위치마다 동일한 FFN을 독립적으로 적용\n",
    "    - 두 개의 Dense 레이어 (d_model → d_ff → d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        Args:\n",
    "            config: Config 객체 (d_model, d_ff 포함)\n",
    "            name: 레이어 이름\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        # 첫 번째 Dense: 확장 → d_ff\n",
    "        self.W_1 = tf.keras.layers.Dense(\n",
    "            config.d_ff,                    # 중간 차원 확장 (예: 3072)\n",
    "            activation=gelu,                # BERT에서는 GELU 사용\n",
    "            kernel_initializer=kernel_initializer(),\n",
    "            bias_initializer=bias_initializer()\n",
    "        )\n",
    "\n",
    "        # 두 번째 Dense: 축소 → d_model\n",
    "        self.W_2 = tf.keras.layers.Dense(\n",
    "            config.d_model,                 # 입력 차원으로 복원 (예: 768)\n",
    "            kernel_initializer=kernel_initializer(),\n",
    "            bias_initializer=bias_initializer()\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        레이어 호출 시 실행\n",
    "        Args:\n",
    "            inputs: (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            ff_val: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))  # GELU → Linear\n",
    "        return ff_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9893ab",
   "metadata": {},
   "source": [
    "## 7.10 EncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4a11009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Transformer Encoder Layer\n",
    "    - 하나의 블록은 Self-Attention → Add & Norm → FFN → Add & Norm 순서로 구성됩니다.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        Args:\n",
    "            config: 모델 설정 객체\n",
    "            name: 레이어 이름\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        # 1. Self-Attention 블록\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        # 2. Position-wise FFN 블록\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        # 3. Dropout (공통으로 사용)\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        Encoder Layer 실행 함수\n",
    "        Args:\n",
    "            enc_embed: (batch_size, seq_len, d_model) → 입력 임베딩 or 이전 레이어 출력\n",
    "            self_mask: (batch_size, seq_len, seq_len) → padding 마스크\n",
    "        Returns:\n",
    "            enc_out: EncoderLayer의 출력 (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Multi-Head Self-Attention + Residual + LayerNorm\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)  # Self-Attention\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))  # Add & Norm\n",
    "\n",
    "        # 2. Position-wise FFN + Residual + LayerNorm\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))  # Add & Norm\n",
    "\n",
    "        return enc_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3084f1",
   "metadata": {},
   "source": [
    "## 7.11 BERT Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "357ff132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT 모델 전체 구조\n",
    "    - 입력: (enc_tokens, segments)\n",
    "    - 출력: [CLS] 토큰 벡터 (문장 분류용), MLM 예측 logits\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        Args:\n",
    "            config: 설정 정보 객체 (Config)\n",
    "            name: 레이어 이름\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad  # pad 토큰 ID\n",
    "\n",
    "        # 1. 임베딩 관련 레이어들\n",
    "        self.embedding = SharedEmbedding(config)                        # token embedding (가중치 공유)\n",
    "        self.position = PositionEmbedding(config)                       # position embedding\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model,     # segment A/B 구분 (0/1)\n",
    "                                                 embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        # 2. 인코더 레이어 N개\n",
    "        self.encoder_layers = [\n",
    "            EncoderLayer(config, name=f\"encoder_layer_{i}\")\n",
    "            for i in range(config.n_layer)\n",
    "        ]\n",
    "\n",
    "        # 3. dropout\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        BERT 실행 함수\n",
    "        Args:\n",
    "            inputs: 튜플 (enc_tokens, segments)\n",
    "            - enc_tokens: (batch, seq_len)\n",
    "            - segments: (batch, seq_len)\n",
    "        Returns:\n",
    "            logits_cls: [CLS] 토큰 벡터 (batch, d_model)\n",
    "            logits_lm: 전체 토큰에 대한 vocab logits (batch, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        # 1. Padding 위치를 가리는 mask 생성\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        # 2. 임베딩 lookup (token + position + segment)\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)  # (batch, seq_len, d_model)\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "\n",
    "        # 3. 인코더 레이어 반복 적용\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        # 4. 출력\n",
    "        logits_cls = enc_out[:, 0]                          # 문장 분류용 [CLS] 토큰 출력\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")  # MLM 예측용: linear projection\n",
    "\n",
    "        return logits_cls, logits_lm\n",
    "\n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        전체 embedding 계산 함수\n",
    "        Args:\n",
    "            tokens: token id 입력 (batch, seq_len)\n",
    "            segments: segment id 입력 (batch, seq_len)\n",
    "        Returns:\n",
    "            embed: 최종 임베딩 벡터 (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        embed = (\n",
    "            self.embedding(tokens) +           # token embedding\n",
    "            self.position(tokens) +            # position embedding\n",
    "            self.segment(segments)             # segment embedding\n",
    "        )\n",
    "        embed = self.norm(embed)               # layer normalization\n",
    "        return embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "599bde75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT의 [CLS] 벡터를 활용한 NSP 분류기\n",
    "    - Dense + tanh → Dense + softmax 구조\n",
    "    NSP용 출력 레이어\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config: 모델 설정값\n",
    "            n_output: 출력 클래스 수 (NSP는 2개: is_next / not_next)\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        # 1. [CLS] 벡터를 비선형 변환\n",
    "        self.dense1 = tf.keras.layers.Dense(\n",
    "            config.d_model, activation=tf.nn.tanh,\n",
    "            kernel_initializer=kernel_initializer(),\n",
    "            bias_initializer=bias_initializer()\n",
    "        )\n",
    "\n",
    "        # 2. softmax로 NSP 결과 출력\n",
    "        self.dense2 = tf.keras.layers.Dense(\n",
    "            n_output, use_bias=False, activation=tf.nn.softmax,  # 이미 마지막에 softmax 있음\n",
    "            name=\"nsp\", kernel_initializer=kernel_initializer()\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9bc6b512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_pre_train(config):\n",
    "    \"\"\"\n",
    "    BERT 사전학습 모델 구성 함수\n",
    "    - 입력: enc_tokens, segments\n",
    "    - 출력: NSP softmax 결과, MLM softmax 결과\n",
    "    \"\"\"\n",
    "    # 1. 입력 정의\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")  # (batch, seq_len)\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")      # (batch, seq_len)\n",
    "\n",
    "    # 2. BERT 인코더 호출\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))  # (batch, d_model), (batch, seq_len, vocab)\n",
    "\n",
    "    # 3. NSP 출력 레이어\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)  # (batch, 2)\n",
    "\n",
    "    # 4. MLM 출력 레이어\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)   # (batch, seq_len, vocab)\n",
    "\n",
    "    # 5. 모델 정의\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e5b155e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 256,\n",
       " 'n_head': 4,\n",
       " 'd_head': 64,\n",
       " 'dropout': 0.05,\n",
       " 'd_ff': 1024,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 3,\n",
       " 'n_seq': 256,\n",
       " 'n_vocab': 32007,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config({\"d_model\": 256, \"n_head\": 4, \"d_head\": 64, \"dropout\": 0.1, \"d_ff\": 1024, \"layernorm_epsilon\": 0.001, \"n_layer\": 3, \"n_seq\": 256, \"n_vocab\": 0, \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "687793fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2/2 [==============================] - 2s 15ms/step - loss: 11.1566 - nsp_loss: 0.7409 - mlm_loss: 10.4158 - nsp_acc: 0.4000 - mlm_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 9.9240 - nsp_loss: 0.6217 - mlm_loss: 9.3023 - nsp_acc: 0.8000 - mlm_acc: 0.0200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7db1ec23d2e0>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 10\n",
    "\n",
    "# make test inputs\n",
    "enc_tokens = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "segments = np.random.randint(0, 2, (10, n_seq))\n",
    "labels_nsp = np.random.randint(0, 2, (10,))\n",
    "labels_mlm = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "\n",
    "test_model = build_model_pre_train(config)\n",
    "test_model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=[\"acc\"])\n",
    "\n",
    "# test model fit\n",
    "test_model.fit((enc_tokens, segments), (labels_nsp, labels_mlm), epochs=2, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecdbaab",
   "metadata": {},
   "source": [
    "# Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a48af87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    MLM 손실 함수 (Masked Language Modeling용)\n",
    "    - padding 위치(0)를 무시하고, 실제 마스크된 위치만 학습\n",
    "    - MLM 학습 중요도를 높이기 위해 20배 보정\n",
    "    Args:\n",
    "        y_true: 정답 토큰 ID (batch, seq_len)\n",
    "        y_pred: 예측 로짓 (batch, seq_len, vocab_size)\n",
    "    Returns:\n",
    "        손실 값 (batch, seq_len)\n",
    "    \"\"\"\n",
    "    # 1. 기본 sparse categorical cross entropy 계산 (mask 미적용)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        reduction=tf.keras.losses.Reduction.NONE  # 위치별로 loss 유지\n",
    "    )(y_true, y_pred)\n",
    "\n",
    "    # 2. PAD 토큰(0) 위치를 마스크로 제거\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)  # (bs, seq_len)\n",
    "    loss *= mask  # PAD 위치의 손실은 0으로 만듦\n",
    "\n",
    "    # 3. MLM은 더 어려운 task이므로 loss를 20배 증폭\n",
    "    return loss * 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9efb8a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    MLM 정확도 계산 함수\n",
    "    - PAD 위치는 정확도에서 제외\n",
    "    Args:\n",
    "        y_true: 정답 토큰 ID (batch, seq_len)\n",
    "        y_pred: 예측 로짓 (batch, seq_len, vocab_size)\n",
    "    Returns:\n",
    "        정확도 스칼라 값\n",
    "    \"\"\"\n",
    "    # 1. 예측 확률 → 클래스 ID\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)  # (bs, seq_len)\n",
    "\n",
    "    # 2. 정답 여부 판단\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "\n",
    "    # 3. PAD 위치 제거\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask  # PAD 위치의 예측은 정확도에서 제외\n",
    "\n",
    "    # 4. 정확도 계산\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)  # 분모가 0이 되지 않도록 보호\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7fbeae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    Warmup + Cosine Decay Learning Rate Schedule\n",
    "    - 초반엔 선형 증가 (Warmup)\n",
    "    - 이후에는 Cosine 함수 기반으로 감소\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train_steps: 전체 학습 step 수\n",
    "            warmup_steps: warmup 동안 선형 증가할 step 수\n",
    "            max_lr: warmup 이후에 도달할 최대 learning rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps, \"Warmup step은 전체보다 작아야 합니다.\"\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        \"\"\"\n",
    "        현재 step에 따른 learning rate 계산\n",
    "        Args:\n",
    "            step_num: 현재 step (정수)\n",
    "        Returns:\n",
    "            학습률 (scalar tensor)\n",
    "        \"\"\"\n",
    "        # 1. Warmup 단계 여부 판단\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "\n",
    "        # 2. warmup 단계: 0 → 1까지 선형 증가\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "\n",
    "        # 3. cosine 단계: pi를 기준으로 학습률 점점 감소\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / \\\n",
    "                   max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "\n",
    "        # 4. warmup이면 lr1 사용, 아니면 lr2 사용\n",
    "        return (state * lr1 + (1.0 - state) * lr2) * self.max_lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35ddf55",
   "metadata": {},
   "source": [
    "0~500 step: 학습률이 선형 증가 → warm-up\n",
    "\n",
    "500~4000 step: 완만한 곡선 형태로 감소 → cosine decay\n",
    "\n",
    "전체 곡선: 부드럽게 올라갔다가 천천히 떨어지는 종 모양(Bump + Tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e245b00e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute lr \n",
    "test_schedule = CosineSchedule(train_steps=4000, warmup_steps=500)\n",
    "lrs = []\n",
    "for step_num in range(4000):\n",
    "    lrs.append(test_schedule(float(step_num)).numpy())\n",
    "\n",
    "# draw\n",
    "plt.plot(lrs, 'r-', label='learning_rate')\n",
    "plt.xlabel('Step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d1fdbc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_tokens (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segments (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BERT)                     ((None, 256), (None, 10629632    enc_tokens[0][0]                 \n",
      "                                                                 segments[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pooled_nsp (PooledOutput)       (None, 2)            66304       bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "nsp (Softmax)                   (None, 2)            0           pooled_nsp[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlm (Softmax)                   (None, None, 32007)  0           bert[0][1]                       \n",
      "==================================================================================================\n",
      "Total params: 10,695,936\n",
      "Trainable params: 10,695,936\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7462de97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 861/2000 [===========>..................] - ETA: 5:28 - loss: 9.0138 - nsp_loss: 0.4075 - mlm_loss: 8.6063 - nsp_acc: 0.9052 - mlm_lm_acc: 0.2506"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_62/2365584240.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# 2. 모델 학습 및 학습 이력 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m history = pre_train_model.fit(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_train_inputs\u001b[0m\u001b[0;34m,\u001b[0m                         \u001b[0;31m# 입력: (enc_tokens, segments)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     y={\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1. 모델 가중치 저장 콜백 설정\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(\n",
    "    f\"{model_dir}/bert_pre_train.hdf5\",   # 저장 경로\n",
    "    monitor=\"mlm_lm_acc\",                 # MLM 정확도를 기준으로 가장 좋은 모델 저장\n",
    "    verbose=1,                            # 저장 시 로그 출력\n",
    "    save_best_only=True,                 # 가장 성능 좋은 모델만 저장\n",
    "    mode=\"max\",                           # 정확도이므로 큰 값이 더 좋은 것\n",
    "    save_freq=\"epoch\",                    # 매 epoch마다 체크\n",
    "    save_weights_only=True                # 전체 모델이 아닌 가중치만 저장\n",
    ")\n",
    "# 2. 모델 학습 및 학습 이력 저장\n",
    "history = pre_train_model.fit(\n",
    "    x=pre_train_inputs,                         # 입력: (enc_tokens, segments)\n",
    "    y={\n",
    "        \"nsp\": pre_train_labels[0],             # NSP 라벨 (is_next 여부)\n",
    "        \"mlm\": pre_train_labels[1]              # MLM 라벨 (마스크된 단어 정답)\n",
    "    },\n",
    "    batch_size=batch_size,                      # 배치 크기\n",
    "    epochs=epochs,                              # 에폭 수\n",
    "    callbacks=[save_weights],                   # 가중치 저장 콜백\n",
    "    verbose=1                                   # 학습 로그 출력\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e2f867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training result\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.plot(history.history['nsp_acc'], 'g-', label='nsp_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.plot(history.history['mlm_loss'], 'r-- ', label='mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bda8df",
   "metadata": {},
   "source": [
    "##  학습 로그 요약 및 분석\n",
    "\n",
    "**첫번째 실험 (순정)**\n",
    "\n",
    "| Epoch | NSP Loss | MLM Loss | NSP Accuracy | MLM LM Accuracy | 성능 개선 여부 |\n",
    "|-------|----------|----------|--------------|------------------|----------------|\n",
    "| 1     | 0.6004   | 19.5624  | 0.6708       | 0.1337           | ✅ 초기화 |\n",
    "| 2     | 0.5926   | 18.8241  | 0.6839       | 0.1400           | ✅ |\n",
    "| 3     | 0.5668   | 17.2935  | 0.7222       | 0.1600           | ✅ |\n",
    "| 4     | 0.5368   | 15.6243  | 0.7602       | 0.1873           | ✅ |\n",
    "| 5     | 0.5073   | 14.7069  | 0.7959       | 0.2065           | ✅ |\n",
    "| 6     | 0.4824   | 14.0687  | 0.8238       | 0.2203           | ✅ |\n",
    "| 7     | 0.4610   | 13.6189  | 0.8475       | 0.2308           | ✅ |\n",
    "| 8     | 0.4463   | 13.3042  | 0.8638       | 0.2386           | ✅ |\n",
    "| 9     | 0.4364   | 13.1060  | 0.8742       | 0.2436           | ✅ |\n",
    "| 10    | `0.4309`   | `13.0093`  | `0.8802`       | `0.2458`           | ✅ |\n",
    "\n",
    "**두번째 실험 [ Mask_prob(0.15→0.1),  droput 0.1 → 0.05) ]**\n",
    "\n",
    "mlm Loss 는 8까지 떨어지는 모습을 보였으나 Accuracy는 큰차이점이 없었음\n",
    "\n",
    "---\n",
    "\n",
    "###  NSP Task\n",
    "\n",
    "- 정확도: **67% → 88%**로 상승  \n",
    "- Loss 지속 감소 → 빠르게 수렴됨  \n",
    "- 📌 *NSP는 비교적 쉬운 task로 빠르게 학습되는 것이 일반적입니다.*\n",
    "\n",
    "---\n",
    "\n",
    "###  MLM Task\n",
    "\n",
    "- Accuracy: **13.3% → 24.6%**까지 상승  \n",
    "- Loss도 꾸준히 감소 중 → **학습이 정상적으로 진행 중**  \n",
    "- 📌 *MLM은 예측해야 할 단어가 많고 난이도가 높아 느리게 학습됨*\n",
    "\n",
    "---\n",
    "\n",
    "###  결론\n",
    "\n",
    "- **전체 학습 흐름**: 매우 안정적으로 진행됨  \n",
    "- **NSP**: 빠르게 수렴 (거의 완료)  \n",
    "- **MLM**: 느리지만 계속 성능 향상 중  \n",
    "- **추가 학습 여지 있음**: MLM 정확도는 더 학습하면 향상 가능성 있음\n",
    "\n",
    "> 🔍 따라서, 현재 모델은 사전학습이 성공적으로 진행되었으며  \n",
    "> 추가적인 epoch 학습을 통해 MLM 성능을 더 끌어올릴 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f90527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
